{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Part 4: Attention-based classification\n",
    "\n",
    "This last part of homework 2 will have you _using_ the vectors we learned from your word2vec implementation to do classification. You should complete the initial word2vec part before before starting on this.\n",
    "\n",
    "Broadly, this last part of the homework consists of a few major steps:\n",
    "1. Load in the data, word vectors, and word-indexing\n",
    "2. Define the attention-based classification network\n",
    "3. Train your model at least one epoch (2+ is recommended though).\n",
    "4. Perform exploratory analyses on attention\n",
    "5. Test the effects of freezing the pre-trained word vectors (see homework PDF for details)\n",
    "\n",
    "After Step 2, you should be able to train your classifier implementation on a small percent of the dataset and verify that it's learning correctly. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**\n",
    "\n",
    "\n",
    "### Estimated performance times\n",
    "\n",
    "We designed this homework to be run on a laptop-grade CPU, so no GPU is required. If your primary computing device is a tablet or similar device, this homework can also be _developed_ on that device but then run on a more powerful machine in the Great Lakes cluster (for free). Such cases are the exception though. Following, we report on the estimated times from our reference implementation for longer-running or data-intensive pieces of the homework. Your timing may vary based on implementation design; major differences in time (e.g., 10x longer) usually point to a performance bug.\n",
    "\n",
    "* Reading data, tokenizing, and converting to ids: ~20 seconds\n",
    "* Training one epoch: ~18 minutes\n",
    "* Training one epoch using frozen embeddings: ~3 minutes\n",
    "* Evaluating on dev/test set: ~5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Attention plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the necessary parameters from the word2vec code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word-to-index mapping we used for word2vec and use the same type\n",
    "# of tokenizer. We'll need to use this to tokenize in the same way and keep \n",
    "# the same word-to-id mapping\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "with open('128_y_mod_idx2word', 'rb') as f:\n",
    "    index_to_word = pickle.load(f)\n",
    "    \n",
    "with open('128_y_mod_word2idx', 'rb') as f:\n",
    "    word_to_index = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Classifier Model\n",
    "\n",
    "Just like we did for word2vec, let's define a PyTorch `nn.Module` class here that will contain our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4806]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5146]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5122]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5210]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class DocumentAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, embeddings_fname):\n",
    "        '''\n",
    "        Creates the new classifier model. embeddings_fname is a string containing the\n",
    "        filename with the saved pytorch parameters (the state dict) for the Embedding\n",
    "        object that should be used to initialize this class's word Embedding parameters\n",
    "        '''\n",
    "        super(DocumentAttentionClassifier, self).__init__()\n",
    "        \n",
    "        torch.manual_seed(1234)\n",
    "        \n",
    "        # Save the input arguments to the state\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.embeddings_fname = embeddings_fname\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create the Embedding object that will hold our word embeddings that we\n",
    "        # learned in word2vec. This embedding object should have the same size\n",
    "        # as what we learned before. However, we don't to start from scratch! \n",
    "        # Once created, load the saved (word2vec-based) parameters into the object\n",
    "        # using load_state_dict.\n",
    "        state_dict = torch.load(embeddings_fname, map_location=torch.device('cpu'))\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings.weight.data = state_dict['target_embeddings.weight']\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "        # Define the attention heads. You have two options:\n",
    "        # \n",
    "        # 1) the worse way to implement this is to define your heads using an Embedding\n",
    "        #    and then access them individually later in forward(). This will be slower\n",
    "        #    but will probably still work \n",
    "        #\n",
    "        # 2) the ideal way is to think of your attention heads as rows in a matrix--\n",
    "        #    just like we do for word2vec. While this is kind of the same as how\n",
    "        #    we represent things like in an Embedding, the key difference is that we\n",
    "        #    can now use **matrix operations** to calculate the different r and a\n",
    "        #    vectors, which will be much faster (and less code). To do this, you'll\n",
    "        #    need to represent the attention heads as a Tensor directly (not a layer)\n",
    "        #    and make sure pytorch runs gradient descent on these parameters.\n",
    "        #\n",
    "        #  It's up to you which to use, but try option 2 first and see what you do \n",
    "        #  in the forward() function\n",
    "        self.attention_heads = nn.Parameter(torch.randn(num_heads, embedding_size))\n",
    "        \n",
    "        \n",
    "        # Define the output layer\n",
    "        \n",
    "        # Define the layer that goes from the concatenated attention heads' outputs\n",
    "        # to the single output value. We'll push this output value through the sigmoid\n",
    "        # to get our prediction\n",
    "   \n",
    "        #self.output_layer = nn.Linear(self.vocab_size * self.num_heads, 1) \n",
    "        self.output_layer = nn.Linear(num_heads * embedding_size, 1)\n",
    "\n",
    "\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def forward(self, word_ids):\n",
    "        \n",
    "        word_embeds = self.embeddings(word_ids)  \n",
    "\n",
    "        attention_heads_expanded = self.attention_heads.unsqueeze(0).expand(word_embeds.size(0), -1, -1)\n",
    "\n",
    "        r = torch.bmm(attention_heads_expanded, word_embeds.transpose(1, 2)) \n",
    "        a = F.softmax(r, dim=2)\n",
    "\n",
    "        weighted_sum = torch.bmm(a, word_embeds)  \n",
    "        concatenated = weighted_sum.view(word_embeds.size(0), -1)  \n",
    "        output = torch.sigmoid(self.output_layer(concatenated))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = DataLoader(train_list, batch_size=1)\n",
    "model = DocumentAttentionClassifier(len(word_to_index), 50, 4, \"128_y_mod.pth\")\n",
    "\n",
    "# model.train()\n",
    "# for index, (token_ids, labels) in enumerate(dataset):\n",
    "#     if index < 10:  \n",
    "#         output = model.forward(token_ids)\n",
    "#         print(output)\n",
    "  \n",
    "#     else:\n",
    "#         break  \n",
    "# #print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the datasets \n",
    "\n",
    "You can keep these as pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_train_df = pd.read_csv(\"sentiment.train.csv\")\n",
    "sent_dev_df = pd.read_csv(\"sentiment.dev.csv\")\n",
    "sent_test_df = pd.read_csv(\"sentiment.test.csv\")\n",
    "\n",
    "file_dict = {\n",
    "    'train':sent_train_df, \n",
    "    'dev':sent_dev_df,\n",
    "    'test':sent_test_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each dataset into a list of tuples of the form `([word-ids,...], label)`. Both the word ids and the label should be numpy arrays so they will get converted into Tensors by our data loader. Note that you did something very similar for creating the word2vec training data. This process will require tokenizing the data in the same way as you did for word2vec and using the same word-to-id mapping (both of which you loaded/created above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DF\n",
      "DEV DF\n",
      "TEST DF\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "dev_list = []\n",
    "test_list = []\n",
    "\n",
    "words = set(list(word_to_index.keys()))\n",
    "\n",
    "print(\"TRAIN DF\")\n",
    "for idx, row in sent_train_df.iterrows():\n",
    "    tokens = tokenizer.tokenize(row['text'])\n",
    "    token_ids = []\n",
    "    label = row['label']\n",
    "    for token in tokens:\n",
    "        if token in words:\n",
    "            token_ids.append(word_to_index[token])\n",
    "        else:\n",
    "            pass\n",
    "    tupple = (np.array(token_ids), np.array(label))\n",
    "    train_list.append(tupple)\n",
    "    \n",
    "print(\"DEV DF\")\n",
    "for idx, row in sent_dev_df.iterrows():\n",
    "    tokens = tokenizer.tokenize(row['text'])\n",
    "    token_ids = []\n",
    "    label = row['label']\n",
    "    for token in tokens:\n",
    "        if token in words:\n",
    "            token_ids.append(word_to_index[token])\n",
    "        else:\n",
    "            pass\n",
    "    tupple = (np.array(token_ids), np.array(label))\n",
    "    dev_list.append(tupple)\n",
    "    \n",
    "print(\"TEST DF\") \n",
    "for idx, row in sent_dev_df.iterrows():\n",
    "    tokens = tokenizer.tokenize(row['text'])\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in words:\n",
    "            token_ids.append(word_to_index[token])\n",
    "        else:\n",
    "            pass\n",
    "    tupple = (np.array(token_ids))\n",
    "    test_list.append(tupple)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this worked you should see XXXX train, XXXX dev, and XXX test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 20000, 20000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list), len(dev_list), len(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the code training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll evaluate periodically so before we start training, let's define a function that takes in some evaluation data (e.g., the dev or test sets) and computes the F1 score on that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, eval_data):\n",
    "    '''\n",
    "    Scores the model on the evaluation data and returns the F1\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The training code will look surprisingly similar to your word2vec code. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. **see below:** Initializes Weights & Biases and periodically write our running-sum of the loss \n",
    "5. Train your model \n",
    "\n",
    "For step 4, in addition to writing the loss, you should write the F1 score on the dev set to the `wandb` as well, using the specified number of steps.\n",
    "\n",
    "**NOTE:** In this training, you'll use a batch size of 1, which will make your life _much_ simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set your training stuff, hyperparameters, models, etc. here\n",
    "\n",
    "# TODO: initialize weights and biases (wandb) here\n",
    "\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "for epoch in []:\n",
    "\n",
    "    loss_sum = 0\n",
    "    \n",
    "    # TODO: use your DataLoader to iterate over the data\n",
    "    for step, data in enumerate([]):\n",
    "\n",
    "        # NOTE: since you created the data np.array instances,\n",
    "        # these have now been converted to Tensor objects for us\n",
    "        word_ids, label = data    \n",
    "        \n",
    "        # TODO: Fill in all the training details here\n",
    "\n",
    "        \n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to weights and biases. Be sure\n",
    "        # to reset the running sum after reporting it.\n",
    "        # \n",
    "        # Optional TODO: periodically evaluate the model on the dev set and \n",
    "        # report it to weights and biases\n",
    "        \n",
    "        \n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        \n",
    "        \n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  103,   956,   248,    21,   514,  1281,  2700,     7,   744,\n",
       "         2096,  2090,   283,  7960, 20362,    89,  1818,     0,    21,\n",
       "           70,     7,  1251,   956,  1692]),\n",
       " array(1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bd12v5d0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss last 100 steps</td><td>▁██████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss last 100 steps</td><td>65.78159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn2</strong> at: <a href='https://wandb.ai/jashkina2/attn2/runs/bd12v5d0' target=\"_blank\">https://wandb.ai/jashkina2/attn2/runs/bd12v5d0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240302_173639-bd12v5d0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bd12v5d0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jashkina/nlp2/nlp-hw2/nlp-hw2/wandb/run-20240302_173658-3ukypjwu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jashkina2/attn2/runs/3ukypjwu' target=\"_blank\">attn2</a></strong> to <a href='https://wandb.ai/jashkina2/attn2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jashkina2/attn2' target=\"_blank\">https://wandb.ai/jashkina2/attn2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jashkina2/attn2/runs/3ukypjwu' target=\"_blank\">https://wandb.ai/jashkina2/attn2/runs/3ukypjwu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time 1709419023.6734366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005be1a42cc841e1a9fb36ea66f1d776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1780245/907327351.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mtotal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mepoch_step\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    185\u001b[0m             )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "import os\n",
    "\n",
    "##########################\n",
    "# CHECK DEVICE\n",
    "##########################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################\n",
    "# PARAMETERS\n",
    "##########################\n",
    "early_stop = False\n",
    "batch_size = 1\n",
    "learning_rate = 5e-5  \n",
    "window_size = 2  \n",
    "epochs = 1\n",
    "optimizer_choice = AdamW\n",
    "\n",
    "\n",
    "##########################\n",
    "# MODEL\n",
    "##########################\n",
    "use_tiny = True\n",
    "\n",
    "model = DocumentAttentionClassifier(len(word_to_index), 50, 4, \"128_y_mod.pth\")\n",
    "data_loader = DataLoader(train_list, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optimizer_choice(lr=learning_rate, params=model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "##########################\n",
    "# TRAINING\n",
    "##########################\n",
    "\n",
    "# LOSS STUFF\n",
    "\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "wandb.init(project='attn2', name='attn2')\n",
    "start_time = time.time()\n",
    "print(\"start time\", start_time)\n",
    "loss_data = []\n",
    "total_step = 0\n",
    "loss_sum = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_step = 0\n",
    "    for index, (token_ids, labels) in enumerate(tqdm(data_loader)):\n",
    "        \n",
    "        \n",
    "        token_ids = token_ids.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        #print(\"labels shape\", labels.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(token_ids)[0]\n",
    "        #print(\"output shape\", output.shape)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_step += 1\n",
    "        epoch_step +=1 \n",
    "        #print(loss.item())\n",
    "        \n",
    "        actual_labels.append(labels)\n",
    "        predicted_labels.append(labels)\n",
    "\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (index) % 100 == 0:\n",
    "            wandb.log({\"loss last 100 steps\": loss_sum})\n",
    "            #print(time.time())\n",
    "            #print(loss_sum)\n",
    "            #print(f\"step {epoch_step}, epoch {epoch} (total steps = {total_step}): {loss_sum}\")\n",
    "            loss_data.append({'epoch':epoch, 'loss':loss.item(), 'epoch_step':epoch_step, 'total_step':total_step})\n",
    "            loss_sum = 0\n",
    "        if early_stop:\n",
    "            if total_step > early_stop:\n",
    "                print(\"BREAKING\")\n",
    "                break\n",
    "print(\"DONE\")\n",
    "end_time = time.time()\n",
    "print(\"end time\", end_time)\n",
    "\n",
    "\n",
    "######## d##################\n",
    "# PLOTS\n",
    "##########################\n",
    "model.eval()\n",
    "loss_df = pd.DataFrame(loss_data)\n",
    "#sns.lineplot(data=loss_df, x='total_step', y='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting what the model learned\n",
    "\n",
    "In this last bit of the homework you should look at the model's attention weights. We've written a visualization helper function below that will plot the attention weights. You'll need to fill in the `get_label_and_weights` method that uses the model to classify some new text and structures the attention output in a way that's specified. \n",
    "\n",
    "**NOTE:** most of the code for `get_label_and_weights` is code you've already written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_and_weights(text):\n",
    "    '''\n",
    "    Classifies the text (requires tokenizing, etc.) and returns (1) the classification label, \n",
    "    (2) the tokenized words in the model's vocabulary, \n",
    "    and (3) the attention weights over the in-vocab tokens as a numpy array. Note that the\n",
    "    attention weights will be a matrix, depending on how many heads were used in training.\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(words, attention_weights):\n",
    "    '''\n",
    "    Makes a heatmap figure that visualizes the attention weights for an item.\n",
    "    Attention weights should be a numpy array that has the shape (num_words, num_heads)\n",
    "    '''\n",
    "    fig, ax = plt.subplots() \n",
    "    # Rescale image size based on the input length\n",
    "    fig.set_size_inches((len(words), 4))    \n",
    "    im = ax.imshow(attention_weights.T)\n",
    "\n",
    "    head_labels = [ 'head-%d' % h for h in range(attention_weights.shape[1])]\n",
    "    ax.set_xticks(np.arange(len(words))) # , labels=words)\n",
    "    ax.set_yticks(np.arange(len(head_labels))) #, labels=head_labels)\n",
    "\n",
    "    # Rotate the word labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add the words and axis labels\n",
    "    ax.set_yticklabels(labels=range(attention_weights.shape[1]), fontsize=16)\n",
    "    ax.set_ylabel('Attention Head', fontsize=16)\n",
    "    ax.set_xticklabels(labels=words, fontsize=16)\n",
    "\n",
    "    # Add a color bar to show probability scaling\n",
    "    cb = fig.colorbar(im, ax=ax, label='Probability', pad = 0.01)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    cb.set_label(label='Probability',size=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example messages to try visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Just as I remembered it, one of my favorites from childhood! Great condition, very happy to have this to share with my daughter. Packaging was so nice and was received quickly.'\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''\n",
    "I'm a big fan of his, and I have to say that this was a BIG letdown. It features: Stilted dialogue, no character development, no suspense, no description of Indian tradition and poor editing.\\n\\nAvoid at all costs.\n",
    "'''\n",
    "pred, tokens, attn = get_label_and_weights(s)\n",
    "visualize_attention(tokens, attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional TODOs:\n",
    "\n",
    "### How many instances do we need to learn?\n",
    "\n",
    "Since the word2vec vectors capture word meaning, do we need to see a lot of examples to train an effective classifier? Maybe we can get away with fewer (or not?). Try making a plot that shows the performance of training on 1 epoch with varying numbers of training examples. What if we just had 10 examples? 100? 1000? \n",
    "\n",
    "### Make the \"important word\" vectors learn different attentions\n",
    "\n",
    "If your attention vectors are learning to look at similar words, we could try to add some structure to how we learn so these vectors become dissimilar. One idea is to penalize the model according to how similar each head's vector is to the others. You could use cosine similarity or MSE or any weighting function. For example, you could add the cosine similarities of each pair to the `loss` so that the model benefits from learning orthoginal or dissimlar vectors. Would this help? \n",
    "\n",
    "### Add more layers to the network\n",
    "\n",
    "This is the second easiest one, but can be fun. What if you add more layers after you aggregate? Does letting the different attention heads' representations interact give better performance? Find out!\n",
    "\n",
    "### Change the learning rate dynamically\n",
    "\n",
    "We have a fixed learning rate, but what if we wanted to decrease the learning rate as the model starts to converge? In many cases, this can help the model take smaller but more precise steps towards the best possible parameters. PyTorch supports this with _learning rate schedulers_ that tell pytorch how and when to change the learning rate. See if you can get a better performance using a scheduler!\n",
    "\n",
    "### Add support for batch sizes > 1\n",
    "\n",
    "This is non-trivial but will increase training speed _a lot_. The main issue with increasing batch sizes is that our input sequences (the word ids) in a batch will have different lengths. Under the hood, pytorch is turning your code into a series of very fast matrix operations. However, if those matrices suddenly have difference sizes, the math no longer works. As a result developers (like us) have to do a few things:\n",
    "\n",
    "* We need to _pad_ the sequences with empty values so that all sequences have the same length. You could do this by  adding an extra word ID that is the \"empty token\" and make sure its values are set to 0 (so it won't interact with anything)\n",
    "\n",
    "* Set up a collate function in our DataLoader that automatically pads each batch's data based on the longest length in the batch\n",
    "\n",
    "* At inference time, it's also efficient to mask part of the sequence that's the padded part so we ignore the computations for that part in anything downstream. Depending on how you set it up, you may be able to avoid this step.\n",
    "\n",
    "If you want to dig into this, you might see some of the documentation around packed and padded sequences in pytorch. You won't need to use these functions but they can provide more context for what's happening and why.\n",
    "\n",
    "\n",
    "### Add positional information to the word embeddings\n",
    "\n",
    "Right now our model doesn't know much about which order the words are in. What if we helped the model in this? One way that people have done this is to _add_ some positional embedding to the word embedding, where the positional embedding represents which position in the sequence is in. There are many complicated schemes for this, but one potential idea is to _learn the positional embeddings_. You would keep a separate `Embedding` object for positions with the number of positions up to the length of your longest sequence in the data. Then for the word in the first position, you add `position_embeddings(0)` to it. You can definitely speed that up by passing in a sequence of the positions in the current input and, conveniently, pytorch will let you easily add all the position embeddings to the word embeddings easily (no for loop required).\n",
    "\n",
    "Will it help here? I have no idea but I'm curious.\n",
    "\n",
    "\n",
    "**No extra credit is given for these; they're just for folks who want to explore more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
