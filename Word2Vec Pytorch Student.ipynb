{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Parts 1-3: Word2Vec\n",
    "\n",
    "This homework will have you implementing word2vec using PyTorch and let you familiarize yourself with building more complex neural networks and the larger PyTorch development infrastructure.\n",
    "\n",
    "Broadly, this homework consists of a few major parts:\n",
    "1. Implement a `Corpus` class that will load the dataset and convert it to a sequence of token ids\n",
    "2. Implement negative sampling to select tokens to be used as negative examples of words in the context\n",
    "3. Create your dataset of positive and negative examples per context and load it into PyTorch's `DataLoader` to use for sampling\n",
    "4. Implement a `Word2Vec` class that is a PyTorch neural network\n",
    "5. Implement a training loop that samples a _batch_ of target words and their respective positive/negative context words\n",
    "6. Implement rare word removal and frequent word subsampling\n",
    "7. Run your model on the full dataset for at least one epoch\n",
    "8. Do the exploratory parts of the homework\n",
    "9. Save vectors and word-indexing data for later use in training a classifier\n",
    "\n",
    "After Step 5, you should be able to run your word2vec implementation on a small dataset and verify that it's learning correctly. Once you can verify everything is working, proceed with steps 6 and beyond. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**\n",
    "\n",
    "### Estimated performance times on medium dataset\n",
    "\n",
    "We designed this homework to be run on a laptop-grade CPU, so no GPU is required. If your primary computing device is a tablet or similar device, this homework can also be _developed_ on that device but then run on a more powerful machine in the Great Lakes cluster (for free). Such cases are the exception though. Following, we report on the estimated times from our reference implementation on the medium dataset for longer-running or data-intensive pieces of the homework. Your timing may vary based on implementation design; major differences in time (e.g., 10x longer) usually point to a performance bug.\n",
    "\n",
    "* Reading and tokenizing: ~5 seconds\n",
    "* Subsampling and converting to token ids: ~15 seconds\n",
    "* Generating the list of training examples: ~2 minutes (~15 minutes before the random number generator fix)\n",
    "* Training one epoch: ~12 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe5d8501390>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "#import wandb\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Handy command-line argument parsing\n",
    "import argparse\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'cat']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokenizer.tokenize(text=\"I am a cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an efficient random number generator (do this part this later)\n",
    "\n",
    "Computers have to work to generate random numbers. However, the effort for getting those random numbers varies by how many you ask for. In practice, it's _much_ more efficient to generate many random numbers at once, rather than one at a time. \n",
    "\n",
    "\n",
    "In generating the training data for word2vec, you'll be generating a lot of random numbers. We've added a helpful class that you can use to eventually speed things up. You should use an instance of this `RandomNumberGenerator` class to generate the numbers you need (rather than using `np.random`). This class should work as a quick drop-in in its current implementation. **You should change the implementation of this class _only_ after getting the rest of your code debugged**. Once you get things working, update the code in this class so that it will create large buffers of random numbers and then when asked for a new number, read the next number from the buffer instead of calling `random` or `randint`. Essentially, this class will pre-allocate many random numbers ahead of time and then return them in order to avoid the overhead of generating them one at a time. You could see up to an ~80% performance improvement in your negatve sampling code generation as a result of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNumberGenerator:\n",
    "    ''' \n",
    "    A wrapper class for a random number generator that will (eventually) hold buffers of pre-generated random numbers for\n",
    "    faster access. For now, it just calls np.random.randint and np.random.random to generate these numbers \n",
    "    at the time they are needed.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, buffer_size, seed=12345):\n",
    "        '''\n",
    "        Initializes the random number generator with a seed and a buffer size of random numbers to use\n",
    "\n",
    "        Args:\n",
    "            buffer_size: The number of random numbers to pre-generate. You will eventually want \n",
    "                         this to be a large-enough number than you're not frequently regenerating the buffer\n",
    "            seed: The seed for the random number generator\n",
    "        '''\n",
    "        self.max_val = -1\n",
    "        # TODO (later): create a random number generator using numpy and set its seed    \n",
    "        # TODO (later): pre-generate a buffer of random floats to use for random()\n",
    "\n",
    "    def random(self):\n",
    "        '''\n",
    "        Returns a random float value between 0 and 1\n",
    "        '''\n",
    "        # TODO (later): get a random number from the float buffer, rather than calling np.random.random\n",
    "        # NOTE: If you reach the end of the buffer, you should refill it with new random float numbers\n",
    "        return np.random.random()\n",
    "\n",
    "    def set_max_val(self, max_val):\n",
    "        '''\n",
    "        Sets the maximum integer value for randint and creates a buffer of random integers\n",
    "        '''\n",
    "        self.max_val = max_val\n",
    "        # NOTE: This default implemenation just sets the max_val and does not create a buffer of random integers\n",
    "        # TODO (later): Implement a buffer of random integers (for now, we'll just use np.random.randint)\n",
    "\n",
    "    def randint(self):\n",
    "        '''\n",
    "        Returns a random int value between 0 and self.max_val (inclusive)\n",
    "        '''        \n",
    "        if self.max_val == -1:\n",
    "            raise ValueError(\"Need to call set_max_val before calling randint\")\n",
    "        \n",
    "        # TODO (later): get a random number from the int buffer, rather than calling np.random.randint\n",
    "        # NOTE: If you reach the end of the buffer, you should refill it with new random ints\n",
    "        return np.random.randint(0, self.max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class to hold the data\n",
    "\n",
    "Before we get to training word2vec, we'll need to process the corpus into some representation. The `Corpus` class will handle much of the functionality for corpus reading and keeping track of which word types belong to which ids. The `Corpus` class will also handle the crucial functionality of generating negative samples for training (i.e., randomly-sampled words that were not in the target word's context).\n",
    "\n",
    "Some parts of this class can be completed after you've gotten word2vec up and running, so see the notes below and the details in the homework PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, rng):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.rng = rng\n",
    "\n",
    "        # These state variables become populated with function calls\n",
    "        #\n",
    "        # 1. load_data()\n",
    "        # 2. generate_negative_sampling_table()\n",
    "        #\n",
    "        # See those functions for how the various values get filled in\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.full_token_sequence_as_ids = None\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "        \n",
    "        with open(\"reviews-word2vec.tiny.txt\", 'r') as f:\n",
    "            l = f.readlines()  \n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        all_tokens = []\n",
    "        print('Reading data and tokenizing')\n",
    "        for line in l:\n",
    "            tokens = self.tokenizer.tokenize(str(l))\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        print('Counting token frequencies')\n",
    "        token_counts = dict(Counter(all_tokens))\n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK>\n",
    "        # token. \n",
    "        #\n",
    "        # NOTE: You can do this step later if needed\n",
    "        print(\"Performing minimum thresholding\")\n",
    "        filtered_tokens = []\n",
    "        for (word, count) in token_counts.items():\n",
    "            print(word, count)\n",
    "            if count >= min_token_freq:\n",
    "                filtered_tokens.append(word)\n",
    "            else:\n",
    "                filtered_tokens.append(\"<UNK>\")\n",
    "\n",
    "\n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "        # occurs (including <UNK>)\n",
    "        self.word_counts = dict(Counter(filtered_tokens))\n",
    "        # print(\"UNK\", self.word_counts['<UNK>'])\n",
    "\n",
    "        \n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping.\n",
    "        \n",
    "        # Create word_to_index and index_to_word mappings\n",
    "        unique_words = list(self.word_counts.keys())\n",
    "        self.word_to_index = {word: i for i, word in enumerate(self.word_counts)}\n",
    "        self.index_to_word = {index: word for word, index in self.word_to_index.items()}\n",
    "        self.full_token_sequence_as_ids = [self.word_to_index.get(token, self.word_to_index[\"<UNK>\"]) for token in filtered_tokens]\n",
    "\n",
    "        \n",
    "        # Step 6: Compute the probability of keeping any particular *token* of a\n",
    "        # word in the training sequence, which we'll use to subsample. This subsampling\n",
    "        # avoids having the training data be filled with many overly common words\n",
    "        # as positive examples in the context\n",
    "   \n",
    "                        \n",
    "        # Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "        # a new list self.full_token_sequence_as_ids where \n",
    "        #\n",
    "        # (1) we probabilistically choose whether to keep each *token* based on the\n",
    "        # subsampling probabilities (note that this does not mean we drop\n",
    "        # an entire word!) and \n",
    "        #\n",
    "        # (2) all tokens are convered to their unique ids for faster training.\n",
    "        #\n",
    "        # NOTE: You can skip the subsampling part and just do step 2 to get\n",
    "        # your model up and running.\n",
    "            \n",
    "        # NOTE 2: You will perform token-based subsampling based on the probabilities in\n",
    "        # word_to_sample_prob. When subsampling, you are modifying the sequence itself \n",
    "        # (like deleting an item in a list). This action effectively makes the context\n",
    "        # window  larger for some target words by removing context words that are common\n",
    "        # from a particular context before the training occurs (which then would now include\n",
    "        # other words that were previously just outside the window).\n",
    "\n",
    "\n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "        \n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "        '''\n",
    "        Generates a big list data structure that we can quickly randomly index into\n",
    "        in order to select a negative training example (i.e., a word that was\n",
    "        *not* present in the context). \n",
    "        '''       \n",
    "        \n",
    "        words = np.array(list(self.word_to_index.values()))\n",
    "        freqs = np.array(list(self.word_counts.values()))\n",
    "        weights = np.power(freqs, exp_power)\n",
    "        sum_weights = np.sum(weights)\n",
    "        probs = weights / sum_weights\n",
    "    \n",
    "        \n",
    "        # Step 1: Figure out how many instances of each word need to go into the\n",
    "        # negative sampling table. \n",
    "        #\n",
    "        # HINT: np.power and np.fill might be useful here        \n",
    "        print(\"Generating sampling table\")\n",
    "        samples_per_word = probs * table_size\n",
    "\n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "        adjusted_table_size = int(np.sum(np.floor(samples_per_word)))\n",
    "        sampling_table = np.zeros(adjusted_table_size, dtype=int)\n",
    "\n",
    "\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled.\n",
    "        #\n",
    "        # Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "        # 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "        # (before converting the words to IDs):\n",
    "        #\n",
    "        # [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "        #\n",
    "        \n",
    "        idx = 0\n",
    "        for word_idx, samples in enumerate(samples_per_word):\n",
    "            count = int(np.floor(samples))\n",
    "            sampling_table[idx:idx+count] = word_idx\n",
    "            idx += count\n",
    "        self.negative_sampling_table = sampling_table\n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):\n",
    "        '''\n",
    "        Randomly samples the specified number of negative samples from the lookup\n",
    "        table and returns this list of IDs as a numpy array. As a performance\n",
    "        improvement, avoid sampling a negative example that has the same ID as\n",
    "        the current positive context word.\n",
    "        '''\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Create a list and sample from the negative_sampling_table to\n",
    "        # grow the list to num_samples, avoiding adding a negative example that\n",
    "        # has the same ID as the current context_word\n",
    "\n",
    "\n",
    "        return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"I am a cat cat cat\"\n",
    "l = dict(Counter(x))\n",
    "sg = [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the corpus\n",
    "\n",
    "Now that we have code to turn the text into training data, let's do so. We've provided several files for you to help:\n",
    "\n",
    "* `reviews-word2vec.tiny.txt` -- use this to debug your corpus reader\n",
    "* `reviews-word2vec.med.txt` -- use this to debug/verify the whole word2vec works\n",
    "* `reviews-word2vec.large.txt.gz` -- use this when everything works to generate your vectors for later parts\n",
    "* `reviews-word2vec.HUGE.gz` -- _do not use this_ unless (1) everything works and (2) you really want to test/explore. This file is not needed at all to do your homework.\n",
    "\n",
    "We recommend starting to debug with the first file, as it is small and fast to load (quicker to find bugs). When debugging, we recommend setting the `min_token_freq` argument to 2 so that you can verify that part of the code is working but you still have enough word types left to test the rest.\n",
    "\n",
    "You'll use the remaining files later, where they're described.\n",
    "\n",
    "In the next cell, create your `Corpus`, read in the data, and generate the negative sampling table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "This 58000\n",
      "was 131000\n",
      "bought 6000\n",
      "as 54000\n",
      "a 263500\n",
      "gift 3500\n",
      "but 82500\n",
      "the 463500\n",
      "person 3000\n",
      "who 24000\n",
      "got 11500\n",
      "it 171500\n",
      "loved 10000\n",
      "And 3500\n",
      "they 21500\n",
      "will 18000\n",
      "use 6500\n",
      "soon 1000\n",
      "n 251000\n",
      "Very 10500\n",
      "well 28000\n",
      "written 17500\n",
      "book 196000\n",
      "on 57500\n",
      "period 2000\n",
      "of 215000\n",
      "World 1000\n",
      "History 1000\n",
      "with 63500\n",
      "which 13500\n",
      "I 315000\n",
      "am 15500\n",
      "very 44000\n",
      "familiar 500\n",
      "Despite 1000\n",
      "my 38000\n",
      "familiarity 500\n",
      "subject 3500\n",
      "area 1500\n",
      "learned 2000\n",
      "lot 12000\n",
      "new 10000\n",
      "information 10000\n",
      "Also 3000\n",
      "one 38500\n",
      "best 10000\n",
      "concise 500\n",
      "descriptions 1500\n",
      "WWII 500\n",
      "that 89000\n",
      "have 62000\n",
      "ever 6500\n",
      "read 88000\n",
      "thought 9000\n",
      "provoking 1000\n",
      "Hot 500\n",
      "Cross 500\n",
      "Buns 500\n",
      "contains 1500\n",
      "cast 500\n",
      "characters 29500\n",
      "you 79000\n",
      "fall 2000\n",
      "in 133500\n",
      "love 15000\n",
      "and 266000\n",
      "want 10500\n",
      "to 261500\n",
      "hang 500\n",
      "out 26000\n",
      "It 49000\n",
      "has 22000\n",
      "few 6000\n",
      "different 8000\n",
      "plot 15500\n",
      "story 53500\n",
      "lines 1500\n",
      "intersect 500\n",
      "unexpected 1000\n",
      "ways 1500\n",
      "The 81500\n",
      "stories 10000\n",
      "are 50000\n",
      "full 3000\n",
      "hope 4000\n",
      "make 8500\n",
      "actually 2500\n",
      "care 8500\n",
      "about 43000\n",
      "outcome 500\n",
      "each 7500\n",
      "their 15500\n",
      "there 21500\n",
      "be 48500\n",
      "sequel 3000\n",
      "so 32500\n",
      "we 10000\n",
      "can 20500\n",
      "see 9000\n",
      "how 19000\n",
      "life 9500\n",
      "comtinues 500\n",
      "unfold 1000\n",
      "for 106500\n",
      "various 1500\n",
      "A 20000\n",
      "really 32000\n",
      "great 27000\n",
      "fun 7500\n",
      "liked 5000\n",
      "this 141500\n",
      "had 25000\n",
      "hard 7500\n",
      "time 25500\n",
      "putting 1000\n",
      "down 8500\n",
      "get 21500\n",
      "anything 3000\n",
      "else 4500\n",
      "done 8000\n",
      "suspense 1500\n",
      "kept 6000\n",
      "me 38000\n",
      "intrigued 1500\n",
      "end 8000\n",
      "discover 1000\n",
      "could 13500\n",
      "hardly 1000\n",
      "wait 6000\n",
      "arrive 1500\n",
      "When 1500\n",
      "did 22500\n",
      "stopped 2500\n",
      "everything 6000\n",
      "doing 1000\n",
      "just 38500\n",
      "manhandled 500\n",
      "box 3000\n",
      "into 14000\n",
      "her 23000\n",
      "style 4000\n",
      "only 16500\n",
      "3 5000\n",
      "D 2000\n",
      "bumpy 500\n",
      "bangle 500\n",
      "front 1000\n",
      "boy 1500\n",
      "surprised 1500\n",
      "at 25000\n",
      "many 17000\n",
      "wonderful 4500\n",
      "colorful 1500\n",
      "projects 500\n",
      "Cover 1000\n",
      "cover 5000\n",
      "already 5000\n",
      "planning 1000\n",
      "color 500\n",
      "factor 500\n",
      "project 1000\n",
      "would 34000\n",
      "Love 4500\n",
      "AWESOME 500\n",
      "Jesus 500\n",
      "s 72500\n",
      "finished 3500\n",
      "work 16000\n",
      "conveyed 500\n",
      "accurately 500\n",
      "effectively 500\n",
      "all 36500\n",
      "glory 500\n",
      "wish 5500\n",
      "every 8000\n",
      "human 1500\n",
      "being 8500\n",
      "avail 500\n",
      "themselves 1000\n",
      "gospel 500\n",
      "Pastor 500\n",
      "Prince 500\n",
      "communicates 500\n",
      "Read 2000\n",
      "give 8000\n",
      "copy 4000\n",
      "everyone 2500\n",
      "know 17000\n",
      "always 3500\n",
      "enjoy 7000\n",
      "Jinx 500\n",
      "Schwartz 500\n",
      "books 34000\n",
      "You 4500\n",
      "genesis 500\n",
      "Hetta 500\n",
      "Coffey 500\n",
      "more 36500\n",
      "DEA 500\n",
      "agents 500\n",
      "ripe 500\n",
      "spin 1000\n",
      "off 6000\n",
      "been 19500\n",
      "BEST 500\n",
      "BOOKS 500\n",
      "ive 500\n",
      "line 5500\n",
      "is 162500\n",
      "awesome 1000\n",
      "developed 5500\n",
      "i 13500\n",
      "friends 3000\n",
      "sholud 500\n",
      "selling 500\n",
      "list 1500\n",
      "Oh 1500\n",
      "don 21500\n",
      "t 78000\n",
      "let 5500\n",
      "fool 500\n",
      "ya 500\n",
      "no 16000\n",
      "smut 500\n",
      "its 8500\n",
      "truly 2500\n",
      "happened 2000\n",
      "day 5000\n",
      "good 43000\n",
      "believable 1000\n",
      "someone 4500\n",
      "keep 5500\n",
      "guessing 1500\n",
      "way 18000\n",
      "quick 4500\n",
      "made 9500\n",
      "think 14500\n",
      "subjective 500\n",
      "reality 2500\n",
      "lies 500\n",
      "tell 2000\n",
      "other 18500\n",
      "ourselves 500\n",
      "people 13500\n",
      "diagnosed 500\n",
      "Fibromyalgia 500\n",
      "explains 1000\n",
      "Didn 3000\n",
      "stop 2500\n",
      "until 2500\n",
      "Good 6000\n",
      "do 16000\n",
      "mystery 3500\n",
      "little 16000\n",
      "romance 4000\n",
      "Thus 1500\n",
      "hockey 1000\n",
      "REALLY 500\n",
      "romantic 500\n",
      "funny 2500\n",
      "heartwarming 500\n",
      "women 3000\n",
      "retreat 500\n",
      "gave 4000\n",
      "th 500\n",
      "bood 500\n",
      "away 4500\n",
      "tht 500\n",
      "needed 4500\n",
      "wanted 5500\n",
      "another 6500\n",
      "again 5500\n",
      "Neil 500\n",
      "told 4500\n",
      "he 20500\n",
      "enjoyed 16000\n",
      "writing 10500\n",
      "teller 500\n",
      "ve 10500\n",
      "several 3000\n",
      "times 5000\n",
      "now 7500\n",
      "buying 2500\n",
      "copies 500\n",
      "because 16000\n",
      "them 16000\n",
      "never 7000\n",
      "disappointed 12000\n",
      "by 25500\n",
      "author 20500\n",
      "downside 500\n",
      "next 12500\n",
      "installment 500\n",
      "trilogy 4000\n",
      "Robin 500\n",
      "Hobb 500\n",
      "weaves 500\n",
      "beautiful 1500\n",
      "saga 500\n",
      "beginning 4500\n",
      "part 9000\n",
      "series 24500\n",
      "revealed 1000\n",
      "things 9500\n",
      "cheap 2000\n",
      "first 24500\n",
      "wonder 1500\n",
      "what 25500\n",
      "like 48500\n",
      "anyway 1500\n",
      "YOU 500\n",
      "MUST 1000\n",
      "READ 500\n",
      "THIS 1500\n",
      "BOOK 1500\n",
      "OR 500\n",
      "BE 500\n",
      "DESTROYED 500\n",
      "most 10000\n",
      "accessible 500\n",
      "introduction 1500\n",
      "Transcendental 500\n",
      "Meditation 500\n",
      "seen 2000\n",
      "Bob 500\n",
      "Roth 500\n",
      "master 1000\n",
      "teacher 1500\n",
      "writes 1500\n",
      "warm 1500\n",
      "personal 3000\n",
      "easy 8000\n",
      "yet 4000\n",
      "comprehensive 2000\n",
      "does 7500\n",
      "not 90000\n",
      "teach 1000\n",
      "TM 500\n",
      "provides 1500\n",
      "an 31000\n",
      "inspiring 500\n",
      "rationale 500\n",
      "practice 2000\n",
      "nice 6500\n",
      "makes 6000\n",
      "these 11000\n",
      "re 10000\n",
      "paper 1000\n",
      "cut 1000\n",
      "outs 500\n",
      "number 2500\n",
      "years 5500\n",
      "keeps 3000\n",
      "going 8000\n",
      "d 4000\n",
      "recommend 10000\n",
      "American 3000\n",
      "classic 1000\n",
      "Thoroughly 500\n",
      "enjoyable 4000\n",
      "Fell 500\n",
      "If 14500\n",
      "were 24000\n",
      "grab 500\n",
      "up 16000\n",
      "heartbeat 500\n",
      "Excellent 2000\n",
      "reading 20500\n",
      "perspective 2000\n",
      "provided 1000\n",
      "Robert 500\n",
      "Hillegonds 500\n",
      "some 21000\n",
      "chapters 5000\n",
      "twice 1000\n",
      "understand 3500\n",
      "completely 2000\n",
      "due 1500\n",
      "historical 3500\n",
      "names 1500\n",
      "church 500\n",
      "history 9000\n",
      "explanation 1000\n",
      "beast 500\n",
      "whose 1500\n",
      "666 500\n",
      "or 17500\n",
      "616 500\n",
      "interesting 21000\n",
      "complete 3500\n",
      "sense 4000\n",
      "Everyone 1000\n",
      "interested 6500\n",
      "End 1000\n",
      "Times 500\n",
      "needs 4000\n",
      "rent 500\n",
      "intermedia 500\n",
      "finance 500\n",
      "class 6000\n",
      "price 4000\n",
      "affordable 500\n",
      "job 3500\n",
      "getting 3500\n",
      "through 19500\n",
      "basics 1000\n",
      "General 500\n",
      "Relativity 500\n",
      "gives 2000\n",
      "enough 10500\n",
      "derivations 500\n",
      "without 5500\n",
      "showing 1000\n",
      "single 500\n",
      "step 1000\n",
      "if 20500\n",
      "reminder 1000\n",
      "skip 500\n",
      "derivation 500\n",
      "learning 1500\n",
      "your 16500\n",
      "Devine 500\n",
      "They 4500\n",
      "excellent 2500\n",
      "Eve 500\n",
      "Roark 500\n",
      "become 2500\n",
      "better 13500\n",
      "share 2000\n",
      "explore 500\n",
      "Boys 500\n",
      "Boat 500\n",
      "account 4500\n",
      "rowing 500\n",
      "set 5500\n",
      "within 3500\n",
      "critical 500\n",
      "world 9500\n",
      "We 3000\n",
      "remember 2000\n",
      "yesterday 1000\n",
      "forever 1500\n",
      "splurge 500\n",
      "sturdy 500\n",
      "covers 2500\n",
      "two 8000\n",
      "children 4500\n",
      "m 15500\n",
      "sure 8000\n",
      "last 6500\n",
      "must 4500\n",
      "collection 1500\n",
      "Schrafft 1500\n",
      "memorabilia 500\n",
      "date 2000\n",
      "Joan 1000\n",
      "Slomanson 500\n",
      "collected 500\n",
      "quotes 1000\n",
      "recipes 5000\n",
      "images 2000\n",
      "memories 500\n",
      "from 34500\n",
      "varied 1000\n",
      "celebrities 500\n",
      "former 500\n",
      "customers 1000\n",
      "employees 500\n",
      "east 1500\n",
      "coast 500\n",
      "mid 500\n",
      "seventies 500\n",
      "surprising 1500\n",
      "photos 2500\n",
      "stores 500\n",
      "dotted 500\n",
      "streets 1000\n",
      "NY 1500\n",
      "Boston 500\n",
      "etc 2000\n",
      "ecspecially 500\n",
      "interiors 500\n",
      "sweet 2000\n",
      "valentine 500\n",
      "era 1500\n",
      "should 10000\n",
      "appeal 1000\n",
      "anyone 7000\n",
      "food 1500\n",
      "genteel 500\n",
      "lifestyle 1000\n",
      "apocalyptic 1000\n",
      "premise 4000\n",
      "unique 1000\n",
      "intriguing 1500\n",
      "tight 1000\n",
      "logic 500\n",
      "breaks 500\n",
      "Book 2500\n",
      "tells 1500\n",
      "german 1500\n",
      "army 1500\n",
      "participation 500\n",
      "holocost 500\n",
      "Show 500\n",
      "units 500\n",
      "commanders 500\n",
      "final 1000\n",
      "solution 1000\n",
      "against 2500\n",
      "jews 1000\n",
      "also 6000\n",
      "shows 1000\n",
      "germans 500\n",
      "operation 500\n",
      "round 500\n",
      "enemies 500\n",
      "deemed 500\n",
      "goverment 500\n",
      "magical 500\n",
      "Got 1500\n",
      "Granddaughter 500\n",
      "Christmas 1500\n",
      "she 12500\n",
      "loves 2000\n",
      "guys 2000\n",
      "stuff 2500\n",
      "deadspin 500\n",
      "his 22500\n",
      "dadspin 500\n",
      "articles 1000\n",
      "My 6000\n",
      "fiance 500\n",
      "reads 3500\n",
      "repeatedly 500\n",
      "bit 9000\n",
      "creepy 500\n",
      "For 3500\n",
      "kids 2500\n",
      "train 500\n",
      "cable 500\n",
      "cars 1000\n",
      "older 500\n",
      "primer 500\n",
      "voting 500\n",
      "politically 500\n",
      "involved 1500\n",
      "matter 3500\n",
      "us 5500\n",
      "recent 500\n",
      "Stephen 500\n",
      "King 1000\n",
      "lately 500\n",
      "indulged 500\n",
      "even 11500\n",
      "after 9000\n",
      "put 6000\n",
      "therefore 500\n",
      "trouble 1000\n",
      "shifting 500\n",
      "authors 2500\n",
      "decided 1500\n",
      "Payback 500\n",
      "established 500\n",
      "looking 7500\n",
      "forward 4500\n",
      "Dan 1000\n",
      "Stone 500\n",
      "span 500\n",
      "turbulence 500\n",
      "Ryan 500\n",
      "Shaw 500\n",
      "favor 500\n",
      "giving 3500\n",
      "biblical 1000\n",
      "inspirational 2000\n",
      "basis 1000\n",
      "reaching 500\n",
      "Christ 500\n",
      "raising 500\n",
      "generation 500\n",
      "workers 500\n",
      "go 2500\n",
      "unreached 500\n",
      "Build 1000\n",
      "convictions 500\n",
      "own 4000\n",
      "those 5500\n",
      "around 7000\n",
      "applying 500\n",
      "finally 3000\n",
      "hear 2000\n",
      "mason 500\n",
      "glad 1000\n",
      "HEA 500\n",
      "definitely 3500\n",
      "deserved 500\n",
      "hoping 3000\n",
      "clinton 500\n",
      "Nice 1000\n",
      "designs 500\n",
      "variety 1000\n",
      "Creative 500\n",
      "haven 2500\n",
      "sided 500\n",
      "perforated 500\n",
      "when 17000\n",
      "high 4000\n",
      "school 3500\n",
      "found 8000\n",
      "Meg 1500\n",
      "Dia 500\n",
      "song 1500\n",
      "ordered 3000\n",
      "amazon 1500\n",
      "Alfred 500\n",
      "Hitchcock 500\n",
      "film 1000\n",
      "recommending 500\n",
      "since 5000\n",
      "related 500\n",
      "Wonderful 500\n",
      "addition 2000\n",
      "barbecuing 500\n",
      "cook 1000\n",
      "Neat 500\n",
      "clean 500\n",
      "clear 2000\n",
      "presentation 1000\n",
      "Great 3500\n",
      "pictures 3500\n",
      "graphics 1000\n",
      "What 3000\n",
      "look 8000\n",
      "Really 1000\n",
      "range 500\n",
      "soooooooooooo 500\n",
      "Clementine 500\n",
      "Boo 500\n",
      "boo 4000\n",
      "Enjoyed 3000\n",
      "come 2500\n",
      "together 2500\n",
      "Makes 500\n",
      "thinking 500\n",
      "path 500\n",
      "Jack 2500\n",
      "grow 500\n",
      "role 500\n",
      "head 2000\n",
      "faction 500\n",
      "Quite 500\n",
      "frankly 500\n",
      "biggest 500\n",
      "problem 2000\n",
      "kind 4500\n",
      "dumb 1500\n",
      "Enter 500\n",
      "VR 500\n",
      "escape 1000\n",
      "Real 500\n",
      "destruction 500\n",
      "That 3000\n",
      "past 5000\n",
      "To 2000\n",
      "unite 500\n",
      "tribes 500\n",
      "nation 500\n",
      "opposes 500\n",
      "oligarcal 500\n",
      "empire 500\n",
      "humans 1000\n",
      "Jade 500\n",
      "Lord 500\n",
      "Can 2000\n",
      "becoming 1500\n",
      "thing 4500\n",
      "hates 500\n",
      "2nd 1000\n",
      "sucks 500\n",
      "page 7000\n",
      "doesn 4500\n",
      "Fast 1500\n",
      "paced 2000\n",
      "twists 2000\n",
      "turns 2500\n",
      "visualization 500\n",
      "scenery 500\n",
      "3rd 500\n",
      "Hi 500\n",
      "chance 1000\n",
      "review 5000\n",
      "over 12500\n",
      "quite 4000\n",
      "educational 500\n",
      "something 7000\n",
      "Schnoodle 500\n",
      "puppy 500\n",
      "ready 1000\n",
      "tips 1000\n",
      "advice 1000\n",
      "considering 500\n",
      "mix 1000\n",
      "impressed 1500\n",
      "breeders 500\n",
      "neat 500\n",
      "special 500\n",
      "breed 500\n",
      "opinion 2500\n",
      "Interviews 500\n",
      "Just 7500\n",
      "lots 3000\n",
      "All 3000\n",
      "say 3500\n",
      "art 1000\n",
      "Blackstone 500\n",
      "men 2500\n",
      "Another 1000\n",
      "comes 2500\n",
      "spiral 500\n",
      "bound 500\n",
      "notebook 500\n",
      "DVD 500\n",
      "title 1500\n",
      "handwritten 500\n",
      "Looks 500\n",
      "filmed 500\n",
      "ago 1000\n",
      "VHS 500\n",
      "info 3500\n",
      "pure 500\n",
      "gold 500\n",
      "need 8500\n",
      "started 2500\n",
      "progress 1000\n",
      "intermediate 500\n",
      "stage 500\n",
      "workouts 500\n",
      "percentage 500\n",
      "forms 500\n",
      "right 7500\n",
      "CD 2500\n",
      "proble 500\n",
      "five 3000\n",
      "star 2000\n",
      "fast 4000\n",
      "haave 500\n",
      "far 4500\n",
      "ordermore 500\n",
      "ones 3500\n",
      "Finally 500\n",
      "sanity 500\n",
      "spoken 500\n",
      "open 1000\n",
      "eyes 2500\n",
      "Transgenderism 500\n",
      "Amazing 1000\n",
      "Its 500\n",
      "Not 15500\n",
      "Food 500\n",
      "Don 5000\n",
      "Eat 500\n",
      "questions 5000\n",
      "eating 500\n",
      "correctly 1000\n",
      "processed 500\n",
      "foods 500\n",
      "huge 3500\n",
      "why 7500\n",
      "able 5500\n",
      "healthy 1000\n",
      "Zig 500\n",
      "Price 500\n",
      "encouraging 500\n",
      "such 4500\n",
      "family 6000\n",
      "individual 1500\n",
      "difference 1000\n",
      "Honest 1000\n",
      "true 2500\n",
      "word 3500\n",
      "happy 4000\n",
      "honest 2500\n",
      "purchased 1500\n",
      "husband 3000\n",
      "outlines 500\n",
      "major 2500\n",
      "roads 500\n",
      "Utah 500\n",
      "describes 500\n",
      "geology 500\n",
      "along 1500\n",
      "road 500\n",
      "Loved 2500\n",
      "As 4500\n",
      "turner 1000\n",
      "start 5000\n",
      "finish 4000\n",
      "hiw 500\n",
      "GRE 500\n",
      "strategy 500\n",
      "parts 1500\n",
      "explained 1000\n",
      "clearly 1500\n",
      "deal 4500\n",
      "So 4000\n",
      "much 22500\n",
      "Thanks 500\n",
      "case 2000\n",
      "common 1000\n",
      "Beck 1000\n",
      "second 4500\n",
      "Thomas 500\n",
      "Paine 500\n",
      "original 2000\n",
      "Common 500\n",
      "Sense 500\n",
      "lays 500\n",
      "problems 3000\n",
      "nonsense 1000\n",
      "scary 500\n",
      "Please 1000\n",
      "107 500\n",
      "pages 12000\n",
      "then 8500\n",
      "pass 2000\n",
      "IS 1500\n",
      "important 4000\n",
      "fact 4000\n",
      "show 2500\n",
      "answers 5000\n",
      "circling 500\n",
      "picture 2000\n",
      "rather 1500\n",
      "than 14500\n",
      "telling 1500\n",
      "words 2500\n",
      "mom 500\n",
      "aunts 500\n",
      "retired 500\n",
      "occupy 500\n",
      "while 5500\n",
      "doctors 500\n",
      "office 500\n",
      "highly 2500\n",
      "waiting 1500\n",
      "events 3500\n",
      "drove 500\n",
      "man 3500\n",
      "choose 500\n",
      "intense 1500\n",
      "reservation 500\n",
      "continue 1000\n",
      "buy 7000\n",
      "meet 1000\n",
      "positive 1000\n",
      "inspirations 500\n",
      "feel 7000\n",
      "granddaughters 500\n",
      "message 1500\n",
      "timing 500\n",
      "shipping 1000\n",
      "condition 2500\n",
      "Couldn 1000\n",
      "Pity 500\n",
      "expensive 1000\n",
      "Would 2000\n",
      "James 2000\n",
      "Rollins 1000\n",
      "wonderfully 1000\n",
      "creative 3500\n",
      "writer 2500\n",
      "He 3000\n",
      "ability 500\n",
      "pull 500\n",
      "adventures 500\n",
      "tremendously 500\n",
      "find 12500\n",
      "especially 3500\n",
      "Jake 1500\n",
      "Ransom 500\n",
      "Thrilling 500\n",
      "still 7500\n",
      "Worth 1000\n",
      "download 1000\n",
      "2 9000\n",
      "boys 1000\n",
      "ages 1500\n",
      "1 6000\n",
      "Choo 2000\n",
      "lil 1500\n",
      "received 3000\n",
      "free 4000\n",
      "Amztraders 500\n",
      "Anyone 500\n",
      "chemistry 500\n",
      "Sonochemistry 500\n",
      "fascinating 2000\n",
      "applications 500\n",
      "sonochemistry 500\n",
      "offer 1500\n",
      "organized 2000\n",
      "tool 2000\n",
      "understanding 500\n",
      "Combining 500\n",
      "paranormal 500\n",
      "old 5500\n",
      "west 1500\n",
      "scenarios 500\n",
      "Bought 2500\n",
      "couple 5000\n",
      "Chicago 500\n",
      "Columbian 500\n",
      "Expo 500\n",
      "interest 5000\n",
      "thumb 1000\n",
      "Lots 1500\n",
      "action 3500\n",
      "High 500\n",
      "bred 500\n",
      "city 500\n",
      "turned 1000\n",
      "barn 500\n",
      "manager 1000\n",
      "smart 500\n",
      "innovative 500\n",
      "increasing 500\n",
      "value 1000\n",
      "horse 500\n",
      "farm 1000\n",
      "There 7500\n",
      "greedy 1000\n",
      "mean 1500\n",
      "fail 500\n",
      "gain 500\n",
      "develops 500\n",
      "ups 500\n",
      "downs 500\n",
      "Emily 1500\n",
      "Wilson 500\n",
      "insight 1500\n",
      "Cumberland 500\n",
      "plotted 500\n",
      "cross 500\n",
      "genre 3500\n",
      "fiction 3000\n",
      "murder 1000\n",
      "romp 500\n",
      "English 2500\n",
      "Lakes 500\n",
      "lose 500\n",
      "yourself 1500\n",
      "once 3500\n",
      "starting 1500\n",
      "figure 1500\n",
      "twist 2000\n",
      "Rollicking 500\n",
      "adventure 3000\n",
      "Wizards 500\n",
      "create 1500\n",
      "Every 500\n",
      "chapter 5000\n",
      "science 2000\n",
      "Roundworld 500\n",
      "excellenct 500\n",
      "interspersed 500\n",
      "usual 2500\n",
      "Discworld 500\n",
      "humour 500\n",
      "Discsworld 500\n",
      "novel 5000\n",
      "Syd 500\n",
      "Hoffman 500\n",
      "insightful 2000\n",
      "uplifting 1000\n",
      "resourceful 500\n",
      "worthwhile 1000\n",
      "Day 1000\n",
      "Energy 1000\n",
      "100 1000\n",
      "Ways 500\n",
      "Boost 500\n",
      "Your 500\n",
      "Now 1000\n",
      "offers 1000\n",
      "easily 4000\n",
      "followed 1000\n",
      "suggestions 500\n",
      "painless 500\n",
      "intend 500\n",
      "favorite 2500\n",
      "chilhood 500\n",
      "100th 500\n",
      "philosophical 1000\n",
      "tale 2000\n",
      "goes 3000\n",
      "beyond 1500\n",
      "cute 1000\n",
      "sad 2000\n",
      "One 4000\n",
      "heart 3000\n",
      "essential 500\n",
      "invisible 500\n",
      "Margaret 500\n",
      "Yorke 500\n",
      "unto 500\n",
      "herself 1000\n",
      "Her 2000\n",
      "mysteries 1000\n",
      "detailed 1500\n",
      "contain 1000\n",
      "receive 1000\n",
      "civics 500\n",
      "study 1500\n",
      "US 2000\n",
      "education 1000\n",
      "Julia 1000\n",
      "Quinn 500\n",
      "strikes 500\n",
      "disappoint 1000\n",
      "music 2000\n",
      "Christine 500\n",
      "memoir 1500\n",
      "informative 3500\n",
      "folk 500\n",
      "process 1000\n",
      "troubadour 500\n",
      "Different 500\n",
      "Electronic 500\n",
      "Components 500\n",
      "arrived 1000\n",
      "timely 500\n",
      "manner 2000\n",
      "service 500\n",
      "Only 1000\n",
      "half 3500\n",
      "push 1000\n",
      "change 1000\n",
      "helping 500\n",
      "prepare 1000\n",
      "mentally 1000\n",
      "An 7000\n",
      "military 1000\n",
      "spouses 1000\n",
      "deployed 500\n",
      "While 3000\n",
      "admit 1500\n",
      "usually 2500\n",
      "type 2500\n",
      "hooked 500\n",
      "wanting 2500\n",
      "Olina 500\n",
      "strong 1000\n",
      "character 11000\n",
      "hiding 500\n",
      "behind 1000\n",
      "veil 500\n",
      "develop 500\n",
      "throughout 1500\n",
      "purchase 2500\n",
      "Drew 500\n",
      "unless 1000\n",
      "ample 500\n",
      "concept 4000\n",
      "beautifully 1000\n",
      "executed 500\n",
      "Easy 1500\n",
      "Tarot 1000\n",
      "reader 4000\n",
      "20 3000\n",
      "coloring 500\n",
      "nabbed 500\n",
      "Reed 500\n",
      "draws 500\n",
      "symbolism 500\n",
      "lovely 500\n",
      "known 2500\n",
      "chef 1000\n",
      "early 1500\n",
      "training 1000\n",
      "France 500\n",
      "follows 1500\n",
      "Eric 500\n",
      "Ripert 500\n",
      "age 1500\n",
      "six 1000\n",
      "immigrated 500\n",
      "United 500\n",
      "States 500\n",
      "In 2500\n",
      "apprehensive 500\n",
      "abundance 500\n",
      "details 3000\n",
      "slow 4000\n",
      "However 4500\n",
      "quickly 1000\n",
      "changed 500\n",
      "nail 500\n",
      "bitting 500\n",
      "less 2000\n",
      "days 2000\n",
      "suspenseful 2000\n",
      "depth 3000\n",
      "convinced 1000\n",
      "club 2500\n",
      "discussion 1500\n",
      "Marcus 1000\n",
      "fans 1000\n",
      "him 3500\n",
      "Willows 500\n",
      "savior 500\n",
      "ends 500\n",
      "leaving 1000\n",
      "long 5000\n",
      "haul 500\n",
      "FairTax 1500\n",
      "fund 500\n",
      "our 8000\n",
      "government 1000\n",
      "simple 1500\n",
      "straightforward 500\n",
      "transparent 500\n",
      "pro 4500\n",
      "America 1500\n",
      "business 1500\n",
      "employee 500\n",
      "consumer 500\n",
      "citizen 500\n",
      "pros 1000\n",
      "cons 1000\n",
      "forwarded 500\n",
      "called 1500\n",
      "critics 500\n",
      "empty 500\n",
      "agenda 1000\n",
      "based 2000\n",
      "minutia 500\n",
      "debate 1000\n",
      "concerns 500\n",
      "driven 1000\n",
      "nitpicking 500\n",
      "demagogic 500\n",
      "naysayers 500\n",
      "shamestream 500\n",
      "media 500\n",
      "shameful 500\n",
      "Shame 500\n",
      "May 1000\n",
      "God 500\n",
      "bless 500\n",
      "despite 500\n",
      "interaction 500\n",
      "between 5000\n",
      "castes 500\n",
      "League 500\n",
      "navy 500\n",
      "thoughtful 500\n",
      "wooden 500\n",
      "ships 1000\n",
      "sitting 1000\n",
      "near 1000\n",
      "tired 1000\n",
      "Characters 500\n",
      "draw 1000\n",
      "riveted 500\n",
      "Ms 2500\n",
      "Baker 500\n",
      "created 1500\n",
      "masterpiece 500\n",
      "She 8500\n",
      "leads 500\n",
      "worlds 500\n",
      "subjects 1000\n",
      "deftness 500\n",
      "artist 500\n",
      "certainly 2500\n",
      "felt 5500\n",
      "plucked 500\n",
      "chest 500\n",
      "contents 500\n",
      "poured 1000\n",
      "onto 1000\n",
      "marvelous 500\n",
      "keeper 500\n",
      "mini 500\n",
      "modern 2000\n",
      "issues 5000\n",
      "face 3500\n",
      "source 500\n",
      "any 11000\n",
      "pastor 500\n",
      "counselor 500\n",
      "latter 500\n",
      "saints 1000\n",
      "became 3000\n",
      "religion 2000\n",
      "progressive 500\n",
      "non 1000\n",
      "lofty 500\n",
      "college 1000\n",
      "educated 500\n",
      "woman 2000\n",
      "didn 14500\n",
      "burdened 500\n",
      "textbook 500\n",
      "thats 500\n",
      "Like 1500\n",
      "collaboration 500\n",
      "Jarlan 500\n",
      "Ellison 500\n",
      "Jorge 500\n",
      "Luis 500\n",
      "Borges 500\n",
      "challenged 500\n",
      "Christian 1000\n",
      "Walk 1000\n",
      "opened 2000\n",
      "came 2500\n",
      "weddings 1500\n",
      "four 2000\n",
      "Remington 500\n",
      "sons 500\n",
      "Getting 500\n",
      "guy 3000\n",
      "taken 500\n",
      "pins 1000\n",
      "needles 500\n",
      "guessed 500\n",
      "before 10000\n",
      "beat 500\n",
      "sleeve 500\n",
      "Adria 500\n",
      "precious 500\n",
      "Miss 500\n",
      "Ruby 500\n",
      "cried 1000\n",
      "couples 1000\n",
      "please 2500\n",
      "SJ 500\n",
      "McCoy 500\n",
      "thank 500\n",
      "brothers 500\n",
      "given 500\n",
      "foundation 1000\n",
      "money 8000\n",
      "differently 1000\n",
      "mindset 1000\n",
      "Managing 500\n",
      "currently 500\n",
      "earn 1000\n",
      "changes 1000\n",
      "promising 1500\n",
      "future 1000\n",
      "Thank 2500\n",
      "Mr 2500\n",
      "Banks 500\n",
      "sharing 500\n",
      "Awesome 500\n",
      "After 3000\n",
      "heels 500\n",
      "Four 500\n",
      "tons 2000\n",
      "here 2000\n",
      "ending 5500\n",
      "Tris 500\n",
      "spoilers 1000\n",
      "guess 2500\n",
      "ended 1000\n",
      "lacked 1500\n",
      "emotion 500\n",
      "went 3500\n",
      "further 2000\n",
      "Clancy 500\n",
      "entertain 500\n",
      "thrill 500\n",
      "readers 3500\n",
      "continues 500\n",
      "fantastic 1500\n",
      "three 2000\n",
      "happens 2000\n",
      "sisters 500\n",
      "leaves 2000\n",
      "wondering 2000\n",
      "seemed 6000\n",
      "drag 500\n",
      "His 1500\n",
      "identity 500\n",
      "whole 2500\n",
      "Hope 1500\n",
      "fourth 1000\n",
      "experience 1500\n",
      "delivery 500\n",
      "altogether 500\n",
      "couldn 4000\n",
      "moment 1000\n",
      "spent 2500\n",
      "too 18000\n",
      "close 2000\n",
      "Was 2000\n",
      "flowers 500\n",
      "engaging 2500\n",
      "childhood 1500\n",
      "adulthood 500\n",
      "back 6000\n",
      "forth 1000\n",
      "Alot 500\n",
      "Novel 500\n",
      "ideas 3000\n",
      "communicating 500\n",
      "ghosts 500\n",
      "Lacked 500\n",
      "draggedalittleinplaces 500\n",
      "setting 3000\n",
      "Halloween 500\n",
      "compelling 500\n",
      "reccomend 500\n",
      "visited 500\n",
      "lived 1000\n",
      "Italy 500\n",
      "isn 1000\n",
      "MKA 500\n",
      "pretty 4000\n",
      "overall 7500\n",
      "Has 2000\n",
      "turning 1000\n",
      "sappy 500\n",
      "fuzzy 500\n",
      "feeling 1000\n",
      "Some 2500\n",
      "predictable 5000\n",
      "take 2000\n",
      "large 2500\n",
      "impact 1500\n",
      "everyday 1000\n",
      "living 1000\n",
      "Will 500\n",
      "Author 500\n",
      "semi 500\n",
      "Irish 500\n",
      "immigrant 500\n",
      "families 2000\n",
      "vary 500\n",
      "widely 500\n",
      "culture 1000\n",
      "wealth 500\n",
      "learn 1000\n",
      "younger 1500\n",
      "generations 500\n",
      "marry 500\n",
      "bringing 500\n",
      "both 2000\n",
      "awkwardly 500\n",
      "At 2500\n",
      "discord 500\n",
      "wide 500\n",
      "ranging 500\n",
      "taking 1000\n",
      "place 4000\n",
      "Ireland 500\n",
      "united 500\n",
      "Loudest 1000\n",
      "Silence 1000\n",
      "screaming 500\n",
      "dynamics 500\n",
      "protagonists 500\n",
      "smiling 500\n",
      "flipping 500\n",
      "antagonists 500\n",
      "leave 3500\n",
      "reach 1000\n",
      "Well 1500\n",
      "flows 500\n",
      "extremely 2000\n",
      "fan 11500\n",
      "movies 500\n",
      "ll 4500\n",
      "audible 500\n",
      "version 4500\n",
      "myself 3000\n",
      "listen 1500\n",
      "Funny 500\n",
      "descriptive 1000\n",
      "manic 500\n",
      "depressive 500\n",
      "illness 1000\n",
      "Bipolar 500\n",
      "feels 1000\n",
      "useful 3000\n",
      "struggling 500\n",
      "diagnosis 500\n",
      "graphic 1500\n",
      "format 1500\n",
      "Arrived 500\n",
      "almost 6000\n",
      "seller 2000\n",
      "boring 5000\n",
      "specially 500\n",
      "trying 4000\n",
      "Introduction 500\n",
      "Materials 500\n",
      "Management 500\n",
      "Arnold 500\n",
      "di 500\n",
      "big 2500\n",
      "zombie 2000\n",
      "SHTF 500\n",
      "longer 2000\n",
      "tension 500\n",
      "surviving 1500\n",
      "zombies 500\n",
      "won 3000\n",
      "swear 1000\n",
      "loud 1000\n",
      "cliffhanger 1000\n",
      "Molles 500\n",
      "sir 500\n",
      "paralleled 500\n",
      "brings 1000\n",
      "present 1500\n",
      "live 2500\n",
      "unaware 500\n",
      "bad 10000\n",
      "worse 1000\n",
      "fairly 1000\n",
      "across 1000\n",
      "idea 5000\n",
      "losing 1000\n",
      "oogling 500\n",
      "girls 1000\n",
      "odd 3000\n",
      "week 1500\n",
      "during 500\n",
      "attack 1000\n",
      "Anyway 500\n",
      "storytelling 500\n",
      "sometimes 3500\n",
      "comic 500\n",
      "relief 500\n",
      "attempts 500\n",
      "corny 1500\n",
      "themes 2000\n",
      "choice 500\n",
      "stars 2000\n",
      "cheaper 1000\n",
      "digital 1000\n",
      "factual 500\n",
      "records 500\n",
      "allow 1000\n",
      "perceive 500\n",
      "steadfast 500\n",
      "concern 500\n",
      "verifiability 500\n",
      "respect 1000\n",
      "praise 500\n",
      "loaded 500\n",
      "wasn 6000\n",
      "LOVE 500\n",
      "unfamiliar 500\n",
      "Eleanor 1000\n",
      "Castile 500\n",
      "light 2500\n",
      "fron 500\n",
      "Grewhawk 1000\n",
      "Adventures 500\n",
      "Estes 500\n",
      "writers 1500\n",
      "picked 1500\n",
      "building 500\n",
      "originally 500\n",
      "hero 2000\n",
      "Gary 500\n",
      "G 500\n",
      "taught 500\n",
      "though 8000\n",
      "imagination 1000\n",
      "creativity 500\n",
      "Sorry 1000\n",
      "waxing 500\n",
      "nostalgic 500\n",
      "Master 500\n",
      "Wolf 500\n",
      "nothing 6000\n",
      "complex 1000\n",
      "regard 500\n",
      "subplots 500\n",
      "saved 1000\n",
      "normal 3000\n",
      "Parts 500\n",
      "Nox 500\n",
      "father 1000\n",
      "Oren 1500\n",
      "journey 2000\n",
      "twisted 1000\n",
      "motivated 500\n",
      "Mafia 500\n",
      "Who 1000\n",
      "fantastically 500\n",
      "discussed 500\n",
      "unputdownable 500\n",
      "Infidelity 500\n",
      "techniques 500\n",
      "month 1000\n",
      "LB 500\n",
      "ordinary 500\n",
      "centenarian 500\n",
      "unusual 1000\n",
      "young 3500\n",
      "grandson 500\n",
      "responsible 1000\n",
      "inevitable 500\n",
      "social 2000\n",
      "breakdown 500\n",
      "scarier 500\n",
      "undead 500\n",
      "store 1000\n",
      "These 1500\n",
      "Make 1000\n",
      "starts 1000\n",
      "dates 1000\n",
      "macabre 500\n",
      "Silva 1500\n",
      "Gabriel 500\n",
      "Allon 500\n",
      "Girl 500\n",
      "continuation 500\n",
      "Daniel 500\n",
      "Id 1000\n",
      "earlier 1000\n",
      "Definitely 1500\n",
      "handy 500\n",
      "resource 1000\n",
      "levels 500\n",
      "portion 1500\n",
      "cite 500\n",
      "web 1000\n",
      "resources 500\n",
      "often 3000\n",
      "forget 500\n",
      "exact 1000\n",
      "layout 500\n",
      "papers 500\n",
      "Susan 500\n",
      "M 1000\n",
      "Boyer 500\n",
      "Lowcountry 500\n",
      "Bombshell 500\n",
      "aspect 1500\n",
      "engaged 500\n",
      "dawned 500\n",
      "Liz 500\n",
      "Talbot 500\n",
      "relationship 1000\n",
      "handled 500\n",
      "moments 1000\n",
      "hilarity 500\n",
      "danger 500\n",
      "Overall 2000\n",
      "Highly 500\n",
      "recommended 2000\n",
      "wives 1000\n",
      "knew 1000\n",
      "ok 6000\n",
      "repeats 1000\n",
      "deciding 500\n",
      "item 1500\n",
      "joy 500\n",
      "toying 500\n",
      "feelings 500\n",
      "gleaned 500\n",
      "nuggets 500\n",
      "repetition 1000\n",
      "Out 500\n",
      "sounded 500\n",
      "awhile 500\n",
      "Storyline 500\n",
      "remarkable 500\n",
      "Detective 500\n",
      "dream 500\n",
      "state 500\n",
      "weekend 1000\n",
      "Right 500\n",
      "dated 500\n",
      "used 3000\n",
      "filmmaking 500\n",
      "students 1000\n",
      "Trust 500\n",
      "credit 500\n",
      "tournament 500\n",
      "wins 500\n",
      "alone 500\n",
      "El 500\n",
      "libro 1000\n",
      "llego 1000\n",
      "timpo 500\n",
      "y 1000\n",
      "su 500\n",
      "contenido 500\n",
      "es 1000\n",
      "excelente 1000\n",
      "pero 1500\n",
      "tiene 500\n",
      "una 1000\n",
      "rasgadura 500\n",
      "en 1000\n",
      "la 2000\n",
      "tapa 500\n",
      "frontal 500\n",
      "seguramente 500\n",
      "se 500\n",
      "averio 500\n",
      "duarnte 500\n",
      "el 1500\n",
      "embalaje 500\n",
      "al 500\n",
      "momento 500\n",
      "de 1000\n",
      "ser 500\n",
      "empacado 500\n",
      "contract 500\n",
      "indispensable 500\n",
      "carryout 500\n",
      "functions 1000\n",
      "Information 1000\n",
      "Technology 500\n",
      "agree 3000\n",
      "rest 2500\n",
      "minor 1000\n",
      "criticisms 1000\n",
      "ally 500\n",
      "encourage 500\n",
      "others 4500\n",
      "solid 1000\n",
      "worth 6000\n",
      "boxes 1000\n",
      "attractive 1000\n",
      "Granted 500\n",
      "call 500\n",
      "sensisitve 500\n",
      "artsy 500\n",
      "instruction 500\n",
      "detail 3500\n",
      "plain 500\n",
      "Might 500\n",
      "Heirloom 500\n",
      "neighbors 500\n",
      "bell 500\n",
      "Connor 500\n",
      "Rory 500\n",
      "riot 500\n",
      "Liked 500\n",
      "fairytale 500\n",
      "handed 500\n",
      "transports 500\n",
      "myselfand 500\n",
      "Scott 500\n",
      "O 500\n",
      "DellI 500\n",
      "try 2500\n",
      "gentle 500\n",
      "fair 1000\n",
      "reviews 3500\n",
      "Zia 500\n",
      "Island 500\n",
      "Blue 500\n",
      "Dolphins 500\n",
      "edited 1000\n",
      "Many 1000\n",
      "text 3000\n",
      "passages 1000\n",
      "redundant 500\n",
      "suddenly 1500\n",
      "skips 500\n",
      "ahead 1500\n",
      "unrelated 500\n",
      "Nonetheless 500\n",
      "Native 500\n",
      "Californians 500\n",
      "tenuous 500\n",
      "coexistence 500\n",
      "Spanish 1500\n",
      "missionaries 500\n",
      "hopeful 1000\n",
      "portrait 500\n",
      "dangers 500\n",
      "growing 500\n",
      "instantly 500\n",
      "continuously 500\n",
      "lives 1500\n",
      "relate 500\n",
      "overview 1000\n",
      "travelling 500\n",
      "fly 500\n",
      "fisher 500\n",
      "practicing 500\n",
      "tactical 500\n",
      "motifs 500\n",
      "personally 2000\n",
      "prefer 500\n",
      "CT 500\n",
      "Art 500\n",
      "seem 1000\n",
      "practical 1500\n",
      "Or 500\n",
      "encyclopedia 500\n",
      "chess 500\n",
      "combinations 500\n",
      "departure 500\n",
      "previous 1500\n",
      "Jonathon 500\n",
      "Blair 1000\n",
      "Arkady 500\n",
      "Renko 500\n",
      "Rose 500\n",
      "crafted 500\n",
      "Victorian 500\n",
      "Wigan 500\n",
      "intrigues 500\n",
      "Northern 500\n",
      "industrialists 500\n",
      "tales 500\n",
      "son 2000\n",
      "docked 500\n",
      "explaining 1000\n",
      "potty 500\n",
      "Smaller 500\n",
      "Interesting 1500\n",
      "documenting 500\n",
      "piece 1000\n",
      "ongoing 500\n",
      "struggle 1000\n",
      "gender 500\n",
      "equality 500\n",
      "grew 1000\n",
      "hearing 1000\n",
      "Women 500\n",
      "Lib 500\n",
      "abstract 1000\n",
      "disparaging 500\n",
      "firsthand 500\n",
      "lawsuit 500\n",
      "itself 500\n",
      "kinds 500\n",
      "protests 500\n",
      "New 1000\n",
      "York 1000\n",
      "U 500\n",
      "S 1000\n",
      "normally 500\n",
      "wrote 2000\n",
      "lacking 1500\n",
      "rough 500\n",
      "edges 500\n",
      "absolutely 2500\n",
      "adored 500\n",
      "third 1500\n",
      "emotional 500\n",
      "roller 500\n",
      "coaster 500\n",
      "edge 500\n",
      "seat 500\n",
      "Sarah 500\n",
      "throne 500\n",
      "glass 500\n",
      "cant 500\n",
      "dc 500\n",
      "icon 500\n",
      "catwoman 500\n",
      "purchasing 500\n",
      "hedgehog 500\n",
      "Set 500\n",
      "sections 500\n",
      "villain 500\n",
      "brother 500\n",
      "friend 1500\n",
      "total 1000\n",
      "surprise 500\n",
      "reminds 1000\n",
      "Conan 500\n",
      "Doyle 500\n",
      "Shelock 500\n",
      "Holmes 500\n",
      "brilliant 500\n",
      "unorthodox 500\n",
      "investigator 500\n",
      "faithful 500\n",
      "assistant 500\n",
      "50 2500\n",
      "struck 500\n",
      "Hollywood 500\n",
      "recently 500\n",
      "Anne 500\n",
      "Carson 500\n",
      "amazing 2000\n",
      "Greek 1500\n",
      "mythology 1000\n",
      "approach 1000\n",
      "poem 500\n",
      "Lee 1500\n",
      "Child 500\n",
      "thinks 500\n",
      "Reacher 500\n",
      "real 3500\n",
      "grasp 1000\n",
      "current 1500\n",
      "spy 1000\n",
      "HIs 500\n",
      "beleivable 500\n",
      "typically 500\n",
      "Incredibly 500\n",
      "helpful 5000\n",
      "organic 500\n",
      "reactions 1000\n",
      "professor 1000\n",
      "homework 1000\n",
      "Cute 1000\n",
      "Thought 1500\n",
      "relatable 500\n",
      "Quick 1000\n",
      "haning 500\n",
      "beach 1000\n",
      "pool 1000\n",
      "drama 1500\n",
      "adds 500\n",
      "delicious 500\n",
      "homey 500\n",
      "authentic 500\n",
      "small 2500\n",
      "town 1000\n",
      "Mitford 500\n",
      "Fun 500\n",
      "profound 500\n",
      "humor 1000\n",
      "facinating 500\n",
      "mathematics 500\n",
      "economic 500\n",
      "development 2500\n",
      "wound 500\n",
      "schizophrenic 500\n",
      "irritating 500\n",
      "genius 500\n",
      "John 2000\n",
      "Nash 500\n",
      "30 500\n",
      "year 2000\n",
      "colleagues 500\n",
      "insights 500\n",
      "eventually 500\n",
      "tough 2000\n",
      "tlmes 500\n",
      "missing 3500\n",
      "link 2500\n",
      "graduate 1000\n",
      "today 1500\n",
      "Ph 500\n",
      "professionalization 500\n",
      "rituals 500\n",
      "nowadays 1000\n",
      "passion 500\n",
      "politics 500\n",
      "scholarship 500\n",
      "either 2500\n",
      "corrects 500\n",
      "anomaly 500\n",
      "extent 500\n",
      "required 2500\n",
      "student 500\n",
      "political 500\n",
      "comparativists 500\n",
      "earth 500\n",
      "standards 500\n",
      "thorough 500\n",
      "decent 2000\n",
      "CDs 500\n",
      "o 1000\n",
      "cleaned 500\n",
      "destroyed 500\n",
      "thyey 500\n",
      "ar 500\n",
      "playable 500\n",
      "discs 500\n",
      "Woods 500\n",
      "historically 500\n",
      "accurate 1000\n",
      "William 1000\n",
      "conquest 500\n",
      "England 500\n",
      "consistently 500\n",
      "portrayed 500\n",
      "entertaining 1500\n",
      "illustrations 2000\n",
      "handful 500\n",
      "feature 500\n",
      "interracial 500\n",
      "biracial 500\n",
      "child 1500\n",
      "professionals 500\n",
      "races 500\n",
      "Start 500\n",
      "happening 1000\n",
      "Dealing 500\n",
      "racism 1000\n",
      "shifter 500\n",
      "accepted 500\n",
      "scenes 1000\n",
      "Strong 500\n",
      "female 2000\n",
      "lead 1500\n",
      "secondary 500\n",
      "loving 500\n",
      "rich 500\n",
      "lumpy 500\n",
      "melting 500\n",
      "pot 500\n",
      "City 500\n",
      "might 5500\n",
      "creatures 500\n",
      "existed 500\n",
      "context 1000\n",
      "places 1000\n",
      "held 500\n",
      "Dortmunder 500\n",
      "crew 1000\n",
      "tv 500\n",
      "executing 500\n",
      "heist 500\n",
      "according 1000\n",
      "plan 2000\n",
      "Typical 500\n",
      "entire 1000\n",
      "dose 500\n",
      "suspension 500\n",
      "disbelief 500\n",
      "absolute 1000\n",
      "hoot 500\n",
      "probably 4000\n",
      "funniest 500\n",
      "subplot 500\n",
      "scathingly 500\n",
      "rips 500\n",
      "surreal 500\n",
      "television 500\n",
      "5 1500\n",
      "2012 500\n",
      "Dave 500\n",
      "Barry 500\n",
      "pushing 1000\n",
      "limits 500\n",
      "hold 1000\n",
      "bar 500\n",
      "comedy 500\n",
      "satire 1000\n",
      "polar 500\n",
      "opposite 1500\n",
      "Jews 1500\n",
      "insane 500\n",
      "impossible 1500\n",
      "From 1000\n",
      "Terrorists 500\n",
      "mere 500\n",
      "accident 500\n",
      "blindly 500\n",
      "saving 1000\n",
      "multiple 500\n",
      "countries 1000\n",
      "severe 500\n",
      "bowel 500\n",
      "solve 500\n",
      "crisis 500\n",
      "asinine 500\n",
      "moronic 500\n",
      "belly 500\n",
      "wrenching 1000\n",
      "Received 500\n",
      "Goodreads 500\n",
      "helped 1500\n",
      "73 500\n",
      "chevy 500\n",
      "cheyenne 500\n",
      "super 1500\n",
      "restoration 500\n",
      "help 3500\n",
      "body 500\n",
      "smillar 500\n",
      "truck 500\n",
      "raid 500\n",
      "preparation 1000\n",
      "Trying 500\n",
      "movie 2000\n",
      "saw 2500\n",
      "kid 500\n",
      "adorable 500\n",
      "baby 2000\n",
      "niece 500\n",
      "multilingual 500\n",
      "finding 1000\n",
      "languages 1000\n",
      "perfect 1500\n",
      "parents 1500\n",
      "introduced 500\n",
      "Secret 500\n",
      "Heaven 500\n",
      "Aiden 500\n",
      "fianc 500\n",
      "Miriam 500\n",
      "search 1000\n",
      "Lost 500\n",
      "Bible 500\n",
      "clock 500\n",
      "unfolds 500\n",
      "plenty 500\n",
      "layers 500\n",
      "exciting 1500\n",
      "mouse 500\n",
      "cookie 500\n",
      "basically 1000\n",
      "sick 1500\n",
      "lesbian 1000\n",
      "sex 4000\n",
      "amount 2500\n",
      "although 500\n",
      "formulaic 500\n",
      "mundane 500\n",
      "But 8500\n",
      "inclined 500\n",
      "Heres 500\n",
      "ahhh 500\n",
      "ooooowww 500\n",
      "imagine 500\n",
      "where 3000\n",
      "causing 500\n",
      "potential 2000\n",
      "polishing 500\n",
      "mindfulness 1000\n",
      "introduces 500\n",
      "observing 500\n",
      "approachable 500\n",
      "assuring 500\n",
      "pay 3500\n",
      "attention 3500\n",
      "whats 500\n",
      "By 1000\n",
      "simply 1000\n",
      "noticing 500\n",
      "lifes 500\n",
      "challenges 500\n",
      "enjoying 500\n",
      "birds 500\n",
      "artists 500\n",
      "bird 500\n",
      "enthusiasts 500\n",
      "barred 500\n",
      "7Th 500\n",
      "Cav 500\n",
      "Vietnam 500\n",
      "survived 500\n",
      "ambush 500\n",
      "Ist 500\n",
      "Battle 500\n",
      "Ia 500\n",
      "Drang 500\n",
      "Were 500\n",
      "Soldiers 500\n",
      "Once 1500\n",
      "Young 1000\n",
      "Butcher 500\n",
      "Honor 500\n",
      "Harrington 500\n",
      "mayhem 500\n",
      "seems 2500\n",
      "Bad 500\n",
      "grammar 2000\n",
      "Consistent 500\n",
      "instead 3000\n",
      "example 2500\n",
      "Using 500\n",
      "spell 1000\n",
      "check 1500\n",
      "option 500\n",
      "computer 500\n",
      "frustrating 500\n",
      "terrible 4500\n",
      "habit 500\n",
      "Maybe 2500\n",
      "absurdly 500\n",
      "wealthy 500\n",
      "Pretty 1500\n",
      "storyline 2000\n",
      "flow 500\n",
      "island 1000\n",
      "Shel 500\n",
      "hysterical 500\n",
      "erred 500\n",
      "selection 500\n",
      "aid 500\n",
      "transitioning 500\n",
      "Windows 1000\n",
      "XP 500\n",
      "desktop 500\n",
      "8 1500\n",
      "touch 500\n",
      "screen 500\n",
      "however 3500\n",
      "fine 1500\n",
      "expected 4000\n",
      "GREAT 1000\n",
      "covering 1000\n",
      "Roman 500\n",
      "myths 1000\n",
      "Norse 1000\n",
      "sadly 1000\n",
      "explanations 1500\n",
      "Seemed 500\n",
      "assume 1000\n",
      "Needs 500\n",
      "divided 500\n",
      "proceeds 500\n",
      "answer 1500\n",
      "repeated 1000\n",
      "softball 500\n",
      "rights 1000\n",
      "Castro 500\n",
      "regime 500\n",
      "neither 500\n",
      "ask 1000\n",
      "concerning 500\n",
      "Communism 500\n",
      "asking 1000\n",
      "Cuban 500\n",
      "arts 500\n",
      "Is 1000\n",
      "Cuba 500\n",
      "Tim 500\n",
      "speak 1000\n",
      "Zombie 500\n",
      "Fallout 500\n",
      "Books 500\n",
      "Riley 500\n",
      "aforementioned 500\n",
      "Mark 500\n",
      "Tufo 500\n",
      "Japanese 500\n",
      "description 3500\n",
      "Too 3000\n",
      "general 1500\n",
      "mathematical 500\n",
      "foundations 500\n",
      "computation 500\n",
      "material 2500\n",
      "theoretical 500\n",
      "20th 500\n",
      "century 500\n",
      "transferred 500\n",
      "working 1000\n",
      "machines 500\n",
      "summer 500\n",
      "complicated 1000\n",
      "Told 500\n",
      "point 3000\n",
      "view 2500\n",
      "main 5000\n",
      "killer 1000\n",
      "im 500\n",
      "passing 500\n",
      "reviewer 500\n",
      "directly 500\n",
      "below 500\n",
      "stark 500\n",
      "conjured 500\n",
      "mind 3000\n",
      "Wood 500\n",
      "researched 1000\n",
      "painted 500\n",
      "leaf 500\n",
      "imagined 500\n",
      "Robinton 500\n",
      "lokked 500\n",
      "Likely 500\n",
      "sever 500\n",
      "making 1000\n",
      "regular 1500\n",
      "biography 500\n",
      "comedienne 500\n",
      "tried 1000\n",
      "unmoved 500\n",
      "Bossypants 500\n",
      "Tina 500\n",
      "Fey 500\n",
      "slight 500\n",
      "knee 500\n",
      "run 2500\n",
      "regularly 500\n",
      "yoga 500\n",
      "per 1000\n",
      "vegetarian 500\n",
      "eater 500\n",
      "Diana 500\n",
      "basic 2000\n",
      "stress 500\n",
      "spends 1500\n",
      "talking 1500\n",
      "himself 1500\n",
      "subtly 500\n",
      "products 500\n",
      "website 1000\n",
      "lowered 500\n",
      "rating 1000\n",
      "reason 500\n",
      "willing 1000\n",
      "wade 1000\n",
      "numerous 1000\n",
      "NFL 500\n",
      "resist 500\n",
      "product 2000\n",
      "promotion 1000\n",
      "Care 1500\n",
      "include 1000\n",
      "alot 2500\n",
      "Nursing 2000\n",
      "Plans 1000\n",
      "Diagnosis 500\n",
      "Intervention 500\n",
      "Gulanick 500\n",
      "Judith 500\n",
      "L 1500\n",
      "Myers 500\n",
      "shoved 1000\n",
      "aside 1000\n",
      "listed 1000\n",
      "above 500\n",
      "summary 500\n",
      "gotten 1500\n",
      "Instead 1000\n",
      "dinos 500\n",
      "Are 500\n",
      "DNA 500\n",
      "scared 500\n",
      "Case 1000\n",
      "Bamford 500\n",
      "Puzzle 500\n",
      "Palace 500\n",
      "NSA 500\n",
      "Washington 1000\n",
      "Post 1000\n",
      "notable 500\n",
      "centers 500\n",
      "reporter 500\n",
      "Do 1000\n",
      "win 500\n",
      "prize 500\n",
      "Eh 1000\n",
      "cliches 1000\n",
      "focus 500\n",
      "Olympic 500\n",
      "athletes 500\n",
      "notably 500\n",
      "stereotypically 500\n",
      "feminine 500\n",
      "appearance 500\n",
      "Could 2500\n",
      "sucked 1000\n",
      "pt2 500\n",
      "wink 500\n",
      "Manhattan 500\n",
      "remarkably 500\n",
      "similar 1500\n",
      "Rothman 500\n",
      "nbsp 500\n",
      "data 500\n",
      "hook 500\n",
      "linked 500\n",
      "href 500\n",
      "Hello 1000\n",
      "Illustrated 1000\n",
      "Letter 1000\n",
      "Five 1000\n",
      "Boroughs 1000\n",
      "dp 500\n",
      "1452109842 500\n",
      "ref 500\n",
      "cm_cr_arp_d_rvw_txt 500\n",
      "ie 500\n",
      "UTF8 500\n",
      "Diners 500\n",
      "extinct 500\n",
      "hawks 500\n",
      "aren 500\n",
      "Uber 500\n",
      "outnumber 500\n",
      "taxis 500\n",
      "GENERAL 500\n",
      "blocks 1000\n",
      "avenue 500\n",
      "7 1500\n",
      "avenues 500\n",
      "mile 500\n",
      "crosstown 500\n",
      "rated 500\n",
      "prosecution 500\n",
      "viewpoint 500\n",
      "Marcia 500\n",
      "Clark 500\n",
      "turn 2000\n",
      "mountains 500\n",
      "evidence 500\n",
      "mapped 500\n",
      "convict 500\n",
      "errors 5000\n",
      "Judge 500\n",
      "Ito 500\n",
      "allowed 500\n",
      "insolence 500\n",
      "difficult 1500\n",
      "curious 500\n",
      "perceived 500\n",
      "law 500\n",
      "schools 500\n",
      "analyze 500\n",
      "photographing 500\n",
      "sell 1500\n",
      "Learn 500\n",
      "photogs 500\n",
      "posing 500\n",
      "guide 1500\n",
      "traditional 500\n",
      "poses 500\n",
      "fashion 500\n",
      "todays 500\n",
      "bride 500\n",
      "award 500\n",
      "winning 500\n",
      "wedding 500\n",
      "photographers 500\n",
      "google 500\n",
      "online 1500\n",
      "sources 500\n",
      "Devil 500\n",
      "Brood 500\n",
      "courtship 500\n",
      "HEAs 500\n",
      "Laurens 500\n",
      "fleshes 500\n",
      "coming 1000\n",
      "Sebastian 500\n",
      "siblings 500\n",
      "yarn 500\n",
      "weaknesses 500\n",
      "structure 1000\n",
      "defined 1000\n",
      "ans 500\n",
      "behaviors 500\n",
      "correspond 1500\n",
      "personalities 500\n",
      "Bret 500\n",
      "Easton 500\n",
      "Ellis 500\n",
      "coldness 500\n",
      "violence 1500\n",
      "reflects 500\n",
      "societies 500\n",
      "lie 500\n",
      "watched 500\n",
      "seasons 500\n",
      "OITNB 500\n",
      "bored 1000\n",
      "lost 2000\n",
      "Piper 500\n",
      "took 3500\n",
      "forced 1000\n",
      "points 1000\n",
      "Calorie 500\n",
      "nutrition 500\n",
      "Trivialization 500\n",
      "criminality 500\n",
      "opining 500\n",
      "penal 500\n",
      "wrongs 500\n",
      "self 3000\n",
      "serving 1500\n",
      "PLUS 500\n",
      "guided 1000\n",
      "meditation 1000\n",
      "No 2500\n",
      "included 1000\n",
      "Kornfield 1000\n",
      "cheated 500\n",
      "tje 500\n",
      "Still 1500\n",
      "int 500\n",
      "Buddhist 500\n",
      "arena 500\n",
      "totally 1500\n",
      "Though 500\n",
      "Again 500\n",
      "Hardbound 500\n",
      "scavenger 500\n",
      "hunt 500\n",
      "toddler 500\n",
      "fell 1500\n",
      "apart 1500\n",
      "hours 500\n",
      "receiving 500\n",
      "board 2000\n",
      "pieces 1500\n",
      "jack 500\n",
      "mcclure 500\n",
      "fighting 500\n",
      "intrigue 500\n",
      "drawback 500\n",
      "preaching 500\n",
      "straw 1000\n",
      "dummies 1000\n",
      "burning 500\n",
      "easiily 500\n",
      "ad 500\n",
      "terminology 500\n",
      "waste 7000\n",
      "charting 500\n",
      "death 500\n",
      "Titanic 500\n",
      "shorter 500\n",
      "skimming 500\n",
      "surface 1000\n",
      "tragedy 1000\n",
      "concentrates 1000\n",
      "principally 500\n",
      "course 3000\n",
      "hoped 500\n",
      "construction 1000\n",
      "ship 1000\n",
      "sinking 500\n",
      "particularly 1500\n",
      "constructed 500\n",
      "failed 500\n",
      "prevent 500\n",
      "sets 500\n",
      "Brandon 500\n",
      "accompany 500\n",
      "brief 1500\n",
      "Central 500\n",
      "Park 1000\n",
      "caps 500\n",
      "placed 500\n",
      "photo 500\n",
      "looked 1500\n",
      "similarities 500\n",
      "overcome 500\n",
      "LOVED 500\n",
      "fitting 500\n",
      "reminded 500\n",
      "adult 1000\n",
      "immature 500\n",
      "convenient 500\n",
      "devices 500\n",
      "somewhat 3000\n",
      "dull 1000\n",
      "THE 2500\n",
      "SMALL 500\n",
      "PHOTOGRAPHS 500\n",
      "ARE 1000\n",
      "NOT 2500\n",
      "POSSIBLE 500\n",
      "TO 1000\n",
      "SEE 500\n",
      "DETAILS 500\n",
      "OF 1000\n",
      "TECHNIQUE 500\n",
      "IT 1000\n",
      "SEEMS 500\n",
      "THAT 500\n",
      "TECHNIQUES 1000\n",
      "DIFFICULT 500\n",
      "APPLICATION 500\n",
      "MORE 500\n",
      "GOOD 500\n",
      "WILL 1000\n",
      "COUNT 500\n",
      "IMPORTANT 500\n",
      "Textbook 500\n",
      "principal 500\n",
      "certification 500\n",
      "aspects 500\n",
      "Brenda 500\n",
      "Hampton 500\n",
      "short 2500\n",
      "captivate 500\n",
      "Alaska 500\n",
      "necessary 1000\n",
      "lessons 1000\n",
      "Depending 500\n",
      "skills 500\n",
      "Okay 1000\n",
      "flat 1500\n",
      "cerebral 500\n",
      "execution 500\n",
      "slog 500\n",
      "artistry 500\n",
      "crazy 500\n",
      "realllly 500\n",
      "daring 500\n",
      "hairstyles 500\n",
      "teased 500\n",
      "feathers 500\n",
      "sticking 1000\n",
      "chopsticks 500\n",
      "overload 500\n",
      "porcupine 500\n",
      "hairstyle 500\n",
      "inspiration 500\n",
      "perhaps 1500\n",
      "wild 500\n",
      "runway 500\n",
      "looks 1000\n",
      "attempt 1000\n",
      "wear 500\n",
      "styles 1000\n",
      "public 500\n",
      "caught 500\n",
      "privileged 500\n",
      "Hmmm 500\n",
      "correcting 500\n",
      "wrong 3000\n",
      "missed 2000\n",
      "annoying 3000\n",
      "carry 500\n",
      "oh 500\n",
      "definately 500\n",
      "frequent 500\n",
      "Nigel 500\n",
      "cooks 500\n",
      "meal 500\n",
      "Over 500\n",
      "verbage 500\n",
      "Misuse 500\n",
      "hanger 500\n",
      "hangar 500\n",
      "Polish 500\n",
      "may 1000\n",
      "indeed 500\n",
      "pick 500\n",
      "appt 500\n",
      "whatever 500\n",
      "areas 1000\n",
      "typical 1000\n",
      "Dont 500\n",
      "constant 500\n",
      "repetitiveness 500\n",
      "ARC 500\n",
      "un 500\n",
      "bias 500\n",
      "Initially 500\n",
      "follow 3000\n",
      "jumped 500\n",
      "Eventually 500\n",
      "toward 1000\n",
      "binder 1000\n",
      "anticipated 500\n",
      "drawn 2000\n",
      "skimmed 500\n",
      "thick 1000\n",
      "Kept 500\n",
      "Even 500\n",
      "lucky 1000\n",
      "freedom 500\n",
      "Most 1500\n",
      "contributing 500\n",
      "commercial 1000\n",
      "fishing 500\n",
      "touchy 500\n",
      "feely 500\n",
      "taste 2000\n",
      "quality 5000\n",
      "expecting 2000\n",
      "learnt 500\n",
      "gardening 500\n",
      "organised 500\n",
      "uses 1000\n",
      "standing 500\n",
      "specifically 500\n",
      "superhero 500\n",
      "Then 1500\n",
      "bedroom 1500\n",
      "blurb 500\n",
      "advertises 500\n",
      "Grace 500\n",
      "Beth 500\n",
      "sister 500\n",
      "machinations 500\n",
      "schemes 500\n",
      "battles 1000\n",
      "stalker 500\n",
      "Gizmo 500\n",
      "ex 500\n",
      "therapy 1500\n",
      "dog 1500\n",
      "Being 500\n",
      "Hank 2000\n",
      "Williams 2500\n",
      "Although 1500\n",
      "Country 500\n",
      "Music 500\n",
      "greatest 500\n",
      "average 1000\n",
      "tying 500\n",
      "typed 500\n",
      "Jett 500\n",
      "Jr 500\n",
      "Masters 500\n",
      "Administration 500\n",
      "Actually 500\n",
      "broke 500\n",
      "topic 500\n",
      "raises 500\n",
      "nursing 500\n",
      "program 500\n",
      "under 1000\n",
      "grad 1000\n",
      "post 2000\n",
      "entertained 500\n",
      "seeing 500\n",
      "play 1500\n",
      "invested 1000\n",
      "remainder 500\n",
      "4 2000\n",
      "requirse 500\n",
      "stretch 500\n",
      "encourages 500\n",
      "alternative 1000\n",
      "masculine 500\n",
      "ruled 500\n",
      "stand 1500\n",
      "question 500\n",
      "widow 500\n",
      "intelligence 500\n",
      "skewed 500\n",
      "towards 1500\n",
      "Change 1000\n",
      "evolutionary 500\n",
      "revolutions 500\n",
      "setup 500\n",
      "thriller 500\n",
      "okay 3500\n",
      "Nothing 500\n",
      "Dylan 500\n",
      "tends 500\n",
      "repeat 500\n",
      "Hadju 500\n",
      "Jane 500\n",
      "Graves 500\n",
      "Heart 500\n",
      "strings 500\n",
      "Diamond 500\n",
      "Rings 500\n",
      "filler 500\n",
      "add 1500\n",
      "depressing 2000\n",
      "carried 500\n",
      "shadow 500\n",
      "wrapped 500\n",
      "hate 2000\n",
      "daughter 1000\n",
      "heartbreaking 500\n",
      "issue 500\n",
      "MC 500\n",
      "oblivious 500\n",
      "selfish 1000\n",
      "blamed 500\n",
      "listened 500\n",
      "ir 500\n",
      "Why 3000\n",
      "besides 1000\n",
      "IM 500\n",
      "shot 1000\n",
      "Haldol 500\n",
      "Ativan 500\n",
      "works 1000\n",
      "15 500\n",
      "seconds 500\n",
      "MD 500\n",
      "editor 1000\n",
      "inconsistent 500\n",
      "On 1000\n",
      "Peter 1000\n",
      "rode 500\n",
      "name 2000\n",
      "Patrick 500\n",
      "says 1000\n",
      "Alex 500\n",
      "Yes 500\n",
      "detract 500\n",
      "sloppy 500\n",
      "Bartlett 1000\n",
      "Jeff 1000\n",
      "Resnick 500\n",
      "Short 500\n",
      "milieu 500\n",
      "Stick 500\n",
      "novels 3000\n",
      "TV 500\n",
      "programs 500\n",
      "infant 500\n",
      "puts 500\n",
      "pall 500\n",
      "move 500\n",
      "saddened 500\n",
      "grabbed 500\n",
      "Niall 500\n",
      "cooperative 500\n",
      "backbone 500\n",
      "dealing 500\n",
      "trauma 500\n",
      "Vic 500\n",
      "grammatical 1500\n",
      "typos 1500\n",
      "moves 1500\n",
      "dead 500\n",
      "spots 1000\n",
      "advance 500\n",
      "warning 1500\n",
      "needing 500\n",
      "Mongol 1000\n",
      "shamanism 1500\n",
      "reasonable 500\n",
      "degree 500\n",
      "covered 1000\n",
      "deficient 1000\n",
      "BEFORE 500\n",
      "Russians 500\n",
      "Chinese 500\n",
      "tore 500\n",
      "country 1000\n",
      "forcibly 500\n",
      "converted 500\n",
      "Perhaps 1500\n",
      "knowledge 500\n",
      "thoroughly 1000\n",
      "having 2000\n",
      "breaking 500\n",
      "professors 500\n",
      "lol 500\n",
      "fantasy 500\n",
      "spelling 1000\n",
      "distraction 500\n",
      "Ebooks 500\n",
      "same 5000\n",
      "editorial 1000\n",
      "oversight 500\n",
      "printed 1000\n",
      "Decent 500\n",
      "Kinda 1000\n",
      "lame 1000\n",
      "cool 500\n",
      "Ok 500\n",
      "Anticlimactic 500\n",
      "Ready 500\n",
      "Player 500\n",
      "level 3000\n",
      "expectations 500\n",
      "compared 500\n",
      "combo 500\n",
      "sibling 500\n",
      "moving 500\n",
      "stay 1500\n",
      "middle 500\n",
      "Made 1000\n",
      "side 1000\n",
      "harder 500\n",
      "Supposed 500\n",
      "attached 500\n",
      "inside 3000\n",
      "recall 500\n",
      "stated 1000\n",
      "Wee 500\n",
      "discusses 500\n",
      "colonialism 1000\n",
      "Peru 500\n",
      "Marxist 500\n",
      "propaganda 500\n",
      "criticism 500\n",
      "Latin 500\n",
      "theses 500\n",
      "Stern 500\n",
      "supporting 500\n",
      "argument 500\n",
      "referred 1000\n",
      "edition 2500\n",
      "moth 500\n",
      "marginal 500\n",
      "speaker 500\n",
      "uninteresting 500\n",
      "Naw 500\n",
      "hyped 500\n",
      "fairness 500\n",
      "J 500\n",
      "K 500\n",
      "Clumsy 500\n",
      "contrived 1000\n",
      "cartoon 500\n",
      "abandoned 500\n",
      "mess 1500\n",
      "Chapter 1500\n",
      "gem 500\n",
      "badly 1500\n",
      "mostly 1500\n",
      "require 1500\n",
      "remodeling 500\n",
      "designing 500\n",
      "blue 500\n",
      "prints 500\n",
      "decorating 500\n",
      "geared 500\n",
      "architect 500\n",
      "wall 1000\n",
      "loft 500\n",
      "split 500\n",
      "rooms 500\n",
      "rarely 1000\n",
      "square 500\n",
      "floor 500\n",
      "plans 500\n",
      "SO 500\n",
      "writings 500\n",
      "believe 2000\n",
      "roots 1000\n",
      "accomplished 500\n",
      "follower 500\n",
      "began 500\n",
      "recruit 500\n",
      "painfully 500\n",
      "excruciatingly 500\n",
      "tedious 500\n",
      "flesh 500\n",
      "fault 1500\n",
      "figured 500\n",
      "least 3000\n",
      "Sadly 1500\n",
      "requires 500\n",
      "activity 1000\n",
      "activies 500\n",
      "materials 1000\n",
      "order 2000\n",
      "effective 500\n",
      "classroom 500\n",
      "Found 1000\n",
      "unable 1000\n",
      "poorly 2000\n",
      "presented 1000\n",
      "bland 1000\n",
      "dribble 500\n",
      "initiate 500\n",
      "maintain 500\n",
      "conversation 500\n",
      "stranger 500\n",
      "largely 500\n",
      "talk 1500\n",
      "substantive 500\n",
      "HOW 500\n",
      "bully 1000\n",
      "thru 500\n",
      "said 3000\n",
      "consumed 500\n",
      "Did 3500\n",
      "unbelievable 1500\n",
      "fetched 500\n",
      "realistic 1500\n",
      "finale 500\n",
      "psychopathic 500\n",
      "overplayed 500\n",
      "tiring 500\n",
      "fill 500\n",
      "space 1000\n",
      "overdone 500\n",
      "disgusting 500\n",
      "disappointment 1000\n",
      "unlikable 500\n",
      "Skipping 500\n",
      "Certainly 1000\n",
      "treated 500\n",
      "Amazon 2000\n",
      "kindle 2000\n",
      "hits 500\n",
      "trash 1000\n",
      "interactive 500\n",
      "flaps 1500\n",
      "sturdier 500\n",
      "bends 500\n",
      "librarian 500\n",
      "shown 500\n",
      "mindful 500\n",
      "tearing 500\n",
      "weren 1500\n",
      "designed 500\n",
      "Sabin 500\n",
      "gay 500\n",
      "suppose 1000\n",
      "ladies 500\n",
      "annoyed 500\n",
      "decide 1000\n",
      "personality 1500\n",
      "unrealistic 1000\n",
      "substances 500\n",
      "cussed 500\n",
      "norm 500\n",
      "conversations 500\n",
      "hadn 1000\n",
      "paid 2000\n",
      "mixes 500\n",
      "examples 1500\n",
      "means 500\n",
      "cases 1500\n",
      "understandable 500\n",
      "Absolutley 500\n",
      "Princeton 500\n",
      "AP 500\n",
      "riddled 500\n",
      "aggrivating 500\n",
      "actual 1000\n",
      "content 1500\n",
      "alright 500\n",
      "tests 500\n",
      "bother 1000\n",
      "envelopes 500\n",
      "adequate 500\n",
      "glue 500\n",
      "closed 500\n",
      "mail 500\n",
      "reported 500\n",
      "recipients 500\n",
      "Few 500\n",
      "write 2000\n",
      "Jacobs 1000\n",
      "urban 500\n",
      "takes 500\n",
      "tosses 500\n",
      "scientific 500\n",
      "vat 500\n",
      "spurts 500\n",
      "gets 1000\n",
      "track 500\n",
      "Toronto 500\n",
      "function 500\n",
      "universal 500\n",
      "model 500\n",
      "minute 1000\n",
      "room 1000\n",
      "caged 500\n",
      "animals 500\n",
      "walking 500\n",
      "outside 500\n",
      "easier 1000\n",
      "overlook 500\n",
      "Flowers 500\n",
      "Cannibals 500\n",
      "makeover 500\n",
      "afterward 500\n",
      "indulgent 500\n",
      "excited 500\n",
      "preview 1000\n",
      "wow 500\n",
      "blown 500\n",
      "smh 500\n",
      "worked 2500\n",
      "bring 500\n",
      "borders 500\n",
      "ridiculous 2500\n",
      "grace 500\n",
      "Amazaon 500\n",
      "error 1000\n",
      "address 500\n",
      "holidays 500\n",
      "audio 500\n",
      "ten 1000\n",
      "narration 500\n",
      "continuous 500\n",
      "convince 500\n",
      "shewwwwwwwwieeeeeeeeee 500\n",
      "Civil 1000\n",
      "War 1000\n",
      "technology 500\n",
      "breakthroughs 500\n",
      "Suitable 500\n",
      "eigth 500\n",
      "grade 1500\n",
      "HS 500\n",
      "yeah 500\n",
      "Edgar 500\n",
      "Must 500\n",
      "Free 1000\n",
      "Tibet 1000\n",
      "vote 500\n",
      "committee 500\n",
      "bleh 500\n",
      "drags 500\n",
      "grabs 500\n",
      "gushed 500\n",
      "middling 500\n",
      "coincidences 500\n",
      "Shan 500\n",
      "obscure 500\n",
      "treatment 1000\n",
      "conjunction 500\n",
      "exhibit 500\n",
      "Lack 500\n",
      "clarity 500\n",
      "Overwhelming 500\n",
      "Java 3500\n",
      "beginners 500\n",
      "experienced 500\n",
      "programmer 500\n",
      "reference 1000\n",
      "manual 500\n",
      "Android 500\n",
      "programming 500\n",
      "WHY 500\n",
      "ANDROID 500\n",
      "DEVELOPMENT 500\n",
      "IN 500\n",
      "TITLE 500\n",
      "Buy 500\n",
      "flavor 500\n",
      "bible 1000\n",
      "save 500\n",
      "eat 500\n",
      "meat 1000\n",
      "later 500\n",
      "vital 500\n",
      "steps 1000\n",
      "files 500\n",
      "imported 500\n",
      "independent 500\n",
      "note 500\n",
      "skipped 500\n",
      "erratum 500\n",
      "frustrated 500\n",
      "Contradiction 500\n",
      "Solitude 500\n",
      "secret 1000\n",
      "realize 1000\n",
      "knowing 500\n",
      "With 1000\n",
      "Because 1000\n",
      "Munchy 500\n",
      "mosquito 500\n",
      "drinking 500\n",
      "blood 1500\n",
      "ate 1000\n",
      "sugar 500\n",
      "water 1000\n",
      "Rattles 500\n",
      "snake 500\n",
      "Wiggle 500\n",
      "Mac 500\n",
      "apple 500\n",
      "k 500\n",
      "expect 500\n",
      "closure 500\n",
      "left 2000\n",
      "advertisement 1000\n",
      "Frustrating 500\n",
      "somewhere 500\n",
      "count 1500\n",
      "weak 2000\n",
      "horrid 500\n",
      "whoever 500\n",
      "Similar 500\n",
      "album 500\n",
      "Dollar 500\n",
      "Tree 500\n",
      "tad 500\n",
      "played 500\n",
      "skinny 500\n",
      "Evidently 500\n",
      "vault 500\n",
      "Salinger 500\n",
      "ranks 1000\n",
      "apparently 500\n",
      "copied 500\n",
      "quoting 500\n",
      "Stella 500\n",
      "Cameron 500\n",
      "publishing 1000\n",
      "Oct 500\n",
      "2013 500\n",
      "wondered 500\n",
      "publisher 500\n",
      "Cold 500\n",
      "July 500\n",
      "2003 500\n",
      "uneven 500\n",
      "click 500\n",
      "Said 500\n",
      "LIKE 500\n",
      "NEW 500\n",
      "latest 500\n",
      "claimed 1000\n",
      "bright 500\n",
      "red 500\n",
      "stamps 500\n",
      "library 2000\n",
      "stickers 1000\n",
      "obviously 500\n",
      "checked 500\n",
      "protective 500\n",
      "layering 500\n",
      "begun 500\n",
      "peeling 1000\n",
      "stamped 1000\n",
      "Of 1000\n",
      "OHSHC 500\n",
      "achieve 500\n",
      "Surprisingly 500\n",
      "revised 500\n",
      "south 500\n",
      "reasonably 500\n",
      "Unfortunately 1500\n",
      "stalled 500\n",
      "dialog 1000\n",
      "storylines 500\n",
      "profanity 500\n",
      "unfortunately 500\n",
      "botched 500\n",
      "poor 1500\n",
      "characterization 500\n",
      "immense 500\n",
      "amiss 500\n",
      "juvenile 500\n",
      "culprits 500\n",
      "dissed 500\n",
      "Disappointing 500\n",
      "travel 1000\n",
      "log 500\n",
      "bio 500\n",
      "conveys 500\n",
      "discovery 500\n",
      "Boy 500\n",
      "WASTING 500\n",
      "released 500\n",
      "Adler 500\n",
      "Olsen 500\n",
      "stunned 500\n",
      "translation 500\n",
      "Guilt 500\n",
      "VERY 1000\n",
      "heavy 500\n",
      "contrary 500\n",
      "JAO 500\n",
      "systematically 500\n",
      "mention 1500\n",
      "translator 500\n",
      "mistake 500\n",
      "Tijan 1000\n",
      "becomes 500\n",
      "apologizing 500\n",
      "groveling 500\n",
      "complexity 500\n",
      "minutes 500\n",
      "humorous 1000\n",
      "trendy 500\n",
      "capture 1000\n",
      "Since 1000\n",
      "cannot 1000\n",
      "comment 500\n",
      "Returned 500\n",
      "68 500\n",
      "internet 1000\n",
      "mixed 500\n",
      "hesitant 500\n",
      "THEN 500\n",
      "Have 500\n",
      "College 500\n",
      "upper 1000\n",
      "formulas 500\n",
      "offered 500\n",
      "WW 500\n",
      "II 500\n",
      "9 500\n",
      "months 500\n",
      "clubs 500\n",
      "Unless 1500\n",
      "mood 1000\n",
      "tone 500\n",
      "heavily 500\n",
      "padded 500\n",
      "generic 500\n",
      "fifth 500\n",
      "130 500\n",
      "preferred 500\n",
      "Slow 500\n",
      "Fat 500\n",
      "Triathlete 500\n",
      "Jayne 500\n",
      "funnier 500\n",
      "genuinely 500\n",
      "Honestly 500\n",
      "negative 1000\n",
      "promoted 500\n",
      "Kindle 2500\n",
      "First 500\n",
      "protagonist 1000\n",
      "lack 500\n",
      "begin 500\n",
      "survival 1000\n",
      "direction 1000\n",
      "types 500\n",
      "supplies 500\n",
      "emergency 500\n",
      "00 500\n",
      "ebook 500\n",
      "exchange 500\n",
      "unbiased 500\n",
      "dramatic 500\n",
      "British 500\n",
      "certain 500\n",
      "silly 1000\n",
      "witch 500\n",
      "meant 1000\n",
      "grahic 500\n",
      "curse 500\n",
      "black 2500\n",
      "sacrafice 500\n",
      "halo 500\n",
      "adults 1000\n",
      "Reviewed 500\n",
      "mother 500\n",
      "13 1500\n",
      "yr 500\n",
      "terrorize 500\n",
      "neighborhoods 500\n",
      "saying 500\n",
      "Sure 500\n",
      "shallow 500\n",
      "endearing 500\n",
      "mockery 500\n",
      "parodies 500\n",
      "blessing 500\n",
      "dive 500\n",
      "borrow 500\n",
      "Part 500\n",
      "concentrate 500\n",
      "wouldn 1000\n",
      "Walmart 500\n",
      "larger 500\n",
      "print 1500\n",
      "thicker 500\n",
      "unending 500\n",
      "woe 500\n",
      "thinly 500\n",
      "veiled 500\n",
      "support 500\n",
      "overreach 500\n",
      "overemphasizes 500\n",
      "bleak 500\n",
      "resulting 500\n",
      "Largo 500\n",
      "Tal 500\n",
      "vez 500\n",
      "pudo 500\n",
      "acotar 500\n",
      "los 1000\n",
      "hechos 500\n",
      "por 500\n",
      "momentos 500\n",
      "tent 500\n",
      "saltar 500\n",
      "hojas 500\n",
      "conclusin 500\n",
      "vali 500\n",
      "lectura 500\n",
      "internal 500\n",
      "medical 500\n",
      "terms 500\n",
      "intriged 500\n",
      "hands 500\n",
      "disapointment 500\n",
      "Lilith 1000\n",
      "behaves 500\n",
      "stupid 2000\n",
      "adolescent 500\n",
      "insult 500\n",
      "injury 500\n",
      "learns 500\n",
      "landed 500\n",
      "puppet 500\n",
      "admittedly 500\n",
      "actions 1000\n",
      "worst 2000\n",
      "sort 500\n",
      "mistery 500\n",
      "duche 500\n",
      "Huge 500\n",
      "letdown 500\n",
      "Goldberg 500\n",
      "competent 1000\n",
      "readable 1000\n",
      "heckuva 500\n",
      "80 500\n",
      "violent 500\n",
      "arrests 500\n",
      "gasp 500\n",
      "scene 500\n",
      "hasn 1000\n",
      "build 500\n",
      "sample 1500\n",
      "assuming 500\n",
      "5th 500\n",
      "300 500\n",
      "happily 500\n",
      "donate 500\n",
      "Little 500\n",
      "Library 500\n",
      "Spark 500\n",
      "Award 500\n",
      "bite 500\n",
      "Bogged 500\n",
      "extraneous 500\n",
      "predictability 500\n",
      "uniqueness 500\n",
      "Started 1000\n",
      "holes 1500\n",
      "virgin 500\n",
      "notice 500\n",
      "calls 500\n",
      "disapproving 500\n",
      "spending 500\n",
      "pointless 500\n",
      "Picture 500\n",
      "Worse 500\n",
      "17 500\n",
      "owners 500\n",
      "10 500\n",
      "cents 1000\n",
      "pictured 500\n",
      "stands 500\n",
      "sore 500\n",
      "match 1500\n",
      "return 1500\n",
      "Knowing 500\n",
      "accordion 500\n",
      "scale 1000\n",
      "notes 500\n",
      "sight 500\n",
      "jumps 500\n",
      "dancing 500\n",
      "button 500\n",
      "rows 500\n",
      "sit 500\n",
      "scales 500\n",
      "downhill 500\n",
      "slide 1000\n",
      "system 500\n",
      "OK 1000\n",
      "flashback 500\n",
      "rehash 500\n",
      "rerun 500\n",
      "trip 500\n",
      "bank 500\n",
      "permit 500\n",
      "MOVIE 500\n",
      "literary 500\n",
      "1st 500\n",
      "previews 500\n",
      "elements 500\n",
      "horror 500\n",
      "dark 500\n",
      "ego 500\n",
      "Patte 500\n",
      "Boyd 500\n",
      "ugh 500\n",
      "wouldnt 1000\n",
      "recomend 1000\n",
      "outdated 1000\n",
      "incorrect 1000\n",
      "Dough 500\n",
      "explain 1000\n",
      "Research 500\n",
      "effectiveness 500\n",
      "concepts 500\n",
      "harm 500\n",
      "produce 500\n",
      "results 500\n",
      "career 500\n",
      "race 500\n",
      "competing 500\n",
      "including 500\n",
      "sequence 500\n",
      "outlandish 500\n",
      "halfway 500\n",
      "charactersfound 500\n",
      "quarters 500\n",
      "surprisingly 500\n",
      "Harry 1500\n",
      "Potter 1500\n",
      "Predictable 500\n",
      "directions 1000\n",
      "69 500\n",
      "92 500\n",
      "duplicates 500\n",
      "93 500\n",
      "117 500\n",
      "borrowed 500\n",
      "maybe 500\n",
      "editions 500\n",
      "Mine 500\n",
      "printing 1000\n",
      "2014 500\n",
      "Paneer 500\n",
      "Chivo 500\n",
      "Fresco 500\n",
      "Honeyed 500\n",
      "Toast 500\n",
      "Cheese 500\n",
      "cheeses 500\n",
      "bummed 500\n",
      "incomplete 500\n",
      "therapist 500\n",
      "residential 500\n",
      "environment 500\n",
      "activities 1500\n",
      "group 500\n",
      "suggested 500\n",
      "instructions 500\n",
      "therapists 500\n",
      "email 1000\n",
      "Rule 1000\n",
      "Jet 500\n",
      "afraid 500\n",
      "reviewers 500\n",
      "Martini 1000\n",
      "Africa 500\n",
      "Need 500\n",
      "Jodi 500\n",
      "Picoult 500\n",
      "weakest 500\n",
      "intended 500\n",
      "unanswered 500\n",
      "unsatisfying 500\n",
      "motivations 500\n",
      "Learning 500\n",
      "rules 500\n",
      "lyrics 500\n",
      "lyric 500\n",
      "sheet 500\n",
      "recording 500\n",
      "damaged 500\n",
      "stains 500\n",
      "sun 500\n",
      "damage 500\n",
      "costly 500\n",
      "send 500\n",
      "Fern 500\n",
      "Michaels 500\n",
      "dragged 500\n",
      "payoff 1000\n",
      "useless 1000\n",
      "access 500\n",
      "obtained 500\n",
      "destination 500\n",
      "venue 500\n",
      "Otherwise 500\n",
      "obtain 500\n",
      "FREE 500\n",
      "awful 2000\n",
      "Answers 500\n",
      "Selected 500\n",
      "Problems 500\n",
      "Horrible 500\n",
      "guides 500\n",
      "studied 500\n",
      "religiously 500\n",
      "weeks 500\n",
      "leading 500\n",
      "test 1000\n",
      "none 500\n",
      "studying 500\n",
      "misspelled 500\n",
      "Im 500\n",
      "NEED 500\n",
      "dimensional 500\n",
      "cardboard 500\n",
      "imitations 500\n",
      "expressed 500\n",
      "mainly 1000\n",
      "pair 500\n",
      "fasten 500\n",
      "cloak 500\n",
      "Leon 500\n",
      "map 500\n",
      "taxi 500\n",
      "drivers 500\n",
      "Boring 500\n",
      "mark 500\n",
      "quit 500\n",
      "sorry 1500\n",
      "Everything 500\n",
      "bam 1500\n",
      "Keep 1000\n",
      "Takes 500\n",
      "ur 500\n",
      "scholarly 500\n",
      "introspection 500\n",
      "antiquity 500\n",
      "fomented 500\n",
      "field 500\n",
      "monster 500\n",
      "horrible 1000\n",
      "patched 500\n",
      "plow 500\n",
      "somehow 500\n",
      "Terrible 1000\n",
      "dollar 1000\n",
      "wrestling 1500\n",
      "Batistas 500\n",
      "summed 500\n",
      "untalented 500\n",
      "musclehead 500\n",
      "Yet 500\n",
      "befriended 500\n",
      "HHH 500\n",
      "undeserved 500\n",
      "moon 500\n",
      "terrific 500\n",
      "grabbing 500\n",
      "possessed 500\n",
      "connections 500\n",
      "threw 1000\n",
      "exploded 500\n",
      "Such 500\n",
      "tidbits 500\n",
      "magazines 500\n",
      "compiled 500\n",
      "hearted 500\n",
      "Vicky 500\n",
      "Iovine 500\n",
      "Girlfriends 500\n",
      "Guide 500\n",
      "Pregnancy 500\n",
      "goodness 500\n",
      "shocked 500\n",
      "cry 500\n",
      "heck 500\n",
      "packs 500\n",
      "wallop 500\n",
      "schooler 500\n",
      "blog 500\n",
      "crude 500\n",
      "described 500\n",
      "obscene 500\n",
      "senarios 500\n",
      "horrific 500\n",
      "abuse 1000\n",
      "greeted 500\n",
      "voyeristic 500\n",
      "nonchalance 500\n",
      "plumbed 500\n",
      "Homes 500\n",
      "seriously 1000\n",
      "literature 500\n",
      "suburban 500\n",
      "angst 500\n",
      "Cheever 500\n",
      "Edie 500\n",
      "Spence 500\n",
      "disjointed 500\n",
      "suffering 500\n",
      "terribly 500\n",
      "ridiculed 500\n",
      "irrational 500\n",
      "frightened 500\n",
      "anger 500\n",
      "pronouncements 500\n",
      "intelligent 500\n",
      "incredibly 500\n",
      "delusional 500\n",
      "kidding 500\n",
      "digest 500\n",
      "saver 500\n",
      "suck 500\n",
      "u 1000\n",
      "Baldacci 1000\n",
      "delete 500\n",
      "ought 500\n",
      "report 500\n",
      "failure 500\n",
      "lastima 500\n",
      "que 500\n",
      "esta 500\n",
      "espaol 500\n",
      "culpa 500\n",
      "mia 500\n",
      "gracias 500\n",
      "igual 500\n",
      "dating 500\n",
      "crap 500\n",
      "Sean 500\n",
      "majority 500\n",
      "god 500\n",
      "botherer 500\n",
      "amassing 500\n",
      "issuing 500\n",
      "powered 500\n",
      "weoponry 500\n",
      "literally 500\n",
      "equip 500\n",
      "cohorts 500\n",
      "hoarding 500\n",
      "scrap 500\n",
      "benefit 500\n",
      "throw 500\n",
      "Andrews 500\n",
      "unlikely 500\n",
      "scenario 500\n",
      "imagineable 500\n",
      "mates 500\n",
      "Hollerman 500\n",
      "apolyptic 500\n",
      "ghostwalker 500\n",
      "ground 500\n",
      "Feehan 500\n",
      "ashamed 500\n",
      "Major 500\n",
      "impacts 500\n",
      "Dishonest 500\n",
      "depiction 500\n",
      "Jewish 500\n",
      "settlers 500\n",
      "1900 500\n",
      "comparing 500\n",
      "escaping 1000\n",
      "pogroms 500\n",
      "theys 500\n",
      "colonilize 500\n",
      "land 1000\n",
      "Muslims 500\n",
      "Israel 500\n",
      "poverty 500\n",
      "native 500\n",
      "Syria 500\n",
      "Lebanon 500\n",
      "paste 1000\n",
      "research 1500\n",
      "misleading 500\n",
      "myaccess 500\n",
      "card 500\n",
      "promised 500\n",
      "contact 1000\n",
      "Rockswold 500\n",
      "disappointing 1500\n",
      "Chaperone 500\n",
      "assumed 500\n",
      "Moriarty 500\n",
      "Absolutely 500\n",
      "discuss 500\n",
      "Cd 500\n",
      "broken 500\n",
      "apocalyse 500\n",
      "teenage 500\n",
      "declare 500\n",
      "barely 500\n",
      "acclimating 500\n",
      "tribe 500\n",
      "clamoring 500\n",
      "survive 500\n",
      "shopping 500\n",
      "chooseing 500\n",
      "cards 500\n",
      "pharm 500\n",
      "truth 1000\n",
      "mnemonics 500\n",
      "distracting 500\n",
      "regret 500\n",
      "package 500\n",
      "delivered 500\n",
      "home 500\n",
      "careful 500\n",
      "company 1000\n",
      "greatly 500\n",
      "mangled 500\n",
      "confusing 500\n",
      "Seriously 1000\n",
      "fanfiction 500\n",
      "worthy 500\n",
      "Rowling 500\n",
      "approved 500\n",
      "garbage 500\n",
      "cringe 500\n",
      "script 500\n",
      "AND 500\n",
      "DO 500\n",
      "ACCEPT 500\n",
      "AS 500\n",
      "CANON 500\n",
      "Fantastic 500\n",
      "Beasts 500\n",
      "NYT 1000\n",
      "crosswords 1000\n",
      "avid 500\n",
      "puns 500\n",
      "double 500\n",
      "meanings 500\n",
      "ploys 500\n",
      "crossword 1500\n",
      "puzzles 2000\n",
      "employ 500\n",
      "misspelt 500\n",
      "strange 500\n",
      "constructions 500\n",
      "faulty 500\n",
      "definitions 500\n",
      "weird 500\n",
      "clues 500\n",
      "obtuse 500\n",
      "avoid 500\n",
      "condemnation 500\n",
      "Admittedly 500\n",
      "dang 500\n",
      "cluster 500\n",
      "Poorly 500\n",
      "size 1000\n",
      "Mack 500\n",
      "trucks 500\n",
      "dialogue 500\n",
      "George 500\n",
      "Lucas 500\n",
      "Shakespeare 500\n",
      "giveaway 500\n",
      "owes 500\n",
      "refund 500\n",
      "Ryker 500\n",
      "kinda 500\n",
      "mouth 500\n",
      "Hoped 500\n",
      "items 1000\n",
      "readily 500\n",
      "available 1000\n",
      "grocery 1000\n",
      "booklet 500\n",
      "musty 500\n",
      "smell 500\n",
      "shelves 500\n",
      "dealt 500\n",
      "heroine 500\n",
      "unnecessarily 500\n",
      "deaths 500\n",
      "captors 500\n",
      "sinister 500\n",
      "evil 1000\n",
      "society 500\n",
      "response 500\n",
      "Dearth 500\n",
      "except 500\n",
      "rubbish 500\n",
      "delusion 500\n",
      "shocking 500\n",
      "Ask 500\n",
      "USS 1000\n",
      "Liberty 1000\n",
      "survivor 500\n",
      "Veterans 500\n",
      "Association 500\n",
      "confuse 500\n",
      "hide 500\n",
      "murderous 500\n",
      "deliberate 500\n",
      "killing 500\n",
      "34 500\n",
      "wounding 500\n",
      "173 500\n",
      "295 500\n",
      "Ted 500\n",
      "Arens 500\n",
      "Worst 500\n",
      "Unbelievable 500\n",
      "German 500\n",
      "Deutsch 500\n",
      "sent 1500\n",
      "reimbursed 500\n",
      "Gave 500\n",
      "nutritionist 500\n",
      "Had 500\n",
      "meals 500\n",
      "consisted 500\n",
      "smoothies 1000\n",
      "stupidest 500\n",
      "Dumb 500\n",
      "af 500\n",
      "dumber 500\n",
      "Lightly 500\n",
      "instrument 500\n",
      "useable 500\n",
      "Patterson 1000\n",
      "newest 500\n",
      "substance 500\n",
      "doubled 500\n",
      "spaced 500\n",
      "paragraphs 500\n",
      "white 1000\n",
      "grinding 500\n",
      "earliest 500\n",
      "daytime 500\n",
      "soap 500\n",
      "opera 500\n",
      "Bunch 500\n",
      "wiening 500\n",
      "complaining 500\n",
      "Flying 500\n",
      "ploy 500\n",
      "publishers 500\n",
      "Harper 500\n",
      "gone 500\n",
      "Richter 500\n",
      "sketches 500\n",
      "paintings 500\n",
      "Completely 500\n",
      "implausible 500\n",
      "progressively 500\n",
      "acceptable 500\n",
      "laughably 500\n",
      "BPM 500\n",
      "businesses 500\n",
      "facing 500\n",
      "Snow 500\n",
      "White 500\n",
      "animation 500\n",
      "unknown 500\n",
      "comments 1000\n",
      "reivew 500\n",
      "coclusion 500\n",
      "nine 500\n",
      "excuse 1000\n",
      "spread 500\n",
      "righteous 500\n",
      "mission 500\n",
      "spreading 500\n",
      "passive 500\n",
      "aggressive 500\n",
      "victim 1000\n",
      "blames 500\n",
      "implies 500\n",
      "husbands 500\n",
      "cheat 500\n",
      "serve 500\n",
      "teachings 500\n",
      "dangerous 500\n",
      "ignorant 500\n",
      "cares 500\n",
      "spewing 500\n",
      "venom 500\n",
      "caring 500\n",
      "drink 500\n",
      "koolaid 500\n",
      "dude 500\n",
      "foot 500\n",
      "introductory 500\n",
      "teaching 500\n",
      "formatting 500\n",
      "accessibility 500\n",
      "vague 500\n",
      "desired 500\n",
      "rodda 500\n",
      "nora 1000\n",
      "hess 1000\n",
      "happen 500\n",
      "punch 500\n",
      "raven 500\n",
      "ruin 500\n",
      "Luckily 500\n",
      "cleverer 500\n",
      "unimpressive 500\n",
      "melded 500\n",
      "P 1000\n",
      "remixes 500\n",
      "satires 500\n",
      "racist 500\n",
      "Orientals 500\n",
      "nerves 500\n",
      "disturbed 500\n",
      "Austen 500\n",
      "originals 500\n",
      "chosen 500\n",
      "Fens 500\n",
      "underlying 500\n",
      "IF 500\n",
      "breathe 500\n",
      "air 500\n",
      "zero 500\n",
      "couldgive 500\n",
      "aw 500\n",
      "stole 500\n",
      "athurs 500\n",
      "Wait 500\n",
      "Till 500\n",
      "Helen 500\n",
      "Comes 500\n",
      "STOP 500\n",
      "Programming 500\n",
      "Which 500\n",
      "causes 500\n",
      "serious 500\n",
      "NO 1000\n",
      "6 500\n",
      "sellier 500\n",
      "Beautiful 500\n",
      "girl 500\n",
      "kidnapped 500\n",
      "handsome 500\n",
      "rescues 500\n",
      "record 500\n",
      "deployment 500\n",
      "sensitive 500\n",
      "charged 500\n",
      "cancel 500\n",
      "revisionist 500\n",
      "experts 500\n",
      "homosexuality 500\n",
      "proportion 500\n",
      "population 500\n",
      "pirates 500\n",
      "Those 500\n",
      "sea 500\n",
      "riches 500\n",
      "Absolute 500\n",
      "prophecies 500\n",
      "Malachy 500\n",
      "proven 500\n",
      "forgeries 500\n",
      "Notwithstanding 500\n",
      "false 500\n",
      "prophecy 500\n",
      "112 500\n",
      "Popes 500\n",
      "load 500\n",
      "foolishness 500\n",
      "expects 500\n",
      "Looking 500\n",
      "length 500\n",
      "volume 500\n",
      "99 500\n",
      "enriching 500\n",
      "Tate 500\n",
      "BLANK 500\n",
      "sessions 500\n",
      "toy 500\n",
      "giant 500\n",
      "Never 500\n",
      "legal 500\n",
      "FREEBIES 500\n",
      "JERRY 500\n",
      "outed 500\n",
      "Rangers 500\n",
      "hater 500\n",
      "accept 500\n",
      "opinions 500\n",
      "suggests 500\n",
      "Hitler 500\n",
      "resounding 500\n",
      "views 500\n",
      "tainted 500\n",
      "ink 500\n",
      "amazed 500\n",
      "Indiana 500\n",
      "University 500\n",
      "considered 500\n",
      "attaching 500\n",
      "falsification 500\n",
      "Georgian 500\n",
      "straightforwardly 500\n",
      "INSULT 500\n",
      "emotions 500\n",
      "Georgia 500\n",
      "mislead 500\n",
      "cryptic 500\n",
      "Grid 500\n",
      "61 500\n",
      "nor 500\n",
      "giftee 500\n",
      "grid 500\n",
      "TRASH 500\n",
      "sign 500\n",
      "petition 500\n",
      "org 500\n",
      "remove 500\n",
      "filth 1000\n",
      "site 500\n",
      "physically 500\n",
      "ABSOLUTELY 500\n",
      "DISGUSTING 500\n",
      "describe 500\n",
      "obese 500\n",
      "exhausted 500\n",
      "weed 500\n",
      "fluff 500\n",
      "likeable 500\n",
      "earned 500\n",
      "Shatner 500\n",
      "demand 500\n",
      "low 500\n",
      "Battletech 500\n",
      "Mister 500\n",
      "Intelligentsia 500\n",
      "extraordinaire 500\n",
      "STRONGLY 500\n",
      "Genesis 500\n",
      "Code 500\n",
      "Friedman 500\n",
      "truely 500\n",
      "wants 500\n",
      "BarryJ 500\n",
      "experiences 500\n",
      "war 1000\n",
      "liberal 500\n",
      "anti 500\n",
      "balanced 500\n",
      "Yay 500\n",
      "stack 500\n",
      "top 500\n",
      "Volumes 500\n",
      "hundreds 1000\n",
      "dollars 500\n",
      "Version 500\n",
      "Table 500\n",
      "Contents 500\n",
      "1200 500\n",
      "e 500\n",
      "TOC 500\n",
      "fix 500\n",
      "update 500\n",
      "generally 500\n",
      "supplement 500\n",
      "brought 500\n",
      "proof 500\n",
      "responding 500\n",
      "Math 500\n",
      "jeopardizes 500\n",
      "Contento 500\n",
      "con 500\n",
      "Si 500\n",
      "saber 500\n",
      "gastos 500\n",
      "envo 500\n",
      "sorpresa 500\n",
      "lo 500\n",
      "compro 500\n",
      "Complete 500\n",
      "Punjabi 500\n",
      "translations 500\n",
      "Magee 500\n",
      "awkward 500\n",
      "slap 500\n",
      "equivalent 500\n",
      "porn 500\n",
      "subhumans 500\n",
      "prison 500\n",
      "heard 1000\n",
      "Stupid 500\n",
      "supposed 500\n",
      "homicide 500\n",
      "cop 500\n",
      "idiot 1000\n",
      "fooled 500\n",
      "psychopath 500\n",
      "fake 500\n",
      "boobs 500\n",
      "suffer 500\n",
      "forewarn 500\n",
      "footnotes 500\n",
      "hyphen 500\n",
      "table 500\n",
      "contemts 500\n",
      "links 500\n",
      "remains 500\n",
      "keeping 500\n",
      "debating 500\n",
      "extra 500\n",
      "Ten 500\n",
      "hated 500\n",
      "TERRIBLE 500\n",
      "mouthing 500\n",
      "Trash 500\n",
      "Always 500\n",
      "sales 500\n",
      "opportunity 500\n",
      "advantage 500\n",
      "NEVER 500\n",
      "Palin 500\n",
      "NRA 500\n",
      "politician 500\n",
      "vilifies 500\n",
      "Obama 500\n",
      "Give 500\n",
      "break 500\n",
      "dissapointing 500\n",
      "Dr 500\n",
      "Hook 500\n",
      "bearing 500\n",
      "guest 500\n",
      "lists 500\n",
      "attendees 500\n",
      "cramped 500\n",
      "improvement 500\n",
      "fully 500\n",
      "stuck 500\n",
      "Loaded all data from reviews-word2vec.tiny.txt; saw 4100 tokens (692 unique)\n",
      "Generating sampling table\n",
      "Negative sampling table\n",
      "[  0   0   0 ... 691 691 691]\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(rng=RandomNumberGenerator)\n",
    "corpus.load_data(file_name=\"reviews-word2vec.tiny.txt\", min_token_freq=2000)\n",
    "corpus.generate_negative_sampling_table()\n",
    "\n",
    "print(\"Negative sampling table\")\n",
    "print(corpus.negative_sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the training data\n",
    "\n",
    "Once we have the corpus ready, we need to generate our training dataset. Each instance in the dataset is a target word and positive and negative examples of contexts words. Given the target word as input, we'll want to predict (or not predict) these positive and negative context words as outputs using our network. Your task here is to create a python `list` of instances. \n",
    "\n",
    "Your final training data should be a list of tuples in the format ([target_word_id], [word_id_1, ...], [predicted_labels]), where each item in the list is a list:\n",
    "1. The first item is a list consisting only of the target word's ID.\n",
    "2. The second item is a list of word ids for both context words and negative samples \n",
    "3. The third item is a list of labels to predicted for each of the word ids in the second list (i.e., `1` for context words and `0` for negative samples). \n",
    "\n",
    "You will feed these tuples into the PyTorch `DatasetLoader` later that will do the converstion to `Tensor` objects. You will need to make sure that all of the lists in each tuple are `np.array` instances and are not plain python lists for this `Tensor` converstion to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106aa59fd17f4662bedf5fc8a1453d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23011087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 2\n",
    "num_negative_samples_per_target = 2\n",
    "\n",
    "training_data = []\n",
    "    \n",
    "# Loop through each token in the corpus and generate an instance for each, \n",
    "# adding it to training_data\n",
    "for _ in []:\n",
    "\n",
    "    # For exach target word in our dataset, select context words \n",
    "    # within +/- the window size in the token sequence\n",
    "    \n",
    "    # For each positive target, we need to select negative examples of\n",
    "    # words that were not in the context. Use the num_negative_samples_per_target\n",
    "    # hyperparameter to generate these, using the generate_negative_samples()\n",
    "    # method from the Corpus class\n",
    "\n",
    "    # NOTE: this part might not make sense until later when you do the training \n",
    "    # so feel free to revisit it to see why it happens.\n",
    "    #\n",
    "    # Our training will use batches of instances together (compare that \n",
    "    # with HW1's SGD that used one item at a time). PyTorch will require\n",
    "    # that all instances in a batches have the same size, which creates an issue\n",
    "    # for us here since the target wordss at the very beginning or end of the corpus\n",
    "    # have shorter contexts. \n",
    "    # \n",
    "    # To work around these edge-cases, we need to ensure that each instance has\n",
    "    # the same size, which means it needs to have the same number of positive\n",
    "    # and negative examples. Since we are short on positive examples here (due\n",
    "    # to the edge of the corpus), we can just add more negative samples.\n",
    "    #\n",
    "    # YOUR TASK: determine what is the maximum number of context words (positive\n",
    "    # and negative) for any instance and then, for instances that have fewer than\n",
    "    # this number of context words, add in negative examples.\n",
    "    #\n",
    "    # NOTE: The maximum is fixed, so you can precompute this outside the loop\n",
    "    # ahead of time.\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network\n",
    "\n",
    "We'll create a new neural network as a subclass of `nn.Module` like we did in Homework 1. However, _unlike_ the network you built in Homework 1, we do not need to used linear layers to implement word2vec. Instead, we will use PyTorch's `Emedding` class, which maps an index (e.g., a word id in this case) to an embedding. \n",
    "\n",
    "Roughly speaking, word2vec's network makes a prediction by computing the dot product of the target word's embedding and a context word's embedding and then passing this dot product through the sigmoid function ($\\sigma$) to predict the probability that the context word was actually in the context. The homework write-up has lots of details on how this works. Your `forward()` function will have to implement this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        # Save what state you want and create the embeddings for your\n",
    "        # target and context words\n",
    "        self.target_embbedings = None\n",
    "        self.context_embbedings = None\n",
    "        \n",
    "        # Once created, let's fill the embeddings with non-zero random\n",
    "        # numbers. We need to do this to get the training started. \n",
    "        #\n",
    "        # NOTE: Why do this? Think about what happens if all the embeddings\n",
    "        # are all zeros initially. What would the predictions look like for\n",
    "        # word2vec with these embeddings and how would the updated work?\n",
    "        \n",
    "        self.init_emb(init_range=0.5/self.vocab_size)\n",
    "        \n",
    "    def init_emb(self, init_range):\n",
    "        \n",
    "        # Fill your two embeddings with random numbers uniformly sampled\n",
    "        # between +/- init_range\n",
    "    \n",
    "        pass\n",
    "        \n",
    "    def forward(self, target_word_id, context_word_ids):\n",
    "        ''' \n",
    "        Predicts whether each context word was actually in the context of the target word.\n",
    "        The input is a tensor with a single target word's id and a tensor containing each\n",
    "        of the context words' ids (this includes both positive and negative examples).\n",
    "        '''\n",
    "        \n",
    "        # NOTE 1: This is probably the hardest part of the homework, so you'll\n",
    "        # need to figure out how to do the dot-product between embeddings and return\n",
    "        # the sigmoid. Be prepared for lots of debugging. For some reference,\n",
    "        # our implementation is three lines and really the hard part is just\n",
    "        # the last line. However, it's usually a matter of figuring out what\n",
    "        # that one line looks like that ends up being the hard part.\n",
    "        \n",
    "        # NOTE 2: In this homework you'll be dealing with *batches* of instances\n",
    "        # rather than a single instance at once. PyTorch mostly handles this\n",
    "        # seamlessly under the hood for you (which is very nice) but batching\n",
    "        # can show in weird ways and create challenges in debugging initially.\n",
    "        # For one, your inputs will get an extra dimension. So, for example,\n",
    "        # if you have a batch size of 4, your input for target_word_id will\n",
    "        # really be 4 x 1. If you get the embeddings of those targets,\n",
    "        # it then becomes 4x50! The same applies to the context_word_ids, except\n",
    "        # that was alreayd a list so now you have things with shape \n",
    "        #\n",
    "        #    (batch x context_words x embedding_size)\n",
    "        #\n",
    "        # One of your tasks will be to figure out how to get things lined up\n",
    "        # so everything \"just works\". When it does, the code looks surprisingly\n",
    "        # simple, but it might take a lot of debugging (or not!) to get there.\n",
    "        \n",
    "        # NOTE 3: We *strongly* discourage you from looking for existing \n",
    "        # implementations of word2vec online. Sadly, having reviewed most of the\n",
    "        # highly-visible ones, they are actually wrong (wow!) or are doing\n",
    "        # inefficient things like computing the full softmax instead of doing\n",
    "        # the negative sampling. Looking at these will likely leave you more\n",
    "        # confused than if you just tried to figure it out yourself.\n",
    "        \n",
    "        # NOTE 4: There many ways to implement this, some more efficient\n",
    "        # than others. You will want to get it working first and then\n",
    "        # test the timing to see how long it takes. As long as the\n",
    "        # code works (vector comparisons look good) you'll receive full\n",
    "        # credit. However, very slow implementations may take hours(!)\n",
    "        # to converge so plan ahead.\n",
    "        \n",
    "        \n",
    "        # Hint 1: You may want to review the mathematical operations on how\n",
    "        # to compute the dot product to see how to do these\n",
    "        \n",
    "        # Hint 2: the \"dim\" argument for some operations may come in handy,\n",
    "        # depending on your implementation\n",
    "        \n",
    "           \n",
    "        # TODO: Implement the forward pass of word2vec\n",
    "            \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network!\n",
    "\n",
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The trainin code will look surprisingly similar at times to your pytorch code from Homework 1 since all networks share the same base training setup. However, we'll add a few new elements to get you familiar with more common training techniques. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. Create a new `SummaryWriter` to periodically write our running-sum of the loss to a tensorboard\n",
    "5. Train your model \n",
    "\n",
    "Two new elements show up. First, we'll be using `DataLoader` which is going to sample data for us and put it in a batch (and also convert the data to `Tensor` objects. You can iterate over the batches and each iteration will return all the items eventually, one batch at a time (a full epoch's worth).\n",
    "\n",
    "The second new part is using `wandb`. As you might have noticed in Homework 1, training neural models can take some time. [Weights & Biases](https://wandb.ai/) is a handy web-based view that you can check during training to see how the model is doing. We'll use it here and periodically log a running sum of the loss after a set number of steps. The Homework write up has a plot of what this looks like. We'll be doing something simple here with wandb but it will come in handy later as you train larger models (for longer) and may want to visually check if your model is converging and is [easy to integrate](https://docs.wandb.ai/guides/integrations/pytorch).\n",
    "\n",
    "Once you get the code working, to start training, we recommend training on the `reviews-word2vec.med.txt` dataset. This data is small enough you can get through an epoch in a few minutes (or less) while still being large enough you can test whether the model is learning anything by examining common words. Below this cell we've added a few helper functions that you can use to debug and query your model. In particular, the `get_neighbors()` function is a great way to test: if your model has learned anything, the nearest neighbors for common words should seem reasonable (without having to jump through mental hoops). An easy word to test on the `med` data is \"january\" which should return month-related words as being most similar.\n",
    "\n",
    "**NOTE**: Since we're training biographies, the text itself will be skewed towards words likely to show up biographices--which isn't necessary like \"regular\" text. You may find that your model has few instances of words you think are common, or that the model learns poor or unusual neighbors for these. When querying the neighbors, it can help to think of which words you think are likely to show up in biographies on Wikipedia and use those as probes to see what the model has learned.\n",
    "\n",
    "Once you're convinced the model is learning, switch to the `med` data and train your model as specified in the PDF. Once trained, save your model using the `save()` function at the end of the notebook. This function records your data in a common format for word2vec vectors and lets you load the vectors into other libraries that have more advanced functionality. In particular, you can use the [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) code in other notebook included to explore the vectors and do simple vector analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a66dec2f4f46d2a53c2467d7830d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ceb1653c8943e5819789979c7d8a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42826.35546875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Set your training stuff, hyperparameters, models, etc. here\n",
    "model = None\n",
    "\n",
    "# TODO: Initialize weights and biases (wandb) here \n",
    "\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "for epoch in []:\n",
    "\n",
    "    loss_sum = 0\n",
    "    \n",
    "    # TODO: use your DataLoader to iterate over the data\n",
    "    for step, data in enumerate([]):\n",
    "\n",
    "        # NOTE: since you created the data as a tuple of three np.array instances,\n",
    "        # these have now been converted to Tensor objects for us\n",
    "        target_ids, context_ids, labels = data    \n",
    "\n",
    "        \n",
    "        # TODO: Fill in all the training details here\n",
    "\n",
    "        \n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to Weights & Biases (wandb).\n",
    "        # Be sure to reset the running sum after reporting it.\n",
    "        \n",
    "        \n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        \n",
    "        \n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify things are working\n",
    "\n",
    "Once you have an initial model trained, try using the following code to query the model for what are the nearest neighbor of a word. This code is intended to help you debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(model, word_to_index, target_word):\n",
    "    \"\"\" \n",
    "    Finds the top 10 most similar words to a target word\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for word, index in tqdm(word_to_index.items(), total=len(word_to_index)):\n",
    "        similarity = compute_cosine_similarity(model, word_to_index, target_word, word)\n",
    "        result = {\"word\": word, \"score\": similarity}\n",
    "        outputs.append(result)\n",
    "\n",
    "    # Sort by highest scores\n",
    "    neighbors = sorted(outputs, key=lambda o: o['score'], reverse=True)\n",
    "    return neighbors[1:11]\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    '''\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]))\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().squeeze().numpy(),\n",
    "                                      embedding_two.detach().squeeze().numpy())))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"recommend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"son\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your vectors for the gensim inspection part!\n",
    "\n",
    "Once you have a fully trained model, save it using the code below. Note that we only save the `target_embeddings` from the model, but you could modify the code if you want to save the context vectors--or even try doing fancier things like saving the concatenation of the two or the average of the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "    kv = KeyedVectors(vector_size=model.embedding_size)        \n",
    "    vectors = []\n",
    "    words = []\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        vectors.append(model.target_embeddings(torch.LongTensor([index])).detach().numpy()[0])\n",
    "        words.append(word)\n",
    "\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your vectors / data for the pytorch classifier in Part 4!\n",
    "\n",
    "We'll be to using these vectors later in Part 4. We want to save them in a format that PyTorch can easily use. In particular you'll need to save the _state dict_ of the embeddings, which captures all of its information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the mapping from word to index so we can figure out which embedding to use for different words. Save the `corpus` objects mapping to a file using your preferred format (e.g., pickle or json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (misinformation_sim)",
   "language": "python",
   "name": "pycharm-9607488f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
