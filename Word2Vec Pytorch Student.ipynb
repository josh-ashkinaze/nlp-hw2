{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Parts 1-3: Word2Vec\n",
    "\n",
    "This homework will have you implementing word2vec using PyTorch and let you familiarize yourself with building more complex neural networks and the larger PyTorch development infrastructure.\n",
    "\n",
    "Broadly, this homework consists of a few major parts:\n",
    "1. Implement a `Corpus` class that will load the dataset and convert it to a sequence of token ids\n",
    "2. Implement negative sampling to select tokens to be used as negative examples of words in the context\n",
    "3. Create your dataset of positive and negative examples per context and load it into PyTorch's `DataLoader` to use for sampling\n",
    "4. Implement a `Word2Vec` class that is a PyTorch neural network\n",
    "5. Implement a training loop that samples a _batch_ of target words and their respective positive/negative context words\n",
    "6. Implement rare word removal and frequent word subsampling\n",
    "7. Run your model on the full dataset for at least one epoch\n",
    "8. Do the exploratory parts of the homework\n",
    "9. Save vectors and word-indexing data for later use in training a classifier\n",
    "\n",
    "After Step 5, you should be able to run your word2vec implementation on a small dataset and verify that it's learning correctly. Once you can verify everything is working, proceed with steps 6 and beyond. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**\n",
    "\n",
    "### Estimated performance times on medium dataset\n",
    "\n",
    "We designed this homework to be run on a laptop-grade CPU, so no GPU is required. If your primary computing device is a tablet or similar device, this homework can also be _developed_ on that device but then run on a more powerful machine in the Great Lakes cluster (for free). Such cases are the exception though. Following, we report on the estimated times from our reference implementation on the medium dataset for longer-running or data-intensive pieces of the homework. Your timing may vary based on implementation design; major differences in time (e.g., 10x longer) usually point to a performance bug.\n",
    "\n",
    "* Reading and tokenizing: ~5 seconds\n",
    "* Subsampling and converting to token ids: ~15 seconds\n",
    "* Generating the list of training examples: ~2 minutes (~15 minutes before the random number generator fix)\n",
    "* Training one epoch: ~12 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x154b47955810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torch\n",
    "# !pip install tqdm \n",
    "# !pip install nltk \n",
    "# !pip install gensim \n",
    "# !pip install wandb\n",
    "# !pip install pandas\n",
    "# !pip install seaborn \n",
    "# !pip install wandb\n",
    "# !pip install scipy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Handy command-line argument parsing\n",
    "import argparse\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'cat']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokenizer.tokenize(text=\"I am a cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an efficient random number generator (do this part this later)\n",
    "\n",
    "Computers have to work to generate random numbers. However, the effort for getting those random numbers varies by how many you ask for. In practice, it's _much_ more efficient to generate many random numbers at once, rather than one at a time. \n",
    "\n",
    "\n",
    "In generating the training data for word2vec, you'll be generating a lot of random numbers. We've added a helpful class that you can use to eventually speed things up. You should use an instance of this `RandomNumberGenerator` class to generate the numbers you need (rather than using `np.random`). This class should work as a quick drop-in in its current implementation. **You should change the implementation of this class _only_ after getting the rest of your code debugged**. Once you get things working, update the code in this class so that it will create large buffers of random numbers and then when asked for a new number, read the next number from the buffer instead of calling `random` or `randint`. Essentially, this class will pre-allocate many random numbers ahead of time and then return them in order to avoid the overhead of generating them one at a time. You could see up to an ~80% performance improvement in your negatve sampling code generation as a result of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNumberGenerator:\n",
    "    ''' \n",
    "    A wrapper class for a random number generator that will (eventually) hold buffers of pre-generated random numbers for\n",
    "    faster access. For now, it just calls np.random.randint and np.random.random to generate these numbers \n",
    "    at the time they are needed.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, buffer_size, seed=12345):\n",
    "        '''\n",
    "        Initializes the random number generator with a seed and a buffer size of random numbers to use\n",
    "\n",
    "        Args:\n",
    "            buffer_size: The number of random numbers to pre-generate. You will eventually want \n",
    "                         this to be a large-enough number than you're not frequently regenerating the buffer\n",
    "            seed: The seed for the random number generator\n",
    "        '''\n",
    "        self.max_val = -1\n",
    "        # TODO (later): create a random number generator using numpy and set its seed    \n",
    "        # TODO (later): pre-generate a buffer of random floats to use for random()\n",
    "\n",
    "    def random(self):\n",
    "        '''\n",
    "        Returns a random float value between 0 and 1\n",
    "        '''\n",
    "        # TODO (later): get a random number from the float buffer, rather than calling np.random.random\n",
    "        # NOTE: If you reach the end of the buffer, you should refill it with new random float numbers\n",
    "        return np.random.random()\n",
    "\n",
    "    def set_max_val(self, max_val):\n",
    "        '''\n",
    "        Sets the maximum integer value for randint and creates a buffer of random integers\n",
    "        '''\n",
    "        self.max_val = max_val\n",
    "        # NOTE: This default implemenation just sets the max_val and does not create a buffer of random integers\n",
    "        # TODO (later): Implement a buffer of random integers (for now, we'll just use np.random.randint)\n",
    "\n",
    "    def randint(self):\n",
    "        '''\n",
    "        Returns a random int value between 0 and self.max_val (inclusive)\n",
    "        '''        \n",
    "        if self.max_val == -1:\n",
    "            raise ValueError(\"Need to call set_max_val before calling randint\")\n",
    "        \n",
    "        # TODO (later): get a random number from the int buffer, rather than calling np.random.randint\n",
    "        # NOTE: If you reach the end of the buffer, you should refill it with new random ints\n",
    "        return np.random.randint(0, self.max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class to hold the data\n",
    "\n",
    "Before we get to training word2vec, we'll need to process the corpus into some representation. The `Corpus` class will handle much of the functionality for corpus reading and keeping track of which word types belong to which ids. The `Corpus` class will also handle the crucial functionality of generating negative samples for training (i.e., randomly-sampled words that were not in the target word's context).\n",
    "\n",
    "Some parts of this class can be completed after you've gotten word2vec up and running, so see the notes below and the details in the homework PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, rng):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.rng = rng\n",
    "\n",
    "        # These state variables become populated with function calls\n",
    "        #\n",
    "        # 1. load_data()\n",
    "        # 2. generate_negative_sampling_table()\n",
    "        #\n",
    "        # See those functions for how the various values get filled in\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.full_token_sequence_as_ids = None\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "        \n",
    "        with open(file_name, 'r') as f:\n",
    "            l = f.readlines()  \n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        all_tokens = []\n",
    "        print('Reading data and tokenizing')\n",
    "        for line in l:\n",
    "            tokens = self.tokenizer.tokenize(line)\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        print('Counting token frequencies')\n",
    "        token_counts = dict(Counter(all_tokens))\n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK>\n",
    "        # token. \n",
    "        #\n",
    "        # NOTE: You can do this step later if needed\n",
    "        \n",
    "        print(\"Performing minimum thresholding\")\n",
    "        tokens_to_replace = {token for token, count in token_counts.items() if count < min_token_freq}\n",
    "        filtered_tokens = [token if token not in tokens_to_replace else \"<UNK>\" for token in all_tokens]\n",
    "\n",
    "\n",
    "\n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "        # occurs (including <UNK>)\n",
    "        self.word_counts = dict(Counter(filtered_tokens))\n",
    "        # print(\"UNK\", self.word_counts['<UNK>'])\n",
    "\n",
    "        \n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping.\n",
    "        \n",
    "        # Create word_to_index and index_to_word mappings\n",
    "\n",
    "        unique_words = list(self.word_counts.keys())\n",
    "        self.word_to_index = {word: i for i, word in enumerate(unique_words)}\n",
    "        self.index_to_word = {i: word for word, i in self.word_to_index.items()}\n",
    "        self.full_token_sequence_as_ids = [self.word_to_index[token] for token in filtered_tokens]\n",
    "        \n",
    "        # Step 6: Compute the probability of keeping any particular *token* of a\n",
    "        # word in the training sequence, which we'll use to subsample. This subsampling\n",
    "        # avoids having the training data be filled with many overly common words\n",
    "        # as positive examples in the context\n",
    "   \n",
    "                        \n",
    "        # Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "        # a new list self.full_token_sequence_as_ids where \n",
    "        #\n",
    "        # (1) we probabilistically choose whether to keep each *token* based on the\n",
    "        # subsampling probabilities (note that this does not mean we drop\n",
    "        # an entire word!) and \n",
    "        #\n",
    "        # (2) all tokens are convered to their unique ids for faster training.\n",
    "        #\n",
    "        # NOTE: You can skip the subsampling part and just do step 2 to get\n",
    "        # your model up and running.\n",
    "            \n",
    "        # NOTE 2: You will perform token-based subsampling based on the probabilities in\n",
    "        # word_to_sample_prob. When subsampling, you are modifying the sequence itself \n",
    "        # (like deleting an item in a list). This action effectively makes the context\n",
    "        # window  larger for some target words by removing context words that are common\n",
    "        # from a particular context before the training occurs (which then would now include\n",
    "        # other words that were previously just outside the window).\n",
    "\n",
    "#         t = 0.001\n",
    "#         total_words = sum(self.word_counts.values())\n",
    "#         word_fractions = {word: count / total_words for word, count in self.word_counts.items()}\n",
    "#         word_probabilities = {word: (np.sqrt(fraction / t) + 1) * (t / fraction) \n",
    "#                               for word, fraction in word_fractions.items()}\n",
    "\n",
    "#         thresholded_tokens = []\n",
    "#         for token in filtered_tokens:\n",
    "#             prob = word_probabilities.get(token, 0)\n",
    "#             if np.random.random() < prob:\n",
    "#                 thresholded_tokens.append(token)\n",
    "\n",
    "#         self.full_token_sequence_as_ids = [self.word_to_index[token] for token in thresholded_tokens]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "        \n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "        '''\n",
    "        Generates a big list data structure that we can quickly randomly index into\n",
    "        in order to select a negative training example (i.e., a word that was\n",
    "        *not* present in the context). \n",
    "        '''       \n",
    "        \n",
    "        words = np.array(list(self.word_to_index.values()))\n",
    "        freqs = np.array(list(self.word_counts.values()))\n",
    "        weights = np.power(freqs, exp_power)\n",
    "        sum_weights = np.sum(weights)\n",
    "        probs = weights / sum_weights\n",
    "    \n",
    "        \n",
    "        # Step 1: Figure out how many instances of each word need to go into the\n",
    "        # negative sampling table. \n",
    "        #\n",
    "        # HINT: np.power and np.fill might be useful here        \n",
    "        print(\"Generating sampling table\")\n",
    "        samples_per_word = probs * table_size\n",
    "\n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "        adjusted_table_size = int(np.sum(np.floor(samples_per_word)))\n",
    "\n",
    "\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled.\n",
    "        #\n",
    "        # Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "        # 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "        # (before converting the words to IDs):\n",
    "        #\n",
    "        # [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "        #\n",
    "        \n",
    "        sampling_table = []\n",
    "        for word_idx, samples in enumerate(samples_per_word):\n",
    "            count = int(np.round(samples))\n",
    "            sampling_table.extend([word_idx]*count)\n",
    "        \n",
    "        self.negative_sampling_table = np.array(sampling_table)\n",
    "        \n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):\n",
    "        '''\n",
    "        Randomly samples the specified number of negative samples from the lookup\n",
    "        table and returns this list of IDs as a numpy array. As a performance\n",
    "        improvement, avoid sampling a negative example that has the same ID as\n",
    "        the current positive context word.\n",
    "        '''\n",
    "        #print(\"getting negative samples\")\n",
    "        results = []\n",
    "\n",
    "        # Create a list and sample from the negative_sampling_table to\n",
    "        # grow the list to num_samples, avoiding adding a negative example that\n",
    "        # has the same ID as the current context_word\n",
    "        while len(results) < num_samples:\n",
    "            rand_number = np.random.randint(0, len(self.negative_sampling_table))\n",
    "            sampled = self.negative_sampling_table[rand_number]\n",
    "            if sampled != cur_context_word_id:\n",
    "                results.append(sampled)\n",
    "        return np.array(results)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the corpus\n",
    "\n",
    "Now that we have code to turn the text into training data, let's do so. We've provided several files for you to help:\n",
    "\n",
    "* `reviews-word2vec.tiny.txt` -- use this to debug your corpus reader\n",
    "* `reviews-word2vec.med.txt` -- use this to debug/verify the whole word2vec works\n",
    "* `reviews-word2vec.large.txt.gz` -- use this when everything works to generate your vectors for later parts\n",
    "* `reviews-word2vec.HUGE.gz` -- _do not use this_ unless (1) everything works and (2) you really want to test/explore. This file is not needed at all to do your homework.\n",
    "\n",
    "We recommend starting to debug with the first file, as it is small and fast to load (quicker to find bugs). When debugging, we recommend setting the `min_token_freq` argument to 2 so that you can verify that part of the code is working but you still have enough word types left to test the rest.\n",
    "\n",
    "You'll use the remaining files later, where they're described.\n",
    "\n",
    "In the next cell, create your `Corpus`, read in the data, and generate the negative sampling table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from reviews-word2vec.tiny.txt; saw 20985 tokens (557 unique)\n",
      "Generating sampling table\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(rng=RandomNumberGenerator)\n",
    "corpus.load_data(file_name=\"reviews-word2vec.tiny.txt\", min_token_freq=5)\n",
    "corpus.generate_negative_sampling_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the training data\n",
    "\n",
    "Once we have the corpus ready, we need to generate our training dataset. Each instance in the dataset is a target word and positive and negative examples of contexts words. Given the target word as input, we'll want to predict (or not predict) these positive and negative context words as outputs using our network. Your task here is to create a python `list` of instances. \n",
    "\n",
    "Your final training data should be a list of tuples in the format ([target_word_id], [word_id_1, ...], [predicted_labels]), where each item in the list is a list:\n",
    "1. The first item is a list consisting only of the target word's ID.\n",
    "2. The second item is a list of word ids for both context words and negative samples \n",
    "3. The third item is a list of labels to predicted for each of the word ids in the second list (i.e., `1` for context words and `0` for negative samples). \n",
    "\n",
    "You will feed these tuples into the PyTorch `DatasetLoader` later that will do the converstion to `Tensor` objects. You will need to make sure that all of the lists in each tuple are `np.array` instances and are not plain python lists for this `Tensor` converstion to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data(corpus, window_size=2, num_negative_samples_per_target=2):\n",
    "    unk = corpus.word_to_index['<UNK>']\n",
    "    filtered_list = [x for x in corpus.full_token_sequence_as_ids if x != unk]\n",
    "    len_corpus = len(filtered_list)\n",
    "    max_words = window_size*2  + (num_negative_samples_per_target*(window_size)*2)\n",
    "\n",
    "    \n",
    "    training_data = []\n",
    "\n",
    "    for index, target_word_id in tqdm(enumerate(filtered_list)):\n",
    "        neg_ids = []\n",
    "        all_ids = []\n",
    "        labels = []\n",
    "\n",
    "        start_idx = max(0, index - window_size)\n",
    "        end_idx = min(len_corpus, index + window_size + 1)  \n",
    "\n",
    "        context_word_ids = filtered_list[start_idx:index] + filtered_list[index + 1:end_idx]\n",
    "\n",
    "        for _ in range(num_negative_samples_per_target * len(context_word_ids)):\n",
    "            neg_ids.extend(corpus.generate_negative_samples(target_word_id, 1))\n",
    "\n",
    "        total_ids = len(context_word_ids) + len(neg_ids)\n",
    "        gap = max_words - total_ids\n",
    "        if gap > 0:\n",
    "            extra_neg_ids = corpus.generate_negative_samples(target_word_id, gap)\n",
    "            neg_ids.extend(extra_neg_ids)\n",
    "\n",
    "        all_ids = context_word_ids + neg_ids\n",
    "        labels = [1] * len(context_word_ids) + [0] * len(neg_ids)\n",
    "\n",
    "        training_data.append((np.array([target_word_id]), np.array(all_ids), np.array(labels)))\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "\n",
    "# def gen_training_data(corpus, window_size=2, num_negative_samples_per_target=2):\n",
    "#     unk = corpus.word_to_index['<UNK>']\n",
    "#     filtered_list = [x for x in corpus.full_token_sequence_as_ids if x != unk]\n",
    "#     len_corpus = len(filtered_list)\n",
    "#     max_words = window_size*2  + (num_negative_samples_per_target*(window_size)*2)\n",
    "\n",
    "    \n",
    "#     training_data = []\n",
    "\n",
    "#     for index, target_word_id in enumerate(filtered_list):\n",
    "#         neg_ids = []\n",
    "#         all_ids = []\n",
    "#         labels = []\n",
    "\n",
    "#         start_idx = max(0, index - window_size)\n",
    "#         end_idx = min(len_corpus, index + window_size + 1)  \n",
    "\n",
    "#         context_word_ids = filtered_list[start_idx:index] + filtered_list[index + 1:end_idx]\n",
    "\n",
    "#         for _ in range(num_negative_samples_per_target * len(context_word_ids)):\n",
    "#             neg_ids.extend(corpus.generate_negative_samples(target_word_id, 1))\n",
    "\n",
    "#         total_ids = len(context_word_ids) + len(neg_ids)\n",
    "#         gap = max_words - total_ids\n",
    "#         if gap > 0:\n",
    "#             extra_neg_ids = corpus.generate_negative_samples(target_word_id, gap)\n",
    "#             neg_ids.extend(extra_neg_ids)\n",
    "\n",
    "#         all_ids = context_word_ids + neg_ids\n",
    "#         labels = [1] * len(context_word_ids) + [0] * len(neg_ids)\n",
    "\n",
    "#         training_data.append((np.array([target_word_id]), np.array(all_ids), np.array(labels)))\n",
    "\n",
    "#     return training_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10607824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c67363ba144da3bab25b547bf0730d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1515831/3694093237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_token_sequence_as_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1515831/3926115586.py\u001b[0m in \u001b[0;36mgen_training_data\u001b[0;34m(corpus, window_size, num_negative_samples_per_target)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_negative_samples_per_target\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_word_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mneg_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_negative_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_word_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_word_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1515831/3221338154.py\u001b[0m in \u001b[0;36mgenerate_negative_samples\u001b[0;34m(self, cur_context_word_id, num_samples)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msampled\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcur_context_word_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(len(corpus.full_token_sequence_as_ids))\n",
    "# print(td[20])\n",
    "\n",
    "td = gen_training_data(corpus)\n",
    "targets = [max(td[i][1]) for i in range(len(td))]\n",
    "print(max(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# targets = [td[i][0][0] for i in range(len(td))]\n",
    "# max(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = [max(td[i][1]) for i in range(len(td))]\n",
    "max(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10]),\n",
       " array([  8,   9,  17,  56, 130,   6, 231, 357,  87, 283,   4,  11]),\n",
       " array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network\n",
    "\n",
    "We'll create a new neural network as a subclass of `nn.Module` like we did in Homework 1. However, _unlike_ the network you built in Homework 1, we do not need to used linear layers to implement word2vec. Instead, we will use PyTorch's `Emedding` class, which maps an index (e.g., a word id in this case) to an embedding. \n",
    "\n",
    "Roughly speaking, word2vec's network makes a prediction by computing the dot product of the target word's embedding and a context word's embedding and then passing this dot product through the sigmoid function ($\\sigma$) to predict the probability that the context word was actually in the context. The homework write-up has lots of details on how this works. Your `forward()` function will have to implement this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        # Save what state you want and create the embeddings for your\n",
    "        # target and context words\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Initialize embeddings with non-zero random numbers\n",
    "        self.init_emb(init_range=0.5/self.vocab_size)\n",
    "        \n",
    "    def init_emb(self, init_range):\n",
    "        \n",
    "        # Fill your two embeddings with random numbers uniformly sampled\n",
    "        # between +/- init_range\n",
    "    \n",
    "        self.target_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, target_word_id, context_word_ids):\n",
    "        ''' \n",
    "        Predicts whether each context word was actually in the context of the target word.\n",
    "        The input is a tensor with a single target word's id and a tensor containing each\n",
    "        of the context words' ids (this includes both positive and negative examples).\n",
    "        '''\n",
    "        \n",
    "        # NOTE 1: This is probably the hardest part of the homework, so you'll\n",
    "        # need to figure out how to do the dot-product between embeddings and return\n",
    "        # the sigmoid. Be prepared for lots of debugging. For some reference,\n",
    "        # our implementation is three lines and really the hard part is just\n",
    "        # the last line. However, it's usually a matter of figuring out what\n",
    "        # that one line looks like that ends up being the hard part.\n",
    "        \n",
    "        # NOTE 2: In this homework you'll be dealing with *batches* of instances\n",
    "        # rather than a single instance at once. PyTorch mostly handles this\n",
    "        # seamlessly under the hood for you (which is very nice) but batching\n",
    "        # can show in weird ways and create challenges in debugging initially.\n",
    "        # For one, your inputs will get an extra dimension. So, for example,\n",
    "        # if you have a batch size of 4, your input for target_word_id will\n",
    "        # really be 4 x 1. If you get the embeddings of those targets,\n",
    "        # it then becomes 4x50! The same applies to the context_word_ids, except\n",
    "        # that was alreayd a list so now you have things with shape \n",
    "        #\n",
    "        #    (batch x context_words x embedding_size)\n",
    "        #\n",
    "        # One of your tasks will be to figure out how to get things lined up\n",
    "        # so everything \"just works\". When it does, the code looks surprisingly\n",
    "        # simple, but it might take a lot of debugging (or not!) to get there.\n",
    "        \n",
    "        # NOTE 3: We *strongly* discourage you from looking for existing \n",
    "        # implementations of word2vec online. Sadly, having reviewed most of the\n",
    "        # highly-visible ones, they are actually wrong (wow!) or are doing\n",
    "        # inefficient things like computing the full softmax instead of doing\n",
    "        # the negative sampling. Looking at these will likely leave you more\n",
    "        # confused than if you just tried to figure it out yourself.\n",
    "        \n",
    "        # NOTE 4: There many ways to implement this, some more efficient\n",
    "        # than others. You will want to get it working first and then\n",
    "        # test the timing to see how long it takes. As long as the\n",
    "        # code works (vector comparisons look good) you'll receive full\n",
    "        # credit. However, very slow implementations may take hours(!)\n",
    "        # to converge so plan ahead.\n",
    "        \n",
    "        \n",
    "        # Hint 1: You may want to review the mathematical operations on how\n",
    "        # to compute the dot product to see how to do these\n",
    "        \n",
    "        target_embeddings = self.target_embeddings.weight[target_word_ids]  \n",
    "        context_embeddings = self.context_embeddings.weight[context_word_ids]  \n",
    "        preds = torch.sigmoid(torch.bmm(target_embeddings, context_embeddings.transpose(1, 2) )).squeeze()\n",
    "\n",
    "\n",
    "    \n",
    "        return preds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# a =dict( enumerate(loss_l))\n",
    "# b = pd.DataFrame(a, index=[0]).T.reset_index()\n",
    "# b.columns = ['log_time', 'val']\n",
    "# sns.lineplot(data=b, x='log_time', y='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network!\n",
    "\n",
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The trainin code will look surprisingly similar at times to your pytorch code from Homework 1 since all networks share the same base training setup. However, we'll add a few new elements to get you familiar with more common training techniques. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. Create a new `SummaryWriter` to periodically write our running-sum of the loss to a tensorboard\n",
    "5. Train your model \n",
    "\n",
    "Two new elements show up. First, we'll be using `DataLoader` which is going to sample data for us and put it in a batch (and also convert the data to `Tensor` objects. You can iterate over the batches and each iteration will return all the items eventually, one batch at a time (a full epoch's worth).\n",
    "\n",
    "The second new part is using `wandb`. As you might have noticed in Homework 1, training neural models can take some time. [Weights & Biases](https://wandb.ai/) is a handy web-based view that you can check during training to see how the model is doing. We'll use it here and periodically log a running sum of the loss after a set number of steps. The Homework write up has a plot of what this looks like. We'll be doing something simple here with wandb but it will come in handy later as you train larger models (for longer) and may want to visually check if your model is converging and is [easy to integrate](https://docs.wandb.ai/guides/integrations/pytorch).\n",
    "\n",
    "Once you get the code working, to start training, we recommend training on the `reviews-word2vec.med.txt` dataset. This data is small enough you can get through an epoch in a few minutes (or less) while still being large enough you can test whether the model is learning anything by examining common words. Below this cell we've added a few helper functions that you can use to debug and query your model. In particular, the `get_neighbors()` function is a great way to test: if your model has learned anything, the nearest neighbors for common words should seem reasonable (without having to jump through mental hoops). An easy word to test on the `med` data is \"january\" which should return month-related words as being most similar.\n",
    "\n",
    "**NOTE**: Since we're training biographies, the text itself will be skewed towards words likely to show up biographices--which isn't necessary like \"regular\" text. You may find that your model has few instances of words you think are common, or that the model learns poor or unusual neighbors for these. When querying the neighbors, it can help to think of which words you think are likely to show up in biographies on Wikipedia and use those as probes to see what the model has learned.\n",
    "\n",
    "Once you're convinced the model is learning, switch to the `med` data and train your model as specified in the PDF. Once trained, save your model using the `save()` function at the end of the notebook. This function records your data in a common format for word2vec vectors and lets you load the vectors into other libraries that have more advanced functionality. In particular, you can use the [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) code in other notebook included to explore the vectors and do simple vector analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****PROCESSING TINY DATA****\n",
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from reviews-word2vec.tiny.txt; saw 20985 tokens (557 unique)\n",
      "STATUS: Loaded data\n",
      "Generating sampling table\n",
      "STATUS: Loaded negative sampling data\n",
      "STATUS: Starting training data generation at time 1709385003.9560635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf38b5e74874706b49a51d3c6be5c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS: Finished training data generation at time 1709385004.7592645\n",
      "****PROCESSING MED DATA****\n",
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from reviews-word2vec.med.txt; saw 10607824 tokens (35193 unique)\n",
      "STATUS: Loaded data\n",
      "Generating sampling table\n",
      "STATUS: Loaded negative sampling data\n",
      "STATUS: Starting training data generation at time 1709385011.6601784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ace5d2cfbbe450b91d955ba450238c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS: Finished training data generation at time 1709385513.5090294\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle \n",
    "\n",
    "\n",
    "##########################\n",
    "# DATA\n",
    "##########################\n",
    "\n",
    "process_med = True\n",
    "\n",
    "# TINY DATA\n",
    "print(\"****PROCESSING TINY DATA****\")\n",
    "corpus_s = Corpus(rng=RandomNumberGenerator)\n",
    "corpus_s.load_data(file_name=\"reviews-word2vec.tiny.txt\", min_token_freq=5)\n",
    "print(\"STATUS: Loaded data\")\n",
    "corpus_s.generate_negative_sampling_table()\n",
    "print(\"STATUS: Loaded negative sampling data\")\n",
    "print(\"STATUS: Starting training data generation at time\", time.time())\n",
    "training_data_s = gen_training_data(corpus_s)\n",
    "print(\"STATUS: Finished training data generation at time\", time.time())\n",
    "\n",
    "\n",
    "# MED DATA\n",
    "if process_med:\n",
    "    print(\"****PROCESSING MED DATA****\")\n",
    "    corpus = Corpus(rng=RandomNumberGenerator)\n",
    "    corpus.load_data(file_name=\"reviews-word2vec.med.txt\", min_token_freq=5)\n",
    "    print(\"STATUS: Loaded data\")\n",
    "    corpus.generate_negative_sampling_table()\n",
    "    print(\"STATUS: Loaded negative sampling data\")\n",
    "    print(\"STATUS: Starting training data generation at time\", time.time())\n",
    "    training_data = gen_training_data(corpus)\n",
    "    print(\"STATUS: Finished training data generation at time\", time.time())\n",
    "# with open(\"med_train.pkl\", 'rb') as file:\n",
    "#     training_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([28]),\n",
       " array([ 26,  27,  29,  24,  17, 537, 257, 420,  61,  46,  78, 184]),\n",
       " array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_s[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjashkina\u001b[0m (\u001b[33mjashkina2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jashkina/nlp2/nlp-hw2/nlp-hw2/wandb/run-20240302_082155-ntvy4a0g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jashkina2/medium_run_very_fast_l5/runs/ntvy4a0g' target=\"_blank\">medium_run_very_fast_l5</a></strong> to <a href='https://wandb.ai/jashkina2/medium_run_very_fast_l5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jashkina2/medium_run_very_fast_l5' target=\"_blank\">https://wandb.ai/jashkina2/medium_run_very_fast_l5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jashkina2/medium_run_very_fast_l5/runs/ntvy4a0g' target=\"_blank\">https://wandb.ai/jashkina2/medium_run_very_fast_l5/runs/ntvy4a0g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time 1709385717.8745172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a6d209017645abaf5866ac6a16521a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163495 [00:03<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709385728.8541417\n",
      "step 1, epoch 0 (total steps = 1): 0.6931471824645996\n",
      "1709385729.3717442\n",
      "step 101, epoch 0 (total steps = 101): 69.31467640399933\n",
      "1709385729.795953\n",
      "step 201, epoch 0 (total steps = 201): 69.31389659643173\n",
      "1709385730.1400223\n",
      "step 301, epoch 0 (total steps = 301): 69.3112923502922\n",
      "1709385730.5884125\n",
      "step 401, epoch 0 (total steps = 401): 69.30637866258621\n",
      "1709385731.048321\n",
      "step 501, epoch 0 (total steps = 501): 69.29888826608658\n",
      "1709385731.4439123\n",
      "step 601, epoch 0 (total steps = 601): 69.28847342729568\n",
      "1709385731.8783937\n",
      "step 701, epoch 0 (total steps = 701): 69.2745612859726\n",
      "1709385732.2210844\n",
      "step 801, epoch 0 (total steps = 801): 69.25984066724777\n",
      "1709385732.6863742\n",
      "step 901, epoch 0 (total steps = 901): 69.24280798435211\n",
      "1709385733.0486119\n",
      "step 1001, epoch 0 (total steps = 1001): 69.22802984714508\n",
      "1709385733.4126234\n",
      "step 1101, epoch 0 (total steps = 1101): 69.19953006505966\n",
      "1709385733.7578878\n",
      "step 1201, epoch 0 (total steps = 1201): 69.1818026304245\n",
      "1709385734.1109467\n",
      "step 1301, epoch 0 (total steps = 1301): 69.15432542562485\n",
      "1709385734.4888735\n",
      "step 1401, epoch 0 (total steps = 1401): 69.12107664346695\n",
      "1709385734.8561196\n",
      "step 1501, epoch 0 (total steps = 1501): 69.09966999292374\n",
      "1709385735.210248\n",
      "step 1601, epoch 0 (total steps = 1601): 69.06041216850281\n",
      "1709385735.556004\n",
      "step 1701, epoch 0 (total steps = 1701): 69.03121966123581\n",
      "1709385735.9164927\n",
      "step 1801, epoch 0 (total steps = 1801): 68.99247634410858\n",
      "1709385736.2988272\n",
      "step 1901, epoch 0 (total steps = 1901): 68.96385109424591\n",
      "1709385736.6448367\n",
      "step 2001, epoch 0 (total steps = 2001): 68.93447053432465\n",
      "1709385737.0038192\n",
      "step 2101, epoch 0 (total steps = 2101): 68.90593558549881\n",
      "1709385737.3726654\n",
      "step 2201, epoch 0 (total steps = 2201): 68.8500188589096\n",
      "1709385737.7561154\n",
      "step 2301, epoch 0 (total steps = 2301): 68.80803233385086\n",
      "1709385738.1583686\n",
      "step 2401, epoch 0 (total steps = 2401): 68.7879604101181\n",
      "1709385738.5022383\n",
      "step 2501, epoch 0 (total steps = 2501): 68.75369328260422\n",
      "1709385738.858284\n",
      "step 2601, epoch 0 (total steps = 2601): 68.71750754117966\n",
      "1709385739.2139602\n",
      "step 2701, epoch 0 (total steps = 2701): 68.67781054973602\n",
      "1709385739.563106\n",
      "step 2801, epoch 0 (total steps = 2801): 68.62910372018814\n",
      "1709385739.9117353\n",
      "step 2901, epoch 0 (total steps = 2901): 68.5753602385521\n",
      "1709385740.265286\n",
      "step 3001, epoch 0 (total steps = 3001): 68.51866656541824\n",
      "1709385740.6150622\n",
      "step 3101, epoch 0 (total steps = 3101): 68.4988541007042\n",
      "1709385740.9776216\n",
      "step 3201, epoch 0 (total steps = 3201): 68.47785073518753\n",
      "1709385741.3177767\n",
      "step 3301, epoch 0 (total steps = 3301): 68.41224646568298\n",
      "1709385741.6655858\n",
      "step 3401, epoch 0 (total steps = 3401): 68.3736714720726\n",
      "1709385742.0010405\n",
      "step 3501, epoch 0 (total steps = 3501): 68.3642286658287\n",
      "1709385742.3517423\n",
      "step 3601, epoch 0 (total steps = 3601): 68.26423281431198\n",
      "1709385742.6927526\n",
      "step 3701, epoch 0 (total steps = 3701): 68.26439648866653\n",
      "1709385743.0340805\n",
      "step 3801, epoch 0 (total steps = 3801): 68.22699630260468\n",
      "1709385743.389355\n",
      "step 3901, epoch 0 (total steps = 3901): 68.16800725460052\n",
      "1709385743.723454\n",
      "step 4001, epoch 0 (total steps = 4001): 68.08277410268784\n",
      "1709385744.0693831\n",
      "step 4101, epoch 0 (total steps = 4101): 68.10758191347122\n",
      "1709385744.4097302\n",
      "step 4201, epoch 0 (total steps = 4201): 68.03617894649506\n",
      "1709385744.7530615\n",
      "step 4301, epoch 0 (total steps = 4301): 67.98738265037537\n",
      "1709385745.0869586\n",
      "step 4401, epoch 0 (total steps = 4401): 67.95124769210815\n",
      "1709385745.4377708\n",
      "step 4501, epoch 0 (total steps = 4501): 67.9301530122757\n",
      "1709385745.795578\n",
      "step 4601, epoch 0 (total steps = 4601): 67.857128739357\n",
      "1709385746.1400251\n",
      "step 4701, epoch 0 (total steps = 4701): 67.81288409233093\n",
      "1709385746.497625\n",
      "step 4801, epoch 0 (total steps = 4801): 67.75412493944168\n",
      "1709385746.8653295\n",
      "step 4901, epoch 0 (total steps = 4901): 67.66192770004272\n",
      "1709385747.230706\n",
      "step 5001, epoch 0 (total steps = 5001): 67.73280870914459\n",
      "1709385747.594608\n",
      "step 5101, epoch 0 (total steps = 5101): 67.69156229496002\n",
      "1709385747.9644072\n",
      "step 5201, epoch 0 (total steps = 5201): 67.5774627327919\n",
      "1709385748.3162403\n",
      "step 5301, epoch 0 (total steps = 5301): 67.53572022914886\n",
      "1709385748.6593974\n",
      "step 5401, epoch 0 (total steps = 5401): 67.56561034917831\n",
      "1709385749.0033405\n",
      "step 5501, epoch 0 (total steps = 5501): 67.41394245624542\n",
      "1709385749.3440459\n",
      "step 5601, epoch 0 (total steps = 5601): 67.41418695449829\n",
      "1709385749.6931548\n",
      "step 5701, epoch 0 (total steps = 5701): 67.40877264738083\n",
      "1709385750.0397623\n",
      "step 5801, epoch 0 (total steps = 5801): 67.36929279565811\n",
      "1709385750.4153469\n",
      "step 5901, epoch 0 (total steps = 5901): 67.35442596673965\n",
      "1709385750.7573385\n",
      "step 6001, epoch 0 (total steps = 6001): 67.25211668014526\n",
      "1709385751.1326056\n",
      "step 6101, epoch 0 (total steps = 6101): 67.17547816038132\n",
      "1709385751.4930696\n",
      "step 6201, epoch 0 (total steps = 6201): 67.12233465909958\n",
      "1709385751.851866\n",
      "step 6301, epoch 0 (total steps = 6301): 67.18996661901474\n",
      "1709385752.2056017\n",
      "step 6401, epoch 0 (total steps = 6401): 67.13382518291473\n",
      "1709385752.551703\n",
      "step 6501, epoch 0 (total steps = 6501): 67.06099516153336\n",
      "1709385752.8855436\n",
      "step 6601, epoch 0 (total steps = 6601): 66.97528517246246\n",
      "1709385753.2536657\n",
      "step 6701, epoch 0 (total steps = 6701): 66.98356807231903\n",
      "1709385753.624259\n",
      "step 6801, epoch 0 (total steps = 6801): 66.92659163475037\n",
      "1709385753.9951463\n",
      "step 6901, epoch 0 (total steps = 6901): 66.88554406166077\n",
      "1709385754.3479218\n",
      "step 7001, epoch 0 (total steps = 7001): 66.79767662286758\n",
      "1709385754.6826434\n",
      "step 7101, epoch 0 (total steps = 7101): 66.71417611837387\n",
      "1709385755.0387678\n",
      "step 7201, epoch 0 (total steps = 7201): 66.67757511138916\n",
      "1709385755.3990245\n",
      "step 7301, epoch 0 (total steps = 7301): 66.65983992815018\n",
      "1709385755.742088\n",
      "step 7401, epoch 0 (total steps = 7401): 66.75151938199997\n",
      "1709385756.0969112\n",
      "step 7501, epoch 0 (total steps = 7501): 66.62511503696442\n",
      "1709385756.4386404\n",
      "step 7601, epoch 0 (total steps = 7601): 66.52002447843552\n",
      "1709385756.7986205\n",
      "step 7701, epoch 0 (total steps = 7701): 66.49407505989075\n",
      "1709385757.1500928\n",
      "step 7801, epoch 0 (total steps = 7801): 66.44186919927597\n",
      "1709385757.500617\n",
      "step 7901, epoch 0 (total steps = 7901): 66.46041423082352\n",
      "1709385757.851177\n",
      "step 8001, epoch 0 (total steps = 8001): 66.34357672929764\n",
      "1709385758.206724\n",
      "step 8101, epoch 0 (total steps = 8101): 66.31365394592285\n",
      "1709385758.5764236\n",
      "step 8201, epoch 0 (total steps = 8201): 66.44735145568848\n",
      "1709385758.9404933\n",
      "step 8301, epoch 0 (total steps = 8301): 66.30193966627121\n",
      "1709385759.2846084\n",
      "step 8401, epoch 0 (total steps = 8401): 66.21452528238297\n",
      "1709385759.6624284\n",
      "step 8501, epoch 0 (total steps = 8501): 66.15729081630707\n",
      "1709385760.017514\n",
      "step 8601, epoch 0 (total steps = 8601): 66.20366352796555\n",
      "1709385760.3604016\n",
      "step 8701, epoch 0 (total steps = 8701): 66.09962165355682\n",
      "1709385760.7023811\n",
      "step 8801, epoch 0 (total steps = 8801): 66.00721526145935\n",
      "1709385761.0917394\n",
      "step 8901, epoch 0 (total steps = 8901): 66.03512299060822\n",
      "1709385761.4423516\n",
      "step 9001, epoch 0 (total steps = 9001): 66.00533843040466\n",
      "1709385761.8121061\n",
      "step 9101, epoch 0 (total steps = 9101): 65.89952600002289\n",
      "1709385762.1736312\n",
      "step 9201, epoch 0 (total steps = 9201): 65.88657104969025\n",
      "1709385762.5223873\n",
      "step 9301, epoch 0 (total steps = 9301): 65.80453115701675\n",
      "1709385762.8650625\n",
      "step 9401, epoch 0 (total steps = 9401): 65.83406281471252\n",
      "1709385763.2141716\n",
      "step 9501, epoch 0 (total steps = 9501): 65.7137445807457\n",
      "1709385763.5544088\n",
      "step 9601, epoch 0 (total steps = 9601): 65.81980019807816\n",
      "1709385763.9228976\n",
      "step 9701, epoch 0 (total steps = 9701): 65.69924080371857\n",
      "1709385764.3140073\n",
      "step 9801, epoch 0 (total steps = 9801): 65.7052863240242\n",
      "1709385764.6564388\n",
      "step 9901, epoch 0 (total steps = 9901): 65.72867572307587\n",
      "1709385764.993987\n",
      "step 10001, epoch 0 (total steps = 10001): 65.59203094244003\n",
      "1709385765.3431168\n",
      "step 10101, epoch 0 (total steps = 10101): 65.57236433029175\n",
      "1709385765.6935873\n",
      "step 10201, epoch 0 (total steps = 10201): 65.54059290885925\n",
      "1709385766.0740843\n",
      "step 10301, epoch 0 (total steps = 10301): 65.43646550178528\n",
      "1709385766.4146342\n",
      "step 10401, epoch 0 (total steps = 10401): 65.49984872341156\n",
      "1709385766.7662687\n",
      "step 10501, epoch 0 (total steps = 10501): 65.3977552652359\n",
      "1709385767.1197731\n",
      "step 10601, epoch 0 (total steps = 10601): 65.29834282398224\n",
      "1709385767.4690053\n",
      "step 10701, epoch 0 (total steps = 10701): 65.37195611000061\n",
      "1709385767.818161\n",
      "step 10801, epoch 0 (total steps = 10801): 65.35256457328796\n",
      "1709385768.1842465\n",
      "step 10901, epoch 0 (total steps = 10901): 65.33223348855972\n",
      "1709385768.5287957\n",
      "step 11001, epoch 0 (total steps = 11001): 65.17475843429565\n",
      "1709385768.8953667\n",
      "step 11101, epoch 0 (total steps = 11101): 65.02533710002899\n",
      "1709385769.2695272\n",
      "step 11201, epoch 0 (total steps = 11201): 65.13584941625595\n",
      "1709385769.639052\n",
      "step 11301, epoch 0 (total steps = 11301): 65.05189591646194\n",
      "1709385769.9801464\n",
      "step 11401, epoch 0 (total steps = 11401): 64.9134236574173\n",
      "1709385770.336774\n",
      "step 11501, epoch 0 (total steps = 11501): 65.11787950992584\n",
      "1709385770.7052045\n",
      "step 11601, epoch 0 (total steps = 11601): 65.03314220905304\n",
      "1709385771.0438368\n",
      "step 11701, epoch 0 (total steps = 11701): 64.87718510627747\n",
      "1709385771.3907018\n",
      "step 11801, epoch 0 (total steps = 11801): 64.9403287768364\n",
      "1709385771.7401564\n",
      "step 11901, epoch 0 (total steps = 11901): 64.81028521060944\n",
      "1709385772.0864797\n",
      "step 12001, epoch 0 (total steps = 12001): 64.81499201059341\n",
      "1709385772.4248254\n",
      "step 12101, epoch 0 (total steps = 12101): 64.83243423700333\n",
      "1709385772.772886\n",
      "step 12201, epoch 0 (total steps = 12201): 64.88780987262726\n",
      "1709385773.1109388\n",
      "step 12301, epoch 0 (total steps = 12301): 64.79063701629639\n",
      "1709385773.4548135\n",
      "step 12401, epoch 0 (total steps = 12401): 64.66006052494049\n",
      "1709385773.8031492\n",
      "step 12501, epoch 0 (total steps = 12501): 64.7088834643364\n",
      "1709385774.1471438\n",
      "step 12601, epoch 0 (total steps = 12601): 64.65015482902527\n",
      "1709385774.4929855\n",
      "step 12701, epoch 0 (total steps = 12701): 64.66246384382248\n",
      "1709385774.8452528\n",
      "step 12801, epoch 0 (total steps = 12801): 64.58249521255493\n",
      "1709385775.1990526\n",
      "step 12901, epoch 0 (total steps = 12901): 64.5889602303505\n",
      "1709385775.5757158\n",
      "step 13001, epoch 0 (total steps = 13001): 64.5266804099083\n",
      "1709385775.9081423\n",
      "step 13101, epoch 0 (total steps = 13101): 64.49539911746979\n",
      "1709385776.2500448\n",
      "step 13201, epoch 0 (total steps = 13201): 64.37725907564163\n",
      "1709385776.5946753\n",
      "step 13301, epoch 0 (total steps = 13301): 64.29162055253983\n",
      "1709385776.9342368\n",
      "step 13401, epoch 0 (total steps = 13401): 64.3400050997734\n",
      "1709385777.2826838\n",
      "step 13501, epoch 0 (total steps = 13501): 64.34891939163208\n",
      "1709385777.6367486\n",
      "step 13601, epoch 0 (total steps = 13601): 64.3711279630661\n",
      "1709385777.9827788\n",
      "step 13701, epoch 0 (total steps = 13701): 64.33212226629257\n",
      "1709385778.3206267\n",
      "step 13801, epoch 0 (total steps = 13801): 64.31001210212708\n",
      "1709385778.668682\n",
      "step 13901, epoch 0 (total steps = 13901): 64.21013379096985\n",
      "1709385779.0131638\n",
      "step 14001, epoch 0 (total steps = 14001): 64.30874413251877\n",
      "1709385779.3711944\n",
      "step 14101, epoch 0 (total steps = 14101): 64.17091554403305\n",
      "1709385779.7156656\n",
      "step 14201, epoch 0 (total steps = 14201): 64.16813772916794\n",
      "1709385780.086912\n",
      "step 14301, epoch 0 (total steps = 14301): 64.2093533873558\n",
      "1709385780.4330819\n",
      "step 14401, epoch 0 (total steps = 14401): 64.0534285902977\n",
      "1709385780.769841\n",
      "step 14501, epoch 0 (total steps = 14501): 64.07527977228165\n",
      "1709385781.1196034\n",
      "step 14601, epoch 0 (total steps = 14601): 64.1076409816742\n",
      "1709385781.495508\n",
      "step 14701, epoch 0 (total steps = 14701): 63.94505351781845\n",
      "1709385781.8379128\n",
      "step 14801, epoch 0 (total steps = 14801): 63.89171350002289\n",
      "1709385782.1893425\n",
      "step 14901, epoch 0 (total steps = 14901): 63.998195827007294\n",
      "1709385782.5329792\n",
      "step 15001, epoch 0 (total steps = 15001): 63.84579557180405\n",
      "1709385782.8950531\n",
      "step 15101, epoch 0 (total steps = 15101): 63.69694143533707\n",
      "1709385783.25782\n",
      "step 15201, epoch 0 (total steps = 15201): 63.92520344257355\n",
      "1709385783.6063154\n",
      "step 15301, epoch 0 (total steps = 15301): 63.75480532646179\n",
      "1709385783.956358\n",
      "step 15401, epoch 0 (total steps = 15401): 63.79655992984772\n",
      "1709385784.303662\n",
      "step 15501, epoch 0 (total steps = 15501): 63.722956359386444\n",
      "1709385784.6510236\n",
      "step 15601, epoch 0 (total steps = 15601): 63.83206969499588\n",
      "1709385785.020607\n",
      "step 15701, epoch 0 (total steps = 15701): 63.67269843816757\n",
      "1709385785.3584304\n",
      "step 15801, epoch 0 (total steps = 15801): 63.65723246335983\n",
      "1709385785.699739\n",
      "step 15901, epoch 0 (total steps = 15901): 63.62879878282547\n",
      "1709385786.039305\n",
      "step 16001, epoch 0 (total steps = 16001): 63.651763916015625\n",
      "1709385786.3821094\n",
      "step 16101, epoch 0 (total steps = 16101): 63.422816038131714\n",
      "1709385786.7424786\n",
      "step 16201, epoch 0 (total steps = 16201): 63.495830953121185\n",
      "1709385787.0953522\n",
      "step 16301, epoch 0 (total steps = 16301): 63.45173645019531\n",
      "1709385787.4459388\n",
      "step 16401, epoch 0 (total steps = 16401): 63.57484596967697\n",
      "1709385787.8003078\n",
      "step 16501, epoch 0 (total steps = 16501): 63.4930020570755\n",
      "1709385788.1485896\n",
      "step 16601, epoch 0 (total steps = 16601): 63.47552794218063\n",
      "1709385788.5161743\n",
      "step 16701, epoch 0 (total steps = 16701): 63.33827602863312\n",
      "1709385788.8576381\n",
      "step 16801, epoch 0 (total steps = 16801): 63.26427865028381\n",
      "1709385789.2002218\n",
      "step 16901, epoch 0 (total steps = 16901): 63.22498995065689\n",
      "1709385789.5592027\n",
      "step 17001, epoch 0 (total steps = 17001): 63.33252966403961\n",
      "1709385789.9110436\n",
      "step 17101, epoch 0 (total steps = 17101): 63.175376176834106\n",
      "1709385790.2603083\n",
      "step 17201, epoch 0 (total steps = 17201): 63.194720685482025\n",
      "1709385790.633829\n",
      "step 17301, epoch 0 (total steps = 17301): 63.178695261478424\n",
      "1709385790.979113\n",
      "step 17401, epoch 0 (total steps = 17401): 63.10976243019104\n",
      "1709385791.3441913\n",
      "step 17501, epoch 0 (total steps = 17501): 63.11449199914932\n",
      "1709385791.6905444\n",
      "step 17601, epoch 0 (total steps = 17601): 63.14174348115921\n",
      "1709385792.090788\n",
      "step 17701, epoch 0 (total steps = 17701): 63.10836672782898\n",
      "1709385792.4436135\n",
      "step 17801, epoch 0 (total steps = 17801): 62.971070528030396\n",
      "1709385792.8142395\n",
      "step 17901, epoch 0 (total steps = 17901): 62.90336436033249\n",
      "1709385793.171777\n",
      "step 18001, epoch 0 (total steps = 18001): 62.916924476623535\n",
      "1709385793.5193045\n",
      "step 18101, epoch 0 (total steps = 18101): 62.967794597148895\n",
      "1709385793.875485\n",
      "step 18201, epoch 0 (total steps = 18201): 62.92176902294159\n",
      "1709385794.2211425\n",
      "step 18301, epoch 0 (total steps = 18301): 63.15288853645325\n",
      "1709385794.5882018\n",
      "step 18401, epoch 0 (total steps = 18401): 62.955586194992065\n",
      "1709385794.9283001\n",
      "step 18501, epoch 0 (total steps = 18501): 62.898863554000854\n",
      "1709385795.2929983\n",
      "step 18601, epoch 0 (total steps = 18601): 62.99165713787079\n",
      "1709385795.680403\n",
      "step 18701, epoch 0 (total steps = 18701): 62.886515498161316\n",
      "1709385796.018823\n",
      "step 18801, epoch 0 (total steps = 18801): 62.9536959528923\n",
      "1709385796.3622174\n",
      "step 18901, epoch 0 (total steps = 18901): 62.7382847070694\n",
      "1709385796.7132945\n",
      "step 19001, epoch 0 (total steps = 19001): 62.76355355978012\n",
      "1709385797.052488\n",
      "step 19101, epoch 0 (total steps = 19101): 62.701247453689575\n",
      "1709385797.4170032\n",
      "step 19201, epoch 0 (total steps = 19201): 62.683968126773834\n",
      "1709385797.7642622\n",
      "step 19301, epoch 0 (total steps = 19301): 62.67438244819641\n",
      "1709385798.1448617\n",
      "step 19401, epoch 0 (total steps = 19401): 62.6740887761116\n",
      "1709385798.4893885\n",
      "step 19501, epoch 0 (total steps = 19501): 62.623802065849304\n",
      "1709385798.8287034\n",
      "step 19601, epoch 0 (total steps = 19601): 62.56095886230469\n",
      "1709385799.1668482\n",
      "step 19701, epoch 0 (total steps = 19701): 62.597143948078156\n",
      "1709385799.5239613\n",
      "step 19801, epoch 0 (total steps = 19801): 62.71846950054169\n",
      "1709385799.870598\n",
      "step 19901, epoch 0 (total steps = 19901): 62.639444410800934\n",
      "1709385800.2157211\n",
      "step 20001, epoch 0 (total steps = 20001): 62.70131677389145\n",
      "1709385800.5777504\n",
      "step 20101, epoch 0 (total steps = 20101): 62.660698652267456\n",
      "1709385800.941777\n",
      "step 20201, epoch 0 (total steps = 20201): 62.49011141061783\n",
      "1709385801.3101032\n",
      "step 20301, epoch 0 (total steps = 20301): 62.54460525512695\n",
      "1709385801.6601248\n",
      "step 20401, epoch 0 (total steps = 20401): 62.300131022930145\n",
      "1709385802.0083532\n",
      "step 20501, epoch 0 (total steps = 20501): 62.516560077667236\n",
      "1709385802.3690355\n",
      "step 20601, epoch 0 (total steps = 20601): 62.53828513622284\n",
      "1709385802.7244232\n",
      "step 20701, epoch 0 (total steps = 20701): 62.505915224552155\n",
      "1709385803.073923\n",
      "step 20801, epoch 0 (total steps = 20801): 62.39763939380646\n",
      "1709385803.4271762\n",
      "step 20901, epoch 0 (total steps = 20901): 62.22427487373352\n",
      "1709385803.7687733\n",
      "step 21001, epoch 0 (total steps = 21001): 62.263922333717346\n",
      "1709385804.111753\n",
      "step 21101, epoch 0 (total steps = 21101): 62.4273561835289\n",
      "1709385804.4646926\n",
      "step 21201, epoch 0 (total steps = 21201): 62.27366542816162\n",
      "1709385804.80601\n",
      "step 21301, epoch 0 (total steps = 21301): 62.26186954975128\n",
      "1709385805.152404\n",
      "step 21401, epoch 0 (total steps = 21401): 62.23920673131943\n",
      "1709385805.4942777\n",
      "step 21501, epoch 0 (total steps = 21501): 62.20245695114136\n",
      "1709385805.8367064\n",
      "step 21601, epoch 0 (total steps = 21601): 62.24545007944107\n",
      "1709385806.1872668\n",
      "step 21701, epoch 0 (total steps = 21701): 62.126709043979645\n",
      "1709385806.5345352\n",
      "step 21801, epoch 0 (total steps = 21801): 62.15324139595032\n",
      "1709385806.8776786\n",
      "step 21901, epoch 0 (total steps = 21901): 62.14393800497055\n",
      "1709385807.2122285\n",
      "step 22001, epoch 0 (total steps = 22001): 62.18178915977478\n",
      "1709385807.5726683\n",
      "step 22101, epoch 0 (total steps = 22101): 61.97647440433502\n",
      "1709385807.9184804\n",
      "step 22201, epoch 0 (total steps = 22201): 62.15563076734543\n",
      "1709385808.272255\n",
      "step 22301, epoch 0 (total steps = 22301): 62.20690733194351\n",
      "1709385808.6079772\n",
      "step 22401, epoch 0 (total steps = 22401): 62.048558950424194\n",
      "1709385808.9623702\n",
      "step 22501, epoch 0 (total steps = 22501): 61.9064262509346\n",
      "1709385809.3044717\n",
      "step 22601, epoch 0 (total steps = 22601): 61.96665734052658\n",
      "1709385809.6454403\n",
      "step 22701, epoch 0 (total steps = 22701): 62.10870951414108\n",
      "1709385809.9954524\n",
      "step 22801, epoch 0 (total steps = 22801): 61.99422824382782\n",
      "1709385810.365911\n",
      "step 22901, epoch 0 (total steps = 22901): 61.90316891670227\n",
      "1709385810.742245\n",
      "step 23001, epoch 0 (total steps = 23001): 61.805070638656616\n",
      "1709385811.1040893\n",
      "step 23101, epoch 0 (total steps = 23101): 61.90115886926651\n",
      "1709385811.4605803\n",
      "step 23201, epoch 0 (total steps = 23201): 61.85874515771866\n",
      "1709385811.8316927\n",
      "step 23301, epoch 0 (total steps = 23301): 61.684969544410706\n",
      "1709385812.1814685\n",
      "step 23401, epoch 0 (total steps = 23401): 61.877409517765045\n",
      "1709385812.5163758\n",
      "step 23501, epoch 0 (total steps = 23501): 61.773611307144165\n",
      "1709385812.850622\n",
      "step 23601, epoch 0 (total steps = 23601): 61.81552666425705\n",
      "1709385813.196264\n",
      "step 23701, epoch 0 (total steps = 23701): 61.75577801465988\n",
      "1709385813.5539172\n",
      "step 23801, epoch 0 (total steps = 23801): 61.700089395046234\n",
      "1709385813.8955493\n",
      "step 23901, epoch 0 (total steps = 23901): 61.675841152668\n",
      "1709385814.2475286\n",
      "step 24001, epoch 0 (total steps = 24001): 61.66241002082825\n",
      "1709385814.6026511\n",
      "step 24101, epoch 0 (total steps = 24101): 61.963599026203156\n",
      "1709385814.9396262\n",
      "step 24201, epoch 0 (total steps = 24201): 61.532892525196075\n",
      "1709385815.2847993\n",
      "step 24301, epoch 0 (total steps = 24301): 61.598169445991516\n",
      "1709385815.629541\n",
      "step 24401, epoch 0 (total steps = 24401): 61.56339770555496\n",
      "1709385815.9722226\n",
      "step 24501, epoch 0 (total steps = 24501): 61.56347107887268\n",
      "1709385816.325026\n",
      "step 24601, epoch 0 (total steps = 24601): 61.70596432685852\n",
      "1709385816.6755135\n",
      "step 24701, epoch 0 (total steps = 24701): 61.472057580947876\n",
      "1709385817.0141912\n",
      "step 24801, epoch 0 (total steps = 24801): 61.485824406147\n",
      "1709385817.381457\n",
      "step 24901, epoch 0 (total steps = 24901): 61.35311156511307\n",
      "1709385817.7256265\n",
      "step 25001, epoch 0 (total steps = 25001): 61.52679145336151\n",
      "1709385818.0615168\n",
      "step 25101, epoch 0 (total steps = 25101): 61.530171513557434\n",
      "1709385818.441213\n",
      "step 25201, epoch 0 (total steps = 25201): 61.493164002895355\n",
      "1709385818.7781572\n",
      "step 25301, epoch 0 (total steps = 25301): 61.52944213151932\n",
      "1709385819.1423135\n",
      "step 25401, epoch 0 (total steps = 25401): 61.36271542310715\n",
      "1709385819.4929729\n",
      "step 25501, epoch 0 (total steps = 25501): 61.43894135951996\n",
      "1709385819.8307319\n",
      "step 25601, epoch 0 (total steps = 25601): 61.36545616388321\n",
      "1709385820.170204\n",
      "step 25701, epoch 0 (total steps = 25701): 61.32589799165726\n",
      "1709385820.5290403\n",
      "step 25801, epoch 0 (total steps = 25801): 61.27538979053497\n",
      "1709385820.8668666\n",
      "step 25901, epoch 0 (total steps = 25901): 61.41926693916321\n",
      "1709385821.265104\n",
      "step 26001, epoch 0 (total steps = 26001): 61.45559585094452\n",
      "1709385821.6053352\n",
      "step 26101, epoch 0 (total steps = 26101): 61.32368963956833\n",
      "1709385821.9639108\n",
      "step 26201, epoch 0 (total steps = 26201): 61.34813350439072\n",
      "1709385822.3229527\n",
      "step 26301, epoch 0 (total steps = 26301): 61.38163262605667\n",
      "1709385822.6810608\n",
      "step 26401, epoch 0 (total steps = 26401): 61.28544080257416\n",
      "1709385823.0442874\n",
      "step 26501, epoch 0 (total steps = 26501): 61.363236129283905\n",
      "1709385823.397388\n",
      "step 26601, epoch 0 (total steps = 26601): 61.22898328304291\n",
      "1709385823.743164\n",
      "step 26701, epoch 0 (total steps = 26701): 61.227928042411804\n",
      "1709385824.096424\n",
      "step 26801, epoch 0 (total steps = 26801): 61.29439115524292\n",
      "1709385824.4629495\n",
      "step 26901, epoch 0 (total steps = 26901): 61.2067785859108\n",
      "1709385824.81421\n",
      "step 27001, epoch 0 (total steps = 27001): 60.981218099594116\n",
      "1709385825.1601484\n",
      "step 27101, epoch 0 (total steps = 27101): 61.32651752233505\n",
      "1709385825.504918\n",
      "step 27201, epoch 0 (total steps = 27201): 61.17815446853638\n",
      "1709385825.8514369\n",
      "step 27301, epoch 0 (total steps = 27301): 61.0001779794693\n",
      "1709385826.1933248\n",
      "step 27401, epoch 0 (total steps = 27401): 60.94871425628662\n",
      "1709385826.541247\n",
      "step 27501, epoch 0 (total steps = 27501): 60.99088704586029\n",
      "1709385826.8953927\n",
      "step 27601, epoch 0 (total steps = 27601): 61.10574811697006\n",
      "1709385827.2715085\n",
      "step 27701, epoch 0 (total steps = 27701): 61.07774728536606\n",
      "1709385827.631713\n",
      "step 27801, epoch 0 (total steps = 27801): 61.09164983034134\n",
      "1709385827.993192\n",
      "step 27901, epoch 0 (total steps = 27901): 61.120515286922455\n",
      "1709385828.3567154\n",
      "step 28001, epoch 0 (total steps = 28001): 60.869449198246\n",
      "1709385828.6919446\n",
      "step 28101, epoch 0 (total steps = 28101): 61.0379701256752\n",
      "1709385829.0782146\n",
      "step 28201, epoch 0 (total steps = 28201): 61.008007287979126\n",
      "1709385829.421202\n",
      "step 28301, epoch 0 (total steps = 28301): 60.93633496761322\n",
      "1709385829.7678976\n",
      "step 28401, epoch 0 (total steps = 28401): 60.86222541332245\n",
      "1709385830.1110187\n",
      "step 28501, epoch 0 (total steps = 28501): 60.66402745246887\n",
      "1709385830.4645321\n",
      "step 28601, epoch 0 (total steps = 28601): 60.91597718000412\n",
      "1709385830.8187711\n",
      "step 28701, epoch 0 (total steps = 28701): 60.94174110889435\n",
      "1709385831.1680496\n",
      "step 28801, epoch 0 (total steps = 28801): 60.80918365716934\n",
      "1709385831.5206137\n",
      "step 28901, epoch 0 (total steps = 28901): 60.85072934627533\n",
      "1709385831.8687084\n",
      "step 29001, epoch 0 (total steps = 29001): 60.89350789785385\n",
      "1709385832.2070973\n",
      "step 29101, epoch 0 (total steps = 29101): 60.819202065467834\n",
      "1709385832.55461\n",
      "step 29201, epoch 0 (total steps = 29201): 61.084787130355835\n",
      "1709385832.956335\n",
      "step 29301, epoch 0 (total steps = 29301): 60.74294400215149\n",
      "1709385833.3339374\n",
      "step 29401, epoch 0 (total steps = 29401): 60.845860064029694\n",
      "1709385833.6891003\n",
      "step 29501, epoch 0 (total steps = 29501): 60.702278554439545\n",
      "1709385834.0279007\n",
      "step 29601, epoch 0 (total steps = 29601): 60.87829965353012\n",
      "1709385834.3859832\n",
      "step 29701, epoch 0 (total steps = 29701): 60.724222242832184\n",
      "1709385834.7264562\n",
      "step 29801, epoch 0 (total steps = 29801): 60.84918063879013\n",
      "1709385835.0722687\n",
      "step 29901, epoch 0 (total steps = 29901): 60.75515812635422\n",
      "1709385835.417708\n",
      "step 30001, epoch 0 (total steps = 30001): 60.85949295759201\n",
      "1709385835.7996616\n",
      "step 30101, epoch 0 (total steps = 30101): 60.730099737644196\n",
      "1709385836.1897938\n",
      "step 30201, epoch 0 (total steps = 30201): 60.59163200855255\n",
      "1709385836.5281982\n",
      "step 30301, epoch 0 (total steps = 30301): 60.5805441737175\n",
      "1709385836.8772411\n",
      "step 30401, epoch 0 (total steps = 30401): 60.682424545288086\n",
      "1709385837.2454813\n",
      "step 30501, epoch 0 (total steps = 30501): 61.00347864627838\n",
      "1709385837.6055279\n",
      "step 30601, epoch 0 (total steps = 30601): 60.51994091272354\n",
      "1709385837.9456139\n",
      "step 30701, epoch 0 (total steps = 30701): 60.60848206281662\n",
      "1709385838.2891183\n",
      "step 30801, epoch 0 (total steps = 30801): 60.49494129419327\n",
      "1709385838.632285\n",
      "step 30901, epoch 0 (total steps = 30901): 60.58395320177078\n",
      "1709385838.9819949\n",
      "step 31001, epoch 0 (total steps = 31001): 60.65787410736084\n",
      "1709385839.3855224\n",
      "step 31101, epoch 0 (total steps = 31101): 60.5301154255867\n",
      "1709385839.7238789\n",
      "step 31201, epoch 0 (total steps = 31201): 60.5093132853508\n",
      "1709385840.0628147\n",
      "step 31301, epoch 0 (total steps = 31301): 60.6146439909935\n",
      "1709385840.3990915\n",
      "step 31401, epoch 0 (total steps = 31401): 60.5491828918457\n",
      "1709385840.7351863\n",
      "step 31501, epoch 0 (total steps = 31501): 60.46841871738434\n",
      "1709385841.0811055\n",
      "step 31601, epoch 0 (total steps = 31601): 60.450467348098755\n",
      "1709385841.4171863\n",
      "step 31701, epoch 0 (total steps = 31701): 60.290147721767426\n",
      "1709385841.7940352\n",
      "step 31801, epoch 0 (total steps = 31801): 60.52602845430374\n",
      "1709385842.1405942\n",
      "step 31901, epoch 0 (total steps = 31901): 60.370768904685974\n",
      "1709385842.480309\n",
      "step 32001, epoch 0 (total steps = 32001): 60.39822959899902\n",
      "1709385842.8576481\n",
      "step 32101, epoch 0 (total steps = 32101): 60.58353066444397\n",
      "1709385843.2129798\n",
      "step 32201, epoch 0 (total steps = 32201): 60.454344630241394\n",
      "1709385843.5475328\n",
      "step 32301, epoch 0 (total steps = 32301): 60.32884591817856\n",
      "1709385843.8808527\n",
      "step 32401, epoch 0 (total steps = 32401): 60.275702118873596\n",
      "1709385844.2429578\n",
      "step 32501, epoch 0 (total steps = 32501): 60.2166553735733\n",
      "1709385844.5848103\n",
      "step 32601, epoch 0 (total steps = 32601): 60.28241163492203\n",
      "1709385844.9186985\n",
      "step 32701, epoch 0 (total steps = 32701): 60.24993276596069\n",
      "1709385845.2642605\n",
      "step 32801, epoch 0 (total steps = 32801): 60.44312059879303\n",
      "1709385845.5980237\n",
      "step 32901, epoch 0 (total steps = 32901): 60.40031534433365\n",
      "1709385845.9329057\n",
      "step 33001, epoch 0 (total steps = 33001): 60.08940941095352\n",
      "1709385846.2684426\n",
      "step 33101, epoch 0 (total steps = 33101): 60.07753384113312\n",
      "1709385846.630678\n",
      "step 33201, epoch 0 (total steps = 33201): 60.3461012840271\n",
      "1709385846.9668224\n",
      "step 33301, epoch 0 (total steps = 33301): 60.229882299900055\n",
      "1709385847.3160381\n",
      "step 33401, epoch 0 (total steps = 33401): 60.26457637548447\n",
      "1709385847.6678827\n",
      "step 33501, epoch 0 (total steps = 33501): 60.29413104057312\n",
      "1709385848.0310545\n",
      "step 33601, epoch 0 (total steps = 33601): 60.38251185417175\n",
      "1709385848.3842432\n",
      "step 33701, epoch 0 (total steps = 33701): 60.18466401100159\n",
      "1709385848.7268155\n",
      "step 33801, epoch 0 (total steps = 33801): 60.24880093336105\n",
      "1709385849.0622673\n",
      "step 33901, epoch 0 (total steps = 33901): 60.17774999141693\n",
      "1709385849.4201128\n",
      "step 34001, epoch 0 (total steps = 34001): 60.10343998670578\n",
      "1709385849.7629418\n",
      "step 34101, epoch 0 (total steps = 34101): 60.23710787296295\n",
      "1709385850.11375\n",
      "step 34201, epoch 0 (total steps = 34201): 60.14226996898651\n",
      "1709385850.4486368\n",
      "step 34301, epoch 0 (total steps = 34301): 60.09372794628143\n",
      "1709385850.8296204\n",
      "step 34401, epoch 0 (total steps = 34401): 60.113131523132324\n",
      "1709385851.2090669\n",
      "step 34501, epoch 0 (total steps = 34501): 60.07725667953491\n",
      "1709385851.572249\n",
      "step 34601, epoch 0 (total steps = 34601): 59.76131862401962\n",
      "1709385851.9213812\n",
      "step 34701, epoch 0 (total steps = 34701): 60.16285300254822\n",
      "1709385852.2579489\n",
      "step 34801, epoch 0 (total steps = 34801): 59.9020499587059\n",
      "1709385852.6320045\n",
      "step 34901, epoch 0 (total steps = 34901): 59.97544056177139\n",
      "1709385852.9803238\n",
      "step 35001, epoch 0 (total steps = 35001): 60.159718096256256\n",
      "1709385853.3142698\n",
      "step 35101, epoch 0 (total steps = 35101): 60.13801044225693\n",
      "1709385853.6680725\n",
      "step 35201, epoch 0 (total steps = 35201): 60.22653567790985\n",
      "1709385854.0059123\n",
      "step 35301, epoch 0 (total steps = 35301): 59.91809868812561\n",
      "1709385854.3686614\n",
      "step 35401, epoch 0 (total steps = 35401): 59.80576992034912\n",
      "1709385854.7127454\n",
      "step 35501, epoch 0 (total steps = 35501): 59.988670229911804\n",
      "1709385855.0574002\n",
      "step 35601, epoch 0 (total steps = 35601): 60.08169347047806\n",
      "1709385855.407449\n",
      "step 35701, epoch 0 (total steps = 35701): 59.928139448165894\n",
      "1709385855.7707977\n",
      "step 35801, epoch 0 (total steps = 35801): 59.92631471157074\n",
      "1709385856.1175027\n",
      "step 35901, epoch 0 (total steps = 35901): 59.8560865521431\n",
      "1709385856.455475\n",
      "step 36001, epoch 0 (total steps = 36001): 59.74769765138626\n",
      "1709385856.7958663\n",
      "step 36101, epoch 0 (total steps = 36101): 59.826948165893555\n",
      "1709385857.1345825\n",
      "step 36201, epoch 0 (total steps = 36201): 59.829267144203186\n",
      "1709385857.4805176\n",
      "step 36301, epoch 0 (total steps = 36301): 59.775811553001404\n",
      "1709385857.8189306\n",
      "step 36401, epoch 0 (total steps = 36401): 59.789845407009125\n",
      "1709385858.1938343\n",
      "step 36501, epoch 0 (total steps = 36501): 60.148566007614136\n",
      "1709385858.5310144\n",
      "step 36601, epoch 0 (total steps = 36601): 59.804761826992035\n",
      "1709385858.872937\n",
      "step 36701, epoch 0 (total steps = 36701): 59.834972977638245\n",
      "1709385859.2073207\n",
      "step 36801, epoch 0 (total steps = 36801): 59.55297535657883\n",
      "1709385859.5759504\n",
      "step 36901, epoch 0 (total steps = 36901): 59.783876836299896\n",
      "1709385859.9105062\n",
      "step 37001, epoch 0 (total steps = 37001): 59.69033062458038\n",
      "1709385860.2510571\n",
      "step 37101, epoch 0 (total steps = 37101): 59.68165409564972\n",
      "1709385860.598013\n",
      "step 37201, epoch 0 (total steps = 37201): 59.731763541698456\n",
      "1709385860.9320416\n",
      "step 37301, epoch 0 (total steps = 37301): 59.500657081604004\n",
      "1709385861.278268\n",
      "step 37401, epoch 0 (total steps = 37401): 59.81244772672653\n",
      "1709385861.619145\n",
      "step 37501, epoch 0 (total steps = 37501): 59.77228683233261\n",
      "1709385861.9725296\n",
      "step 37601, epoch 0 (total steps = 37601): 59.65932506322861\n",
      "1709385862.3173316\n",
      "step 37701, epoch 0 (total steps = 37701): 59.71397793292999\n",
      "1709385862.6546655\n",
      "step 37801, epoch 0 (total steps = 37801): 59.79387855529785\n",
      "1709385863.027266\n",
      "step 37901, epoch 0 (total steps = 37901): 59.63819032907486\n",
      "1709385863.3624175\n",
      "step 38001, epoch 0 (total steps = 38001): 59.69267666339874\n",
      "1709385863.7127788\n",
      "step 38101, epoch 0 (total steps = 38101): 59.64667749404907\n",
      "1709385864.0521352\n",
      "step 38201, epoch 0 (total steps = 38201): 59.77275985479355\n",
      "1709385864.3873858\n",
      "step 38301, epoch 0 (total steps = 38301): 59.45144426822662\n",
      "1709385864.7425332\n",
      "step 38401, epoch 0 (total steps = 38401): 59.73715525865555\n",
      "1709385865.0863204\n",
      "step 38501, epoch 0 (total steps = 38501): 59.68553978204727\n",
      "1709385865.4365306\n",
      "step 38601, epoch 0 (total steps = 38601): 59.61500358581543\n",
      "1709385865.7896147\n",
      "step 38701, epoch 0 (total steps = 38701): 59.68889492750168\n",
      "1709385866.1569605\n",
      "step 38801, epoch 0 (total steps = 38801): 59.60859775543213\n",
      "1709385866.4978812\n",
      "step 38901, epoch 0 (total steps = 38901): 59.66206043958664\n",
      "1709385866.8252993\n",
      "step 39001, epoch 0 (total steps = 39001): 59.49407559633255\n",
      "1709385867.1595638\n",
      "step 39101, epoch 0 (total steps = 39101): 59.511616826057434\n",
      "1709385867.4924495\n",
      "step 39201, epoch 0 (total steps = 39201): 59.502569913864136\n",
      "1709385867.8219929\n",
      "step 39301, epoch 0 (total steps = 39301): 59.513597548007965\n",
      "1709385868.168083\n",
      "step 39401, epoch 0 (total steps = 39401): 59.307990193367004\n",
      "1709385868.523377\n",
      "step 39501, epoch 0 (total steps = 39501): 59.513436913490295\n",
      "1709385868.8476107\n",
      "step 39601, epoch 0 (total steps = 39601): 59.384002804756165\n",
      "1709385869.1764016\n",
      "step 39701, epoch 0 (total steps = 39701): 59.50365322828293\n",
      "1709385869.5248032\n",
      "step 39801, epoch 0 (total steps = 39801): 59.42513132095337\n",
      "1709385869.879973\n",
      "step 39901, epoch 0 (total steps = 39901): 59.57039302587509\n",
      "1709385870.2112894\n",
      "step 40001, epoch 0 (total steps = 40001): 59.335766077041626\n",
      "1709385870.5430048\n",
      "step 40101, epoch 0 (total steps = 40101): 59.422687292099\n",
      "1709385870.8740134\n",
      "step 40201, epoch 0 (total steps = 40201): 59.31465673446655\n",
      "1709385871.2068126\n",
      "step 40301, epoch 0 (total steps = 40301): 59.473149716854095\n",
      "1709385871.5537443\n",
      "step 40401, epoch 0 (total steps = 40401): 59.368259251117706\n",
      "1709385871.8899891\n",
      "step 40501, epoch 0 (total steps = 40501): 59.3913009762764\n",
      "1709385872.2260826\n",
      "step 40601, epoch 0 (total steps = 40601): 59.3211886882782\n",
      "1709385872.5570512\n",
      "step 40701, epoch 0 (total steps = 40701): 59.420594573020935\n",
      "1709385872.890818\n",
      "step 40801, epoch 0 (total steps = 40801): 59.31702411174774\n",
      "1709385873.25855\n",
      "step 40901, epoch 0 (total steps = 40901): 59.29702574014664\n",
      "1709385873.6198058\n",
      "step 41001, epoch 0 (total steps = 41001): 59.438011944293976\n",
      "1709385873.9635859\n",
      "step 41101, epoch 0 (total steps = 41101): 59.406742215156555\n",
      "1709385874.3003237\n",
      "step 41201, epoch 0 (total steps = 41201): 59.31603008508682\n",
      "1709385874.6449835\n",
      "step 41301, epoch 0 (total steps = 41301): 59.44738459587097\n",
      "1709385874.9962895\n",
      "step 41401, epoch 0 (total steps = 41401): 59.34156692028046\n",
      "1709385875.330177\n",
      "step 41501, epoch 0 (total steps = 41501): 59.28094929456711\n",
      "1709385875.6653206\n",
      "step 41601, epoch 0 (total steps = 41601): 59.23456609249115\n",
      "1709385876.00463\n",
      "step 41701, epoch 0 (total steps = 41701): 59.15401637554169\n",
      "1709385876.3590052\n",
      "step 41801, epoch 0 (total steps = 41801): 59.359267830848694\n",
      "1709385876.7081292\n",
      "step 41901, epoch 0 (total steps = 41901): 59.166068613529205\n",
      "1709385877.0697439\n",
      "step 42001, epoch 0 (total steps = 42001): 59.23211044073105\n",
      "1709385877.4130092\n",
      "step 42101, epoch 0 (total steps = 42101): 59.20320016145706\n",
      "1709385877.7520792\n",
      "step 42201, epoch 0 (total steps = 42201): 59.214925944805145\n",
      "1709385878.09035\n",
      "step 42301, epoch 0 (total steps = 42301): 59.13026142120361\n",
      "1709385878.4389205\n",
      "step 42401, epoch 0 (total steps = 42401): 59.466731905937195\n",
      "1709385878.8067138\n",
      "step 42501, epoch 0 (total steps = 42501): 59.27032148838043\n",
      "1709385879.1747055\n",
      "step 42601, epoch 0 (total steps = 42601): 59.36945766210556\n",
      "1709385879.5121598\n",
      "step 42701, epoch 0 (total steps = 42701): 59.257281959056854\n",
      "1709385879.8536534\n",
      "step 42801, epoch 0 (total steps = 42801): 59.1156587600708\n",
      "1709385880.2205427\n",
      "step 42901, epoch 0 (total steps = 42901): 59.07134681940079\n",
      "1709385880.5661774\n",
      "step 43001, epoch 0 (total steps = 43001): 59.023110151290894\n",
      "1709385880.922538\n",
      "step 43101, epoch 0 (total steps = 43101): 58.98856920003891\n",
      "1709385881.276227\n",
      "step 43201, epoch 0 (total steps = 43201): 59.26282715797424\n",
      "1709385881.6564233\n",
      "step 43301, epoch 0 (total steps = 43301): 59.175340354442596\n",
      "1709385882.0333648\n",
      "step 43401, epoch 0 (total steps = 43401): 59.084083914756775\n",
      "1709385882.3871257\n",
      "step 43501, epoch 0 (total steps = 43501): 59.13560664653778\n",
      "1709385882.72629\n",
      "step 43601, epoch 0 (total steps = 43601): 59.02112030982971\n",
      "1709385883.0614896\n",
      "step 43701, epoch 0 (total steps = 43701): 59.05370020866394\n",
      "1709385883.4032445\n",
      "step 43801, epoch 0 (total steps = 43801): 59.03046375513077\n",
      "1709385883.7428272\n",
      "step 43901, epoch 0 (total steps = 43901): 58.889441668987274\n",
      "1709385884.081635\n",
      "step 44001, epoch 0 (total steps = 44001): 59.009880006313324\n",
      "1709385884.4254005\n",
      "step 44101, epoch 0 (total steps = 44101): 58.930221140384674\n",
      "1709385884.758351\n",
      "step 44201, epoch 0 (total steps = 44201): 58.997696936130524\n",
      "1709385885.1010032\n",
      "step 44301, epoch 0 (total steps = 44301): 59.19074410200119\n",
      "1709385885.44423\n",
      "step 44401, epoch 0 (total steps = 44401): 59.074775874614716\n",
      "1709385885.7854872\n",
      "step 44501, epoch 0 (total steps = 44501): 59.08457726240158\n",
      "1709385886.1288788\n",
      "step 44601, epoch 0 (total steps = 44601): 59.087609112262726\n",
      "1709385886.4752028\n",
      "step 44701, epoch 0 (total steps = 44701): 58.976934254169464\n",
      "1709385886.8236587\n",
      "step 44801, epoch 0 (total steps = 44801): 59.01653027534485\n",
      "1709385887.160657\n",
      "step 44901, epoch 0 (total steps = 44901): 59.16397786140442\n",
      "1709385887.5019348\n",
      "step 45001, epoch 0 (total steps = 45001): 59.17079967260361\n",
      "1709385887.8402245\n",
      "step 45101, epoch 0 (total steps = 45101): 59.10631310939789\n",
      "1709385888.1785545\n",
      "step 45201, epoch 0 (total steps = 45201): 58.92460983991623\n",
      "1709385888.5443792\n",
      "step 45301, epoch 0 (total steps = 45301): 58.65298908948898\n",
      "1709385888.8852398\n",
      "step 45401, epoch 0 (total steps = 45401): 58.82543474435806\n",
      "1709385889.2295036\n",
      "step 45501, epoch 0 (total steps = 45501): 58.98351925611496\n",
      "1709385889.574449\n",
      "step 45601, epoch 0 (total steps = 45601): 58.805934727191925\n",
      "1709385889.9286935\n",
      "step 45701, epoch 0 (total steps = 45701): 58.78511619567871\n",
      "1709385890.2868407\n",
      "step 45801, epoch 0 (total steps = 45801): 58.78056615591049\n",
      "1709385890.6378355\n",
      "step 45901, epoch 0 (total steps = 45901): 58.98026657104492\n",
      "1709385891.0284483\n",
      "step 46001, epoch 0 (total steps = 46001): 58.769681453704834\n",
      "1709385891.3815415\n",
      "step 46101, epoch 0 (total steps = 46101): 58.89156049489975\n",
      "1709385891.7301588\n",
      "step 46201, epoch 0 (total steps = 46201): 58.8434419631958\n",
      "1709385892.0677207\n",
      "step 46301, epoch 0 (total steps = 46301): 58.74479699134827\n",
      "1709385892.418051\n",
      "step 46401, epoch 0 (total steps = 46401): 59.05996471643448\n",
      "1709385892.7562659\n",
      "step 46501, epoch 0 (total steps = 46501): 58.556762874126434\n",
      "1709385893.1016145\n",
      "step 46601, epoch 0 (total steps = 46601): 58.703890919685364\n",
      "1709385893.4427025\n",
      "step 46701, epoch 0 (total steps = 46701): 59.056262850761414\n",
      "1709385893.7773485\n",
      "step 46801, epoch 0 (total steps = 46801): 58.83622807264328\n",
      "1709385894.1129384\n",
      "step 46901, epoch 0 (total steps = 46901): 58.89341551065445\n",
      "1709385894.4614801\n",
      "step 47001, epoch 0 (total steps = 47001): 58.632390558719635\n",
      "1709385894.8117092\n",
      "step 47101, epoch 0 (total steps = 47101): 58.68796855211258\n",
      "1709385895.160431\n",
      "step 47201, epoch 0 (total steps = 47201): 58.60575497150421\n",
      "1709385895.5297425\n",
      "step 47301, epoch 0 (total steps = 47301): 58.84522503614426\n",
      "1709385895.883716\n",
      "step 47401, epoch 0 (total steps = 47401): 58.997171342372894\n",
      "1709385896.2194524\n",
      "step 47501, epoch 0 (total steps = 47501): 58.758378982543945\n",
      "1709385896.5565038\n",
      "step 47601, epoch 0 (total steps = 47601): 58.70978671312332\n",
      "1709385896.8990629\n",
      "step 47701, epoch 0 (total steps = 47701): 58.71555304527283\n",
      "1709385897.2345872\n",
      "step 47801, epoch 0 (total steps = 47801): 58.83694165945053\n",
      "1709385897.5723531\n",
      "step 47901, epoch 0 (total steps = 47901): 58.93393540382385\n",
      "1709385897.9431288\n",
      "step 48001, epoch 0 (total steps = 48001): 58.52333474159241\n",
      "1709385898.2780209\n",
      "step 48101, epoch 0 (total steps = 48101): 58.70192140340805\n",
      "1709385898.6201296\n",
      "step 48201, epoch 0 (total steps = 48201): 58.906113624572754\n",
      "1709385898.955192\n",
      "step 48301, epoch 0 (total steps = 48301): 58.85225069522858\n",
      "1709385899.3244631\n",
      "step 48401, epoch 0 (total steps = 48401): 58.884567737579346\n",
      "1709385899.668046\n",
      "step 48501, epoch 0 (total steps = 48501): 58.72299522161484\n",
      "1709385900.0112827\n",
      "step 48601, epoch 0 (total steps = 48601): 58.65741729736328\n",
      "1709385900.3626707\n",
      "step 48701, epoch 0 (total steps = 48701): 58.523445785045624\n",
      "1709385900.7070134\n",
      "step 48801, epoch 0 (total steps = 48801): 58.70434761047363\n",
      "1709385901.0439742\n",
      "step 48901, epoch 0 (total steps = 48901): 58.4185346364975\n",
      "1709385901.3793676\n",
      "step 49001, epoch 0 (total steps = 49001): 58.50457841157913\n",
      "1709385901.7247972\n",
      "step 49101, epoch 0 (total steps = 49101): 58.61400079727173\n",
      "1709385902.0687103\n",
      "step 49201, epoch 0 (total steps = 49201): 58.58744841814041\n",
      "1709385902.4558766\n",
      "step 49301, epoch 0 (total steps = 49301): 58.40972900390625\n",
      "1709385902.8004646\n",
      "step 49401, epoch 0 (total steps = 49401): 58.73496747016907\n",
      "1709385903.1448088\n",
      "step 49501, epoch 0 (total steps = 49501): 58.50319164991379\n",
      "1709385903.4979885\n",
      "step 49601, epoch 0 (total steps = 49601): 58.67079174518585\n",
      "1709385903.8458867\n",
      "step 49701, epoch 0 (total steps = 49701): 58.39097559452057\n",
      "1709385904.1997752\n",
      "step 49801, epoch 0 (total steps = 49801): 58.47154051065445\n",
      "1709385904.5504835\n",
      "step 49901, epoch 0 (total steps = 49901): 58.483606696128845\n",
      "1709385904.8812792\n",
      "step 50001, epoch 0 (total steps = 50001): 58.39263969659805\n",
      "1709385905.2241633\n",
      "step 50101, epoch 0 (total steps = 50101): 58.65800315141678\n",
      "1709385905.5793595\n",
      "step 50201, epoch 0 (total steps = 50201): 58.4410497546196\n",
      "1709385905.917553\n",
      "step 50301, epoch 0 (total steps = 50301): 58.87970393896103\n",
      "1709385906.2752497\n",
      "step 50401, epoch 0 (total steps = 50401): 58.48730593919754\n",
      "1709385906.6200318\n",
      "step 50501, epoch 0 (total steps = 50501): 58.37213611602783\n",
      "1709385906.969722\n",
      "step 50601, epoch 0 (total steps = 50601): 58.36946111917496\n",
      "1709385907.314693\n",
      "step 50701, epoch 0 (total steps = 50701): 58.40590554475784\n",
      "1709385907.6589334\n",
      "step 50801, epoch 0 (total steps = 50801): 58.46017915010452\n",
      "1709385908.005037\n",
      "step 50901, epoch 0 (total steps = 50901): 58.698249101638794\n",
      "1709385908.3784761\n",
      "step 51001, epoch 0 (total steps = 51001): 58.45103043317795\n",
      "1709385908.7301958\n",
      "step 51101, epoch 0 (total steps = 51101): 58.51887768507004\n",
      "1709385909.0777743\n",
      "step 51201, epoch 0 (total steps = 51201): 58.58504617214203\n",
      "1709385909.4317212\n",
      "step 51301, epoch 0 (total steps = 51301): 58.45149886608124\n",
      "1709385909.7996397\n",
      "step 51401, epoch 0 (total steps = 51401): 58.54488205909729\n",
      "1709385910.1451166\n",
      "step 51501, epoch 0 (total steps = 51501): 58.502437233924866\n",
      "1709385910.4836833\n",
      "step 51601, epoch 0 (total steps = 51601): 58.58344930410385\n",
      "1709385910.8509657\n",
      "step 51701, epoch 0 (total steps = 51701): 58.51118189096451\n",
      "1709385911.1936176\n",
      "step 51801, epoch 0 (total steps = 51801): 58.50628101825714\n",
      "1709385911.532108\n",
      "step 51901, epoch 0 (total steps = 51901): 58.43711405992508\n",
      "1709385911.8693602\n",
      "step 52001, epoch 0 (total steps = 52001): 58.48255634307861\n",
      "1709385912.2225282\n",
      "step 52101, epoch 0 (total steps = 52101): 58.513747811317444\n",
      "1709385912.5835207\n",
      "step 52201, epoch 0 (total steps = 52201): 58.29755645990372\n",
      "1709385912.9271872\n",
      "step 52301, epoch 0 (total steps = 52301): 58.41709917783737\n",
      "1709385913.2666512\n",
      "step 52401, epoch 0 (total steps = 52401): 58.47578775882721\n",
      "1709385913.6081958\n",
      "step 52501, epoch 0 (total steps = 52501): 58.662000834941864\n",
      "1709385913.9533348\n",
      "step 52601, epoch 0 (total steps = 52601): 58.55726832151413\n",
      "1709385914.287552\n",
      "step 52701, epoch 0 (total steps = 52701): 58.44760471582413\n",
      "1709385914.6401405\n",
      "step 52801, epoch 0 (total steps = 52801): 58.34908252954483\n",
      "1709385914.979095\n",
      "step 52901, epoch 0 (total steps = 52901): 58.32215863466263\n",
      "1709385915.3138003\n",
      "step 53001, epoch 0 (total steps = 53001): 58.42302083969116\n",
      "1709385915.64998\n",
      "step 53101, epoch 0 (total steps = 53101): 58.362788021564484\n",
      "1709385915.988741\n",
      "step 53201, epoch 0 (total steps = 53201): 58.41102868318558\n",
      "1709385916.34013\n",
      "step 53301, epoch 0 (total steps = 53301): 58.42907601594925\n",
      "1709385916.6866548\n",
      "step 53401, epoch 0 (total steps = 53401): 58.69422775506973\n",
      "1709385917.0326533\n",
      "step 53501, epoch 0 (total steps = 53501): 58.31960666179657\n",
      "1709385917.3814578\n",
      "step 53601, epoch 0 (total steps = 53601): 58.24686485528946\n",
      "1709385917.7283113\n",
      "step 53701, epoch 0 (total steps = 53701): 58.35883754491806\n",
      "1709385918.064634\n",
      "step 53801, epoch 0 (total steps = 53801): 58.24367964267731\n",
      "1709385918.4007616\n",
      "step 53901, epoch 0 (total steps = 53901): 58.28295975923538\n",
      "1709385918.7387738\n",
      "step 54001, epoch 0 (total steps = 54001): 58.239376068115234\n",
      "1709385919.0773606\n",
      "step 54101, epoch 0 (total steps = 54101): 58.32639396190643\n",
      "1709385919.4180274\n",
      "step 54201, epoch 0 (total steps = 54201): 58.09129065275192\n",
      "1709385919.7612891\n",
      "step 54301, epoch 0 (total steps = 54301): 58.30930840969086\n",
      "1709385920.1386282\n",
      "step 54401, epoch 0 (total steps = 54401): 58.18764978647232\n",
      "1709385920.4744775\n",
      "step 54501, epoch 0 (total steps = 54501): 58.173832654953\n",
      "1709385920.8548775\n",
      "step 54601, epoch 0 (total steps = 54601): 58.07776099443436\n",
      "1709385921.1886172\n",
      "step 54701, epoch 0 (total steps = 54701): 58.22134870290756\n",
      "1709385921.5421977\n",
      "step 54801, epoch 0 (total steps = 54801): 58.33303606510162\n",
      "1709385921.8763423\n",
      "step 54901, epoch 0 (total steps = 54901): 58.32856070995331\n",
      "1709385922.2100108\n",
      "step 55001, epoch 0 (total steps = 55001): 58.359606862068176\n",
      "1709385922.5447757\n",
      "step 55101, epoch 0 (total steps = 55101): 58.258823215961456\n",
      "1709385922.879129\n",
      "step 55201, epoch 0 (total steps = 55201): 58.209885001182556\n",
      "1709385923.2385788\n",
      "step 55301, epoch 0 (total steps = 55301): 58.1666179895401\n",
      "1709385923.5853927\n",
      "step 55401, epoch 0 (total steps = 55401): 58.332919001579285\n",
      "1709385923.9206085\n",
      "step 55501, epoch 0 (total steps = 55501): 58.219104528427124\n",
      "1709385924.2550886\n",
      "step 55601, epoch 0 (total steps = 55601): 58.17402493953705\n",
      "1709385924.616452\n",
      "step 55701, epoch 0 (total steps = 55701): 58.07544529438019\n",
      "1709385924.9863796\n",
      "step 55801, epoch 0 (total steps = 55801): 58.32138007879257\n",
      "1709385925.343158\n",
      "step 55901, epoch 0 (total steps = 55901): 58.05991530418396\n",
      "1709385925.6892414\n",
      "step 56001, epoch 0 (total steps = 56001): 58.06039077043533\n",
      "1709385926.0637717\n",
      "step 56101, epoch 0 (total steps = 56101): 58.200658082962036\n",
      "1709385926.4237275\n",
      "step 56201, epoch 0 (total steps = 56201): 58.23824441432953\n",
      "1709385926.763506\n",
      "step 56301, epoch 0 (total steps = 56301): 58.11192548274994\n",
      "1709385927.1200893\n",
      "step 56401, epoch 0 (total steps = 56401): 58.09522068500519\n",
      "1709385927.45531\n",
      "step 56501, epoch 0 (total steps = 56501): 58.096606612205505\n",
      "1709385927.7923238\n",
      "step 56601, epoch 0 (total steps = 56601): 58.123048424720764\n",
      "1709385928.138456\n",
      "step 56701, epoch 0 (total steps = 56701): 58.195893824100494\n",
      "1709385928.475995\n",
      "step 56801, epoch 0 (total steps = 56801): 58.05415439605713\n",
      "1709385928.8209186\n",
      "step 56901, epoch 0 (total steps = 56901): 58.039221465587616\n",
      "1709385929.1599271\n",
      "step 57001, epoch 0 (total steps = 57001): 57.95530277490616\n",
      "1709385929.5113542\n",
      "step 57101, epoch 0 (total steps = 57101): 58.04608500003815\n",
      "1709385929.864926\n",
      "step 57201, epoch 0 (total steps = 57201): 58.070312559604645\n",
      "1709385930.205701\n",
      "step 57301, epoch 0 (total steps = 57301): 58.074490904808044\n",
      "1709385930.5678473\n",
      "step 57401, epoch 0 (total steps = 57401): 57.99541634321213\n",
      "1709385930.9094622\n",
      "step 57501, epoch 0 (total steps = 57501): 57.87388849258423\n",
      "1709385931.2507632\n",
      "step 57601, epoch 0 (total steps = 57601): 58.146517276763916\n",
      "1709385931.5909872\n",
      "step 57701, epoch 0 (total steps = 57701): 57.95540517568588\n",
      "1709385931.9371202\n",
      "step 57801, epoch 0 (total steps = 57801): 58.23624712228775\n",
      "1709385932.2801747\n",
      "step 57901, epoch 0 (total steps = 57901): 57.992412745952606\n",
      "1709385932.622431\n",
      "step 58001, epoch 0 (total steps = 58001): 57.95715767145157\n",
      "1709385932.9719095\n",
      "step 58101, epoch 0 (total steps = 58101): 58.07671755552292\n",
      "1709385933.3144734\n",
      "step 58201, epoch 0 (total steps = 58201): 57.95912182331085\n",
      "1709385933.6615775\n",
      "step 58301, epoch 0 (total steps = 58301): 57.872148394584656\n",
      "1709385934.0033872\n",
      "step 58401, epoch 0 (total steps = 58401): 57.816699862480164\n",
      "1709385934.3588808\n",
      "step 58501, epoch 0 (total steps = 58501): 57.99325215816498\n",
      "1709385934.7063713\n",
      "step 58601, epoch 0 (total steps = 58601): 58.09184378385544\n",
      "1709385935.051772\n",
      "step 58701, epoch 0 (total steps = 58701): 57.91013431549072\n",
      "1709385935.4234378\n",
      "step 58801, epoch 0 (total steps = 58801): 57.9617663025856\n",
      "1709385935.781852\n",
      "step 58901, epoch 0 (total steps = 58901): 57.7919305562973\n",
      "1709385936.13336\n",
      "step 59001, epoch 0 (total steps = 59001): 57.937903225421906\n",
      "1709385936.4732125\n",
      "step 59101, epoch 0 (total steps = 59101): 57.87565898895264\n",
      "1709385936.8141665\n",
      "step 59201, epoch 0 (total steps = 59201): 57.97097986936569\n",
      "1709385937.199843\n",
      "step 59301, epoch 0 (total steps = 59301): 57.915086805820465\n",
      "1709385937.5529802\n",
      "step 59401, epoch 0 (total steps = 59401): 57.92765647172928\n",
      "1709385937.9151375\n",
      "step 59501, epoch 0 (total steps = 59501): 58.044437885284424\n",
      "1709385938.261177\n",
      "step 59601, epoch 0 (total steps = 59601): 57.69398605823517\n",
      "1709385938.6093614\n",
      "step 59701, epoch 0 (total steps = 59701): 57.73681801557541\n",
      "1709385938.957893\n",
      "step 59801, epoch 0 (total steps = 59801): 57.76957046985626\n",
      "1709385939.346858\n",
      "step 59901, epoch 0 (total steps = 59901): 57.971508502960205\n",
      "1709385939.68288\n",
      "step 60001, epoch 0 (total steps = 60001): 57.85522574186325\n",
      "1709385940.0275643\n",
      "step 60101, epoch 0 (total steps = 60101): 57.902451276779175\n",
      "1709385940.3664079\n",
      "step 60201, epoch 0 (total steps = 60201): 57.757284224033356\n",
      "1709385940.7060938\n",
      "step 60301, epoch 0 (total steps = 60301): 57.99775296449661\n",
      "1709385941.0682821\n",
      "step 60401, epoch 0 (total steps = 60401): 57.873386681079865\n",
      "1709385941.4062805\n",
      "step 60501, epoch 0 (total steps = 60501): 58.065331757068634\n",
      "1709385941.7470956\n",
      "step 60601, epoch 0 (total steps = 60601): 57.900907814502716\n",
      "1709385942.0863137\n",
      "step 60701, epoch 0 (total steps = 60701): 58.01662105321884\n",
      "1709385942.4298184\n",
      "step 60801, epoch 0 (total steps = 60801): 57.65419393777847\n",
      "1709385942.7784388\n",
      "step 60901, epoch 0 (total steps = 60901): 57.85413086414337\n",
      "1709385943.12682\n",
      "step 61001, epoch 0 (total steps = 61001): 57.81467121839523\n",
      "1709385943.470536\n",
      "step 61101, epoch 0 (total steps = 61101): 58.06896984577179\n",
      "1709385943.8239295\n",
      "step 61201, epoch 0 (total steps = 61201): 57.76146811246872\n",
      "1709385944.181107\n",
      "step 61301, epoch 0 (total steps = 61301): 57.51774305105209\n",
      "1709385944.5163083\n",
      "step 61401, epoch 0 (total steps = 61401): 57.66411107778549\n",
      "1709385944.864635\n",
      "step 61501, epoch 0 (total steps = 61501): 57.84918564558029\n",
      "1709385945.2275057\n",
      "step 61601, epoch 0 (total steps = 61601): 57.753914296627045\n",
      "1709385945.5677269\n",
      "step 61701, epoch 0 (total steps = 61701): 57.822837471961975\n",
      "1709385945.9101257\n",
      "step 61801, epoch 0 (total steps = 61801): 57.7431475520134\n",
      "1709385946.2479105\n",
      "step 61901, epoch 0 (total steps = 61901): 57.76833254098892\n",
      "1709385946.5937579\n",
      "step 62001, epoch 0 (total steps = 62001): 57.92607891559601\n",
      "1709385946.9388933\n",
      "step 62101, epoch 0 (total steps = 62101): 57.706410109996796\n",
      "1709385947.2911165\n",
      "step 62201, epoch 0 (total steps = 62201): 57.961757481098175\n",
      "1709385947.6576889\n",
      "step 62301, epoch 0 (total steps = 62301): 57.8767728805542\n",
      "1709385948.001187\n",
      "step 62401, epoch 0 (total steps = 62401): 57.77016168832779\n",
      "1709385948.346454\n",
      "step 62501, epoch 0 (total steps = 62501): 57.91049641370773\n",
      "1709385948.684351\n",
      "step 62601, epoch 0 (total steps = 62601): 57.96475422382355\n",
      "1709385949.022515\n",
      "step 62701, epoch 0 (total steps = 62701): 57.75671064853668\n",
      "1709385949.3894606\n",
      "step 62801, epoch 0 (total steps = 62801): 57.641946613788605\n",
      "1709385949.7932286\n",
      "step 62901, epoch 0 (total steps = 62901): 57.971823930740356\n",
      "1709385950.137089\n",
      "step 63001, epoch 0 (total steps = 63001): 57.75983798503876\n",
      "1709385950.4771845\n",
      "step 63101, epoch 0 (total steps = 63101): 57.64294147491455\n",
      "1709385950.8332\n",
      "step 63201, epoch 0 (total steps = 63201): 57.83362430334091\n",
      "1709385951.1818252\n",
      "step 63301, epoch 0 (total steps = 63301): 57.64643043279648\n",
      "1709385951.520893\n",
      "step 63401, epoch 0 (total steps = 63401): 57.59775495529175\n",
      "1709385951.8749225\n",
      "step 63501, epoch 0 (total steps = 63501): 57.804792046546936\n",
      "1709385952.2466207\n",
      "step 63601, epoch 0 (total steps = 63601): 57.734991908073425\n",
      "1709385952.6111188\n",
      "step 63701, epoch 0 (total steps = 63701): 57.73676925897598\n",
      "1709385952.962771\n",
      "step 63801, epoch 0 (total steps = 63801): 57.678617000579834\n",
      "1709385953.3093274\n",
      "step 63901, epoch 0 (total steps = 63901): 57.74580842256546\n",
      "1709385953.65472\n",
      "step 64001, epoch 0 (total steps = 64001): 57.85416257381439\n",
      "1709385953.9987445\n",
      "step 64101, epoch 0 (total steps = 64101): 57.88866466283798\n",
      "1709385954.3300235\n",
      "step 64201, epoch 0 (total steps = 64201): 57.59761297702789\n",
      "1709385954.6851404\n",
      "step 64301, epoch 0 (total steps = 64301): 57.74719703197479\n",
      "1709385955.029394\n",
      "step 64401, epoch 0 (total steps = 64401): 57.624111354351044\n",
      "1709385955.3699465\n",
      "step 64501, epoch 0 (total steps = 64501): 57.568948686122894\n",
      "1709385955.7102327\n",
      "step 64601, epoch 0 (total steps = 64601): 57.86106222867966\n",
      "1709385956.0561745\n",
      "step 64701, epoch 0 (total steps = 64701): 57.67545264959335\n",
      "1709385956.4085839\n",
      "step 64801, epoch 0 (total steps = 64801): 57.67440074682236\n",
      "1709385956.7593977\n",
      "step 64901, epoch 0 (total steps = 64901): 57.49554789066315\n",
      "1709385957.0989487\n",
      "step 65001, epoch 0 (total steps = 65001): 57.5560068488121\n",
      "1709385957.438932\n",
      "step 65101, epoch 0 (total steps = 65101): 57.73909258842468\n",
      "1709385957.7833476\n",
      "step 65201, epoch 0 (total steps = 65201): 57.77189439535141\n",
      "1709385958.13128\n",
      "step 65301, epoch 0 (total steps = 65301): 57.5955371260643\n",
      "1709385958.4711473\n",
      "step 65401, epoch 0 (total steps = 65401): 57.570574939250946\n",
      "1709385958.8177946\n",
      "step 65501, epoch 0 (total steps = 65501): 57.525356471538544\n",
      "1709385959.1825264\n",
      "step 65601, epoch 0 (total steps = 65601): 57.35258847475052\n",
      "1709385959.5190687\n",
      "step 65701, epoch 0 (total steps = 65701): 57.77207559347153\n",
      "1709385959.8637712\n",
      "step 65801, epoch 0 (total steps = 65801): 57.654560685157776\n",
      "1709385960.2232058\n",
      "step 65901, epoch 0 (total steps = 65901): 57.587334632873535\n",
      "1709385960.5954015\n",
      "step 66001, epoch 0 (total steps = 66001): 57.61885869503021\n",
      "1709385960.9333212\n",
      "step 66101, epoch 0 (total steps = 66101): 57.90462452173233\n",
      "1709385961.2888515\n",
      "step 66201, epoch 0 (total steps = 66201): 57.612285912036896\n",
      "1709385961.6379805\n",
      "step 66301, epoch 0 (total steps = 66301): 57.49937832355499\n",
      "1709385961.986218\n",
      "step 66401, epoch 0 (total steps = 66401): 57.5347443819046\n",
      "1709385962.35465\n",
      "step 66501, epoch 0 (total steps = 66501): 57.55616444349289\n",
      "1709385962.7195358\n",
      "step 66601, epoch 0 (total steps = 66601): 57.589658319950104\n",
      "1709385963.0968602\n",
      "step 66701, epoch 0 (total steps = 66701): 57.43998336791992\n",
      "1709385963.4372504\n",
      "step 66801, epoch 0 (total steps = 66801): 57.46071147918701\n",
      "1709385963.793909\n",
      "step 66901, epoch 0 (total steps = 66901): 57.485350251197815\n",
      "1709385964.1470509\n",
      "step 67001, epoch 0 (total steps = 67001): 57.563646137714386\n",
      "1709385964.4984806\n",
      "step 67101, epoch 0 (total steps = 67101): 57.410039126873016\n",
      "1709385964.8433635\n",
      "step 67201, epoch 0 (total steps = 67201): 57.68707424402237\n",
      "1709385965.196992\n",
      "step 67301, epoch 0 (total steps = 67301): 57.3526925444603\n",
      "1709385965.5403185\n",
      "step 67401, epoch 0 (total steps = 67401): 57.72955930233002\n",
      "1709385965.8792586\n",
      "step 67501, epoch 0 (total steps = 67501): 57.440290689468384\n",
      "1709385966.215372\n",
      "step 67601, epoch 0 (total steps = 67601): 57.5175786614418\n",
      "1709385966.5613537\n",
      "step 67701, epoch 0 (total steps = 67701): 57.484520971775055\n",
      "1709385966.9178526\n",
      "step 67801, epoch 0 (total steps = 67801): 57.395742774009705\n",
      "1709385967.2624297\n",
      "step 67901, epoch 0 (total steps = 67901): 57.45947217941284\n",
      "1709385967.6097357\n",
      "step 68001, epoch 0 (total steps = 68001): 57.541421353816986\n",
      "1709385967.948169\n",
      "step 68101, epoch 0 (total steps = 68101): 57.381126165390015\n",
      "1709385968.3095067\n",
      "step 68201, epoch 0 (total steps = 68201): 57.30632770061493\n",
      "1709385968.6822252\n",
      "step 68301, epoch 0 (total steps = 68301): 57.414054691791534\n",
      "1709385969.0370953\n",
      "step 68401, epoch 0 (total steps = 68401): 57.680258095264435\n",
      "1709385969.3731802\n",
      "step 68501, epoch 0 (total steps = 68501): 57.601326167583466\n",
      "1709385969.7222104\n",
      "step 68601, epoch 0 (total steps = 68601): 57.44382172822952\n",
      "1709385970.0890393\n",
      "step 68701, epoch 0 (total steps = 68701): 57.35113149881363\n",
      "1709385970.432611\n",
      "step 68801, epoch 0 (total steps = 68801): 57.520276725292206\n",
      "1709385970.7788622\n",
      "step 68901, epoch 0 (total steps = 68901): 57.297993659973145\n",
      "1709385971.1227374\n",
      "step 69001, epoch 0 (total steps = 69001): 57.36371797323227\n",
      "1709385971.484103\n",
      "step 69101, epoch 0 (total steps = 69101): 57.52435386180878\n",
      "1709385971.827216\n",
      "step 69201, epoch 0 (total steps = 69201): 57.515857577323914\n",
      "1709385972.1701994\n",
      "step 69301, epoch 0 (total steps = 69301): 57.29324686527252\n",
      "1709385972.5354714\n",
      "step 69401, epoch 0 (total steps = 69401): 57.16389220952988\n",
      "1709385972.909166\n",
      "step 69501, epoch 0 (total steps = 69501): 57.50890773534775\n",
      "1709385973.243715\n",
      "step 69601, epoch 0 (total steps = 69601): 57.354317247867584\n",
      "1709385973.577557\n",
      "step 69701, epoch 0 (total steps = 69701): 57.36846882104874\n",
      "1709385973.9185038\n",
      "step 69801, epoch 0 (total steps = 69801): 57.349567234516144\n",
      "1709385974.253377\n",
      "step 69901, epoch 0 (total steps = 69901): 57.252446830272675\n",
      "1709385974.6043575\n",
      "step 70001, epoch 0 (total steps = 70001): 57.44442963600159\n",
      "1709385974.9453166\n",
      "step 70101, epoch 0 (total steps = 70101): 57.306683361530304\n",
      "1709385975.2926676\n",
      "step 70201, epoch 0 (total steps = 70201): 57.40571469068527\n",
      "1709385975.6353455\n",
      "step 70301, epoch 0 (total steps = 70301): 57.46010798215866\n",
      "1709385975.9853768\n",
      "step 70401, epoch 0 (total steps = 70401): 57.43999654054642\n",
      "1709385976.3222425\n",
      "step 70501, epoch 0 (total steps = 70501): 57.30987274646759\n",
      "1709385976.6650913\n",
      "step 70601, epoch 0 (total steps = 70601): 57.40802443027496\n",
      "1709385977.018886\n",
      "step 70701, epoch 0 (total steps = 70701): 57.40656840801239\n",
      "1709385977.3707871\n",
      "step 70801, epoch 0 (total steps = 70801): 57.42435961961746\n",
      "1709385977.7308936\n",
      "step 70901, epoch 0 (total steps = 70901): 57.197593569755554\n",
      "1709385978.0826757\n",
      "step 71001, epoch 0 (total steps = 71001): 57.366909980773926\n",
      "1709385978.4294062\n",
      "step 71101, epoch 0 (total steps = 71101): 57.31032645702362\n",
      "1709385978.7737494\n",
      "step 71201, epoch 0 (total steps = 71201): 57.298966348171234\n",
      "1709385979.1119394\n",
      "step 71301, epoch 0 (total steps = 71301): 57.515846490859985\n",
      "1709385979.4550712\n",
      "step 71401, epoch 0 (total steps = 71401): 57.26856964826584\n",
      "1709385979.8285882\n",
      "step 71501, epoch 0 (total steps = 71501): 57.53069692850113\n",
      "1709385980.1852098\n",
      "step 71601, epoch 0 (total steps = 71601): 57.58843815326691\n",
      "1709385980.5299146\n",
      "step 71701, epoch 0 (total steps = 71701): 57.479099690914154\n",
      "1709385980.870731\n",
      "step 71801, epoch 0 (total steps = 71801): 57.468070566654205\n",
      "1709385981.22065\n",
      "step 71901, epoch 0 (total steps = 71901): 57.61289471387863\n",
      "1709385981.5520709\n",
      "step 72001, epoch 0 (total steps = 72001): 57.267839193344116\n",
      "1709385981.8874657\n",
      "step 72101, epoch 0 (total steps = 72101): 57.192969381809235\n",
      "1709385982.2212245\n",
      "step 72201, epoch 0 (total steps = 72201): 57.189398884773254\n",
      "1709385982.5791104\n",
      "step 72301, epoch 0 (total steps = 72301): 57.35570603609085\n",
      "1709385982.9357288\n",
      "step 72401, epoch 0 (total steps = 72401): 57.38665163516998\n",
      "1709385983.2734451\n",
      "step 72501, epoch 0 (total steps = 72501): 57.36791205406189\n",
      "1709385983.6120195\n",
      "step 72601, epoch 0 (total steps = 72601): 57.46573102474213\n",
      "1709385983.9664335\n",
      "step 72701, epoch 0 (total steps = 72701): 57.23562002182007\n",
      "1709385984.340387\n",
      "step 72801, epoch 0 (total steps = 72801): 57.39084196090698\n",
      "1709385984.679641\n",
      "step 72901, epoch 0 (total steps = 72901): 57.29829555749893\n",
      "1709385985.023492\n",
      "step 73001, epoch 0 (total steps = 73001): 57.365035116672516\n",
      "1709385985.3800983\n",
      "step 73101, epoch 0 (total steps = 73101): 57.427460968494415\n",
      "1709385985.7302074\n",
      "step 73201, epoch 0 (total steps = 73201): 57.26265895366669\n",
      "1709385986.0721955\n",
      "step 73301, epoch 0 (total steps = 73301): 57.429807305336\n",
      "1709385986.4065437\n",
      "step 73401, epoch 0 (total steps = 73401): 57.03528136014938\n",
      "1709385986.7571585\n",
      "step 73501, epoch 0 (total steps = 73501): 57.38738691806793\n",
      "1709385987.1158235\n",
      "step 73601, epoch 0 (total steps = 73601): 57.09797918796539\n",
      "1709385987.4571788\n",
      "step 73701, epoch 0 (total steps = 73701): 57.374484956264496\n",
      "1709385987.790118\n",
      "step 73801, epoch 0 (total steps = 73801): 57.352565586566925\n",
      "1709385988.133037\n",
      "step 73901, epoch 0 (total steps = 73901): 57.08061325550079\n",
      "1709385988.4765244\n",
      "step 74001, epoch 0 (total steps = 74001): 57.33851021528244\n",
      "1709385988.8201234\n",
      "step 74101, epoch 0 (total steps = 74101): 57.21996068954468\n",
      "1709385989.1548023\n",
      "step 74201, epoch 0 (total steps = 74201): 57.202088534832\n",
      "1709385989.5044878\n",
      "step 74301, epoch 0 (total steps = 74301): 57.16804772615433\n",
      "1709385989.838052\n",
      "step 74401, epoch 0 (total steps = 74401): 57.26580137014389\n",
      "1709385990.1736388\n",
      "step 74501, epoch 0 (total steps = 74501): 57.29169011116028\n",
      "1709385990.5191517\n",
      "step 74601, epoch 0 (total steps = 74601): 57.179322719573975\n",
      "1709385990.8621242\n",
      "step 74701, epoch 0 (total steps = 74701): 57.265993773937225\n",
      "1709385991.2133243\n",
      "step 74801, epoch 0 (total steps = 74801): 57.104785561561584\n",
      "1709385991.5500467\n",
      "step 74901, epoch 0 (total steps = 74901): 57.11208987236023\n",
      "1709385991.8996716\n",
      "step 75001, epoch 0 (total steps = 75001): 57.109910786151886\n",
      "1709385992.2442715\n",
      "step 75101, epoch 0 (total steps = 75101): 57.07359606027603\n",
      "1709385992.6179955\n",
      "step 75201, epoch 0 (total steps = 75201): 57.16614520549774\n",
      "1709385992.953677\n",
      "step 75301, epoch 0 (total steps = 75301): 57.24582505226135\n",
      "1709385993.2977533\n",
      "step 75401, epoch 0 (total steps = 75401): 57.12572491168976\n",
      "1709385993.6413612\n",
      "step 75501, epoch 0 (total steps = 75501): 57.18072456121445\n",
      "1709385994.0059159\n",
      "step 75601, epoch 0 (total steps = 75601): 57.169589161872864\n",
      "1709385994.3509865\n",
      "step 75701, epoch 0 (total steps = 75701): 57.40386205911636\n",
      "1709385994.6885471\n",
      "step 75801, epoch 0 (total steps = 75801): 56.96244388818741\n",
      "1709385995.038532\n",
      "step 75901, epoch 0 (total steps = 75901): 57.42735558748245\n",
      "1709385995.391868\n",
      "step 76001, epoch 0 (total steps = 76001): 57.272581815719604\n",
      "1709385995.7392023\n",
      "step 76101, epoch 0 (total steps = 76101): 57.355717062950134\n",
      "1709385996.1080084\n",
      "step 76201, epoch 0 (total steps = 76201): 56.8651642203331\n",
      "1709385996.4474263\n",
      "step 76301, epoch 0 (total steps = 76301): 57.00352567434311\n",
      "1709385996.7881896\n",
      "step 76401, epoch 0 (total steps = 76401): 57.187034487724304\n",
      "1709385997.1458578\n",
      "step 76501, epoch 0 (total steps = 76501): 57.23247802257538\n",
      "1709385997.489375\n",
      "step 76601, epoch 0 (total steps = 76601): 57.141645073890686\n",
      "1709385997.8636062\n",
      "step 76701, epoch 0 (total steps = 76701): 57.174203395843506\n",
      "1709385998.2167513\n",
      "step 76801, epoch 0 (total steps = 76801): 57.065071523189545\n",
      "1709385998.555586\n",
      "step 76901, epoch 0 (total steps = 76901): 57.1130536198616\n",
      "1709385998.9119284\n",
      "step 77001, epoch 0 (total steps = 77001): 57.11086744070053\n",
      "1709385999.248176\n",
      "step 77101, epoch 0 (total steps = 77101): 57.0669869184494\n",
      "1709385999.5943103\n",
      "step 77201, epoch 0 (total steps = 77201): 57.0729124546051\n",
      "1709385999.9389067\n",
      "step 77301, epoch 0 (total steps = 77301): 57.03946751356125\n",
      "1709386000.2813773\n",
      "step 77401, epoch 0 (total steps = 77401): 56.769166111946106\n",
      "1709386000.6206317\n",
      "step 77501, epoch 0 (total steps = 77501): 57.08477210998535\n",
      "1709386000.9605005\n",
      "step 77601, epoch 0 (total steps = 77601): 57.20182007551193\n",
      "1709386001.315231\n",
      "step 77701, epoch 0 (total steps = 77701): 57.117918848991394\n",
      "1709386001.6666105\n",
      "step 77801, epoch 0 (total steps = 77801): 57.10525596141815\n",
      "1709386002.0091755\n",
      "step 77901, epoch 0 (total steps = 77901): 57.03199768066406\n",
      "1709386002.3450592\n",
      "step 78001, epoch 0 (total steps = 78001): 57.187081813812256\n",
      "1709386002.6840348\n",
      "step 78101, epoch 0 (total steps = 78101): 57.194917023181915\n",
      "1709386003.0311427\n",
      "step 78201, epoch 0 (total steps = 78201): 57.083893179893494\n",
      "1709386003.3888042\n",
      "step 78301, epoch 0 (total steps = 78301): 57.11946541070938\n",
      "1709386003.7516055\n",
      "step 78401, epoch 0 (total steps = 78401): 57.26479512453079\n",
      "1709386004.0979269\n",
      "step 78501, epoch 0 (total steps = 78501): 56.95553547143936\n",
      "1709386004.4376354\n",
      "step 78601, epoch 0 (total steps = 78601): 57.0580929517746\n",
      "1709386004.77909\n",
      "step 78701, epoch 0 (total steps = 78701): 56.89232558012009\n",
      "1709386005.1423774\n",
      "step 78801, epoch 0 (total steps = 78801): 57.11187833547592\n",
      "1709386005.4404905\n",
      "step 78901, epoch 0 (total steps = 78901): 57.053026258945465\n",
      "1709386005.7696729\n",
      "step 79001, epoch 0 (total steps = 79001): 56.902467250823975\n",
      "1709386006.1101677\n",
      "step 79101, epoch 0 (total steps = 79101): 57.21670883893967\n",
      "1709386006.4514365\n",
      "step 79201, epoch 0 (total steps = 79201): 57.028776586055756\n",
      "1709386006.8013492\n",
      "step 79301, epoch 0 (total steps = 79301): 57.131686091423035\n",
      "1709386007.1834018\n",
      "step 79401, epoch 0 (total steps = 79401): 57.18684273958206\n",
      "1709386007.5505233\n",
      "step 79501, epoch 0 (total steps = 79501): 57.007305562496185\n",
      "1709386007.8936012\n",
      "step 79601, epoch 0 (total steps = 79601): 57.16009908914566\n",
      "1709386008.2417457\n",
      "step 79701, epoch 0 (total steps = 79701): 57.37705934047699\n",
      "1709386008.638192\n",
      "step 79801, epoch 0 (total steps = 79801): 57.33691477775574\n",
      "1709386008.9979446\n",
      "step 79901, epoch 0 (total steps = 79901): 56.80956071615219\n",
      "1709386009.3331838\n",
      "step 80001, epoch 0 (total steps = 80001): 57.07613569498062\n",
      "1709386009.6771126\n",
      "step 80101, epoch 0 (total steps = 80101): 56.9626390337944\n",
      "1709386010.0155447\n",
      "step 80201, epoch 0 (total steps = 80201): 57.07894432544708\n",
      "1709386010.3563287\n",
      "step 80301, epoch 0 (total steps = 80301): 56.69096165895462\n",
      "1709386010.697657\n",
      "step 80401, epoch 0 (total steps = 80401): 57.027619898319244\n",
      "1709386011.035452\n",
      "step 80501, epoch 0 (total steps = 80501): 57.00518012046814\n",
      "1709386011.379856\n",
      "step 80601, epoch 0 (total steps = 80601): 56.91681373119354\n",
      "1709386011.7265937\n",
      "step 80701, epoch 0 (total steps = 80701): 57.016239047050476\n",
      "1709386012.089675\n",
      "step 80801, epoch 0 (total steps = 80801): 56.973208248615265\n",
      "1709386012.432482\n",
      "step 80901, epoch 0 (total steps = 80901): 57.0474967956543\n",
      "1709386012.773778\n",
      "step 81001, epoch 0 (total steps = 81001): 57.078533470630646\n",
      "1709386013.1114335\n",
      "step 81101, epoch 0 (total steps = 81101): 56.96021729707718\n",
      "1709386013.4456081\n",
      "step 81201, epoch 0 (total steps = 81201): 56.98148888349533\n",
      "1709386013.7965531\n",
      "step 81301, epoch 0 (total steps = 81301): 56.98924005031586\n",
      "1709386014.1553485\n",
      "step 81401, epoch 0 (total steps = 81401): 56.9423890709877\n",
      "1709386014.4976225\n",
      "step 81501, epoch 0 (total steps = 81501): 57.227611660957336\n",
      "1709386014.835161\n",
      "step 81601, epoch 0 (total steps = 81601): 56.95287483930588\n",
      "1709386015.1725154\n",
      "step 81701, epoch 0 (total steps = 81701): 56.96777004003525\n",
      "1709386015.5456557\n",
      "step 81801, epoch 0 (total steps = 81801): 57.042160391807556\n",
      "1709386015.8827887\n",
      "step 81901, epoch 0 (total steps = 81901): 56.99840313196182\n",
      "1709386016.2374609\n",
      "step 82001, epoch 0 (total steps = 82001): 57.00527614355087\n",
      "1709386016.5885527\n",
      "step 82101, epoch 0 (total steps = 82101): 56.77295958995819\n",
      "1709386016.9570036\n",
      "step 82201, epoch 0 (total steps = 82201): 57.06357342004776\n",
      "1709386017.3013113\n",
      "step 82301, epoch 0 (total steps = 82301): 57.00540900230408\n",
      "1709386017.647158\n",
      "step 82401, epoch 0 (total steps = 82401): 57.0605486035347\n",
      "1709386018.0067847\n",
      "step 82501, epoch 0 (total steps = 82501): 56.85051876306534\n",
      "1709386018.35487\n",
      "step 82601, epoch 0 (total steps = 82601): 56.99087768793106\n",
      "1709386018.7320828\n",
      "step 82701, epoch 0 (total steps = 82701): 56.85480135679245\n",
      "1709386019.0749464\n",
      "step 82801, epoch 0 (total steps = 82801): 56.72027111053467\n",
      "1709386019.4115906\n",
      "step 82901, epoch 0 (total steps = 82901): 56.70409893989563\n",
      "1709386019.765235\n",
      "step 83001, epoch 0 (total steps = 83001): 57.034813821315765\n",
      "1709386020.111687\n",
      "step 83101, epoch 0 (total steps = 83101): 57.02377414703369\n",
      "1709386020.4466562\n",
      "step 83201, epoch 0 (total steps = 83201): 56.9452919960022\n",
      "1709386020.785106\n",
      "step 83301, epoch 0 (total steps = 83301): 56.771570920944214\n",
      "1709386021.135394\n",
      "step 83401, epoch 0 (total steps = 83401): 56.9481565952301\n",
      "1709386021.4726686\n",
      "step 83501, epoch 0 (total steps = 83501): 56.726745545864105\n",
      "1709386021.8076322\n",
      "step 83601, epoch 0 (total steps = 83601): 56.91267395019531\n",
      "1709386022.1580334\n",
      "step 83701, epoch 0 (total steps = 83701): 56.828496754169464\n",
      "1709386022.507967\n",
      "step 83801, epoch 0 (total steps = 83801): 56.91173183917999\n",
      "1709386022.8518014\n",
      "step 83901, epoch 0 (total steps = 83901): 56.72059816122055\n",
      "1709386023.2091622\n",
      "step 84001, epoch 0 (total steps = 84001): 56.874535381793976\n",
      "1709386023.5440192\n",
      "step 84101, epoch 0 (total steps = 84101): 56.83929663896561\n",
      "1709386023.9023676\n",
      "step 84201, epoch 0 (total steps = 84201): 56.745699524879456\n",
      "1709386024.2448816\n",
      "step 84301, epoch 0 (total steps = 84301): 56.90058398246765\n",
      "1709386024.5792038\n",
      "step 84401, epoch 0 (total steps = 84401): 57.03730410337448\n",
      "1709386024.9338617\n",
      "step 84501, epoch 0 (total steps = 84501): 57.050234735012054\n",
      "1709386025.2893221\n",
      "step 84601, epoch 0 (total steps = 84601): 56.800759077072144\n",
      "1709386025.6228774\n",
      "step 84701, epoch 0 (total steps = 84701): 57.083851873874664\n",
      "1709386025.9878316\n",
      "step 84801, epoch 0 (total steps = 84801): 56.86307483911514\n",
      "1709386026.3530126\n",
      "step 84901, epoch 0 (total steps = 84901): 56.852895975112915\n",
      "1709386026.6976686\n",
      "step 85001, epoch 0 (total steps = 85001): 56.80339705944061\n",
      "1709386027.0376627\n",
      "step 85101, epoch 0 (total steps = 85101): 56.69850718975067\n",
      "1709386027.3817532\n",
      "step 85201, epoch 0 (total steps = 85201): 56.581546664237976\n",
      "1709386027.7311962\n",
      "step 85301, epoch 0 (total steps = 85301): 56.713198244571686\n",
      "1709386028.0986583\n",
      "step 85401, epoch 0 (total steps = 85401): 56.84375315904617\n",
      "1709386028.4728808\n",
      "step 85501, epoch 0 (total steps = 85501): 56.739959359169006\n",
      "1709386028.8288555\n",
      "step 85601, epoch 0 (total steps = 85601): 56.82049244642258\n",
      "1709386029.1743152\n",
      "step 85701, epoch 0 (total steps = 85701): 56.9079253077507\n",
      "1709386029.5161302\n",
      "step 85801, epoch 0 (total steps = 85801): 56.88010787963867\n",
      "1709386029.902361\n",
      "step 85901, epoch 0 (total steps = 85901): 56.88805204629898\n",
      "1709386030.2551198\n",
      "step 86001, epoch 0 (total steps = 86001): 56.92737573385239\n",
      "1709386030.5981002\n",
      "step 86101, epoch 0 (total steps = 86101): 56.761547565460205\n",
      "1709386030.9424317\n",
      "step 86201, epoch 0 (total steps = 86201): 56.71843957901001\n",
      "1709386031.2837842\n",
      "step 86301, epoch 0 (total steps = 86301): 56.97279870510101\n",
      "1709386031.6319685\n",
      "step 86401, epoch 0 (total steps = 86401): 56.76443433761597\n",
      "1709386032.0081646\n",
      "step 86501, epoch 0 (total steps = 86501): 56.79802882671356\n",
      "1709386032.3578193\n",
      "step 86601, epoch 0 (total steps = 86601): 56.66730582714081\n",
      "1709386032.7112255\n",
      "step 86701, epoch 0 (total steps = 86701): 56.80306440591812\n",
      "1709386033.0849009\n",
      "step 86801, epoch 0 (total steps = 86801): 56.90359926223755\n",
      "1709386033.4298797\n",
      "step 86901, epoch 0 (total steps = 86901): 56.85456866025925\n",
      "1709386033.781543\n",
      "step 87001, epoch 0 (total steps = 87001): 56.66700613498688\n",
      "1709386034.1261704\n",
      "step 87101, epoch 0 (total steps = 87101): 57.02520394325256\n",
      "1709386034.5024414\n",
      "step 87201, epoch 0 (total steps = 87201): 56.27583426237106\n",
      "1709386034.851132\n",
      "step 87301, epoch 0 (total steps = 87301): 56.757098853588104\n",
      "1709386035.1964536\n",
      "step 87401, epoch 0 (total steps = 87401): 57.059926867485046\n",
      "1709386035.5508058\n",
      "step 87501, epoch 0 (total steps = 87501): 56.56320095062256\n",
      "1709386035.9045548\n",
      "step 87601, epoch 0 (total steps = 87601): 56.65795785188675\n",
      "1709386036.249316\n",
      "step 87701, epoch 0 (total steps = 87701): 56.75922626256943\n",
      "1709386036.6234534\n",
      "step 87801, epoch 0 (total steps = 87801): 56.693280935287476\n",
      "1709386036.9727595\n",
      "step 87901, epoch 0 (total steps = 87901): 56.68478584289551\n",
      "1709386037.3288183\n",
      "step 88001, epoch 0 (total steps = 88001): 56.563444554805756\n",
      "1709386037.6695378\n",
      "step 88101, epoch 0 (total steps = 88101): 56.583727300167084\n",
      "1709386038.0161312\n",
      "step 88201, epoch 0 (total steps = 88201): 56.83790564537048\n",
      "1709386038.3628602\n",
      "step 88301, epoch 0 (total steps = 88301): 56.84388965368271\n",
      "1709386038.7127686\n",
      "step 88401, epoch 0 (total steps = 88401): 56.980567157268524\n",
      "1709386039.0925717\n",
      "step 88501, epoch 0 (total steps = 88501): 56.68673276901245\n",
      "1709386039.460161\n",
      "step 88601, epoch 0 (total steps = 88601): 56.555694937705994\n",
      "1709386039.8025365\n",
      "step 88701, epoch 0 (total steps = 88701): 56.61973416805267\n",
      "1709386040.1435995\n",
      "step 88801, epoch 0 (total steps = 88801): 56.891774117946625\n",
      "1709386040.4848628\n",
      "step 88901, epoch 0 (total steps = 88901): 56.691784143447876\n",
      "1709386040.8488617\n",
      "step 89001, epoch 0 (total steps = 89001): 56.579879462718964\n",
      "1709386041.1905103\n",
      "step 89101, epoch 0 (total steps = 89101): 56.6243017911911\n",
      "1709386041.562178\n",
      "step 89201, epoch 0 (total steps = 89201): 56.97159868478775\n",
      "1709386041.9111238\n",
      "step 89301, epoch 0 (total steps = 89301): 56.87751913070679\n",
      "1709386042.2553067\n",
      "step 89401, epoch 0 (total steps = 89401): 56.81829077005386\n",
      "1709386042.6067848\n",
      "step 89501, epoch 0 (total steps = 89501): 56.75128388404846\n",
      "1709386042.9731581\n",
      "step 89601, epoch 0 (total steps = 89601): 56.68272042274475\n",
      "1709386043.3299265\n",
      "step 89701, epoch 0 (total steps = 89701): 56.61615014076233\n",
      "1709386043.6825044\n",
      "step 89801, epoch 0 (total steps = 89801): 56.48303925991058\n",
      "1709386044.0224962\n",
      "step 89901, epoch 0 (total steps = 89901): 56.82663559913635\n",
      "1709386044.381397\n",
      "step 90001, epoch 0 (total steps = 90001): 56.7774783372879\n",
      "1709386044.7282147\n",
      "step 90101, epoch 0 (total steps = 90101): 56.682979702949524\n",
      "1709386045.0662584\n",
      "step 90201, epoch 0 (total steps = 90201): 56.83480781316757\n",
      "1709386045.403371\n",
      "step 90301, epoch 0 (total steps = 90301): 56.66189879179001\n",
      "1709386045.7415874\n",
      "step 90401, epoch 0 (total steps = 90401): 56.83161270618439\n",
      "1709386046.0951614\n",
      "step 90501, epoch 0 (total steps = 90501): 56.636885583400726\n",
      "1709386046.4344347\n",
      "step 90601, epoch 0 (total steps = 90601): 56.62924021482468\n",
      "1709386046.7912478\n",
      "step 90701, epoch 0 (total steps = 90701): 56.568420469760895\n",
      "1709386047.1398573\n",
      "step 90801, epoch 0 (total steps = 90801): 56.69934421777725\n",
      "1709386047.4860718\n",
      "step 90901, epoch 0 (total steps = 90901): 56.68207573890686\n",
      "1709386047.8199902\n",
      "step 91001, epoch 0 (total steps = 91001): 56.67396330833435\n",
      "1709386048.1571212\n",
      "step 91101, epoch 0 (total steps = 91101): 56.60513311624527\n",
      "1709386048.521239\n",
      "step 91201, epoch 0 (total steps = 91201): 56.58498400449753\n",
      "1709386048.868602\n",
      "step 91301, epoch 0 (total steps = 91301): 56.66361105442047\n",
      "1709386049.2186494\n",
      "step 91401, epoch 0 (total steps = 91401): 56.34988605976105\n",
      "1709386049.5530984\n",
      "step 91501, epoch 0 (total steps = 91501): 56.560378670692444\n",
      "1709386049.8995056\n",
      "step 91601, epoch 0 (total steps = 91601): 56.65766084194183\n",
      "1709386050.2334223\n",
      "step 91701, epoch 0 (total steps = 91701): 56.634800255298615\n",
      "1709386050.5893595\n",
      "step 91801, epoch 0 (total steps = 91801): 56.65331619977951\n",
      "1709386050.931368\n",
      "step 91901, epoch 0 (total steps = 91901): 56.54376417398453\n",
      "1709386051.2699225\n",
      "step 92001, epoch 0 (total steps = 92001): 56.65963566303253\n",
      "1709386051.6349635\n",
      "step 92101, epoch 0 (total steps = 92101): 56.642763555049896\n",
      "1709386051.980882\n",
      "step 92201, epoch 0 (total steps = 92201): 56.43429011106491\n",
      "1709386052.3350642\n",
      "step 92301, epoch 0 (total steps = 92301): 56.80792325735092\n",
      "1709386052.6838963\n",
      "step 92401, epoch 0 (total steps = 92401): 56.67052322626114\n",
      "1709386053.0304537\n",
      "step 92501, epoch 0 (total steps = 92501): 56.48607814311981\n",
      "1709386053.3717766\n",
      "step 92601, epoch 0 (total steps = 92601): 56.62255269289017\n",
      "1709386053.729123\n",
      "step 92701, epoch 0 (total steps = 92701): 56.77345782518387\n",
      "1709386054.0923579\n",
      "step 92801, epoch 0 (total steps = 92801): 56.400171637535095\n",
      "1709386054.4277368\n",
      "step 92901, epoch 0 (total steps = 92901): 56.504288375377655\n",
      "1709386054.7614224\n",
      "step 93001, epoch 0 (total steps = 93001): 56.603329956531525\n",
      "1709386055.0989141\n",
      "step 93101, epoch 0 (total steps = 93101): 56.611322700977325\n",
      "1709386055.5087516\n",
      "step 93201, epoch 0 (total steps = 93201): 56.62094455957413\n",
      "1709386055.8621771\n",
      "step 93301, epoch 0 (total steps = 93301): 56.622631907463074\n",
      "1709386056.2181153\n",
      "step 93401, epoch 0 (total steps = 93401): 56.44885689020157\n",
      "1709386056.5747118\n",
      "step 93501, epoch 0 (total steps = 93501): 56.61406999826431\n",
      "1709386056.927897\n",
      "step 93601, epoch 0 (total steps = 93601): 56.5365914106369\n",
      "1709386057.295034\n",
      "step 93701, epoch 0 (total steps = 93701): 56.49295252561569\n",
      "1709386057.6639738\n",
      "step 93801, epoch 0 (total steps = 93801): 56.76591283082962\n",
      "1709386058.0107064\n",
      "step 93901, epoch 0 (total steps = 93901): 56.71857696771622\n",
      "1709386058.3523762\n",
      "step 94001, epoch 0 (total steps = 94001): 56.50768828392029\n",
      "1709386058.6933901\n",
      "step 94101, epoch 0 (total steps = 94101): 56.578284084796906\n",
      "1709386059.0263264\n",
      "step 94201, epoch 0 (total steps = 94201): 56.63725680112839\n",
      "1709386059.3679903\n",
      "step 94301, epoch 0 (total steps = 94301): 56.6054322719574\n",
      "1709386059.7098613\n",
      "step 94401, epoch 0 (total steps = 94401): 56.8112587928772\n",
      "1709386060.0456848\n",
      "step 94501, epoch 0 (total steps = 94501): 56.633846282958984\n",
      "1709386060.4063394\n",
      "step 94601, epoch 0 (total steps = 94601): 56.49418658018112\n",
      "1709386060.7521555\n",
      "step 94701, epoch 0 (total steps = 94701): 56.47777843475342\n",
      "1709386061.0839412\n",
      "step 94801, epoch 0 (total steps = 94801): 56.55138510465622\n",
      "1709386061.4150727\n",
      "step 94901, epoch 0 (total steps = 94901): 56.359825015068054\n",
      "1709386061.761895\n",
      "step 95001, epoch 0 (total steps = 95001): 56.56495916843414\n",
      "1709386062.0958798\n",
      "step 95101, epoch 0 (total steps = 95101): 56.63191956281662\n",
      "1709386062.4306073\n",
      "step 95201, epoch 0 (total steps = 95201): 56.734820783138275\n",
      "1709386062.764376\n",
      "step 95301, epoch 0 (total steps = 95301): 56.63263154029846\n",
      "1709386063.12217\n",
      "step 95401, epoch 0 (total steps = 95401): 56.513171315193176\n",
      "1709386063.458484\n",
      "step 95501, epoch 0 (total steps = 95501): 56.31138211488724\n",
      "1709386063.7948287\n",
      "step 95601, epoch 0 (total steps = 95601): 56.47739201784134\n",
      "1709386064.1282518\n",
      "step 95701, epoch 0 (total steps = 95701): 56.45903921127319\n",
      "1709386064.4884844\n",
      "step 95801, epoch 0 (total steps = 95801): 56.3049978017807\n",
      "1709386064.831211\n",
      "step 95901, epoch 0 (total steps = 95901): 56.79127126932144\n",
      "1709386065.1720917\n",
      "step 96001, epoch 0 (total steps = 96001): 56.381450176239014\n",
      "1709386065.5128558\n",
      "step 96101, epoch 0 (total steps = 96101): 56.43590646982193\n",
      "1709386065.8753526\n",
      "step 96201, epoch 0 (total steps = 96201): 56.50877684354782\n",
      "1709386066.2220163\n",
      "step 96301, epoch 0 (total steps = 96301): 56.465761840343475\n",
      "1709386066.5749662\n",
      "step 96401, epoch 0 (total steps = 96401): 56.68842309713364\n",
      "1709386066.9328158\n",
      "step 96501, epoch 0 (total steps = 96501): 56.40174102783203\n",
      "1709386067.2677462\n",
      "step 96601, epoch 0 (total steps = 96601): 56.57941538095474\n",
      "1709386067.6478486\n",
      "step 96701, epoch 0 (total steps = 96701): 56.687784075737\n",
      "1709386068.002895\n",
      "step 96801, epoch 0 (total steps = 96801): 56.27258336544037\n",
      "1709386068.3363721\n",
      "step 96901, epoch 0 (total steps = 96901): 56.404667019844055\n",
      "1709386068.6707976\n",
      "step 97001, epoch 0 (total steps = 97001): 56.51884037256241\n",
      "1709386069.0047357\n",
      "step 97101, epoch 0 (total steps = 97101): 56.263597667217255\n",
      "1709386069.342599\n",
      "step 97201, epoch 0 (total steps = 97201): 56.29296374320984\n",
      "1709386069.7052178\n",
      "step 97301, epoch 0 (total steps = 97301): 56.20531439781189\n",
      "1709386070.0636787\n",
      "step 97401, epoch 0 (total steps = 97401): 56.37800490856171\n",
      "1709386070.3973668\n",
      "step 97501, epoch 0 (total steps = 97501): 56.394098579883575\n",
      "1709386070.7389414\n",
      "step 97601, epoch 0 (total steps = 97601): 56.447736620903015\n",
      "1709386071.0799248\n",
      "step 97701, epoch 0 (total steps = 97701): 56.3462478518486\n",
      "1709386071.4162734\n",
      "step 97801, epoch 0 (total steps = 97801): 56.531995475292206\n",
      "1709386071.7615883\n",
      "step 97901, epoch 0 (total steps = 97901): 56.53194320201874\n",
      "1709386072.0990202\n",
      "step 98001, epoch 0 (total steps = 98001): 56.48801875114441\n",
      "1709386072.4753845\n",
      "step 98101, epoch 0 (total steps = 98101): 56.34338843822479\n",
      "1709386072.812767\n",
      "step 98201, epoch 0 (total steps = 98201): 56.27356433868408\n",
      "1709386073.1505787\n",
      "step 98301, epoch 0 (total steps = 98301): 56.49054056406021\n",
      "1709386073.498538\n",
      "step 98401, epoch 0 (total steps = 98401): 56.61058461666107\n",
      "1709386073.833803\n",
      "step 98501, epoch 0 (total steps = 98501): 56.45474249124527\n",
      "1709386074.1892185\n",
      "step 98601, epoch 0 (total steps = 98601): 56.255003929138184\n",
      "1709386074.522789\n",
      "step 98701, epoch 0 (total steps = 98701): 56.20838987827301\n",
      "1709386074.8577583\n",
      "step 98801, epoch 0 (total steps = 98801): 56.2524209022522\n",
      "1709386075.232006\n",
      "step 98901, epoch 0 (total steps = 98901): 56.68509221076965\n",
      "1709386075.5827544\n",
      "step 99001, epoch 0 (total steps = 99001): 56.63978189229965\n",
      "1709386075.947983\n",
      "step 99101, epoch 0 (total steps = 99101): 56.32495701313019\n",
      "1709386076.2822747\n",
      "step 99201, epoch 0 (total steps = 99201): 56.56323003768921\n",
      "1709386076.626856\n",
      "step 99301, epoch 0 (total steps = 99301): 56.29965829849243\n",
      "1709386076.9941947\n",
      "step 99401, epoch 0 (total steps = 99401): 56.232584834098816\n",
      "1709386077.3296628\n",
      "step 99501, epoch 0 (total steps = 99501): 56.50741583108902\n",
      "1709386077.665626\n",
      "step 99601, epoch 0 (total steps = 99601): 56.34743869304657\n",
      "1709386078.0094159\n",
      "step 99701, epoch 0 (total steps = 99701): 56.50248646736145\n",
      "1709386078.3435214\n",
      "step 99801, epoch 0 (total steps = 99801): 56.4015154838562\n",
      "1709386078.7003107\n",
      "step 99901, epoch 0 (total steps = 99901): 56.36562639474869\n",
      "1709386079.044335\n",
      "step 100001, epoch 0 (total steps = 100001): 56.28106588125229\n",
      "1709386079.3811717\n",
      "step 100101, epoch 0 (total steps = 100101): 56.423077285289764\n",
      "1709386079.7291794\n",
      "step 100201, epoch 0 (total steps = 100201): 56.30549705028534\n",
      "1709386080.0851684\n",
      "step 100301, epoch 0 (total steps = 100301): 56.23617321252823\n",
      "1709386080.4391387\n",
      "step 100401, epoch 0 (total steps = 100401): 56.461583852767944\n",
      "1709386080.7807484\n",
      "step 100501, epoch 0 (total steps = 100501): 56.393630146980286\n",
      "1709386081.1206071\n",
      "step 100601, epoch 0 (total steps = 100601): 56.312568843364716\n",
      "1709386081.4674807\n",
      "step 100701, epoch 0 (total steps = 100701): 56.30554109811783\n",
      "1709386081.8225136\n",
      "step 100801, epoch 0 (total steps = 100801): 56.246311008930206\n",
      "1709386082.1813035\n",
      "step 100901, epoch 0 (total steps = 100901): 56.33517372608185\n",
      "1709386082.5170176\n",
      "step 101001, epoch 0 (total steps = 101001): 56.453451573848724\n",
      "1709386082.859721\n",
      "step 101101, epoch 0 (total steps = 101101): 56.4198699593544\n",
      "1709386083.2219896\n",
      "step 101201, epoch 0 (total steps = 101201): 56.19067758321762\n",
      "1709386083.5811892\n",
      "step 101301, epoch 0 (total steps = 101301): 56.34375560283661\n",
      "1709386083.9467437\n",
      "step 101401, epoch 0 (total steps = 101401): 56.63208293914795\n",
      "1709386084.2845764\n",
      "step 101501, epoch 0 (total steps = 101501): 56.09030771255493\n",
      "1709386084.6282144\n",
      "step 101601, epoch 0 (total steps = 101601): 56.572794675827026\n",
      "1709386084.971951\n",
      "step 101701, epoch 0 (total steps = 101701): 56.29612743854523\n",
      "1709386085.320556\n",
      "step 101801, epoch 0 (total steps = 101801): 56.55380356311798\n",
      "1709386085.659657\n",
      "step 101901, epoch 0 (total steps = 101901): 56.50194841623306\n",
      "1709386086.0287209\n",
      "step 102001, epoch 0 (total steps = 102001): 56.406593799591064\n",
      "1709386086.382946\n",
      "step 102101, epoch 0 (total steps = 102101): 56.45125252008438\n",
      "1709386086.7404912\n",
      "step 102201, epoch 0 (total steps = 102201): 56.29393273591995\n",
      "1709386087.0744717\n",
      "step 102301, epoch 0 (total steps = 102301): 56.08022028207779\n",
      "1709386087.436896\n",
      "step 102401, epoch 0 (total steps = 102401): 56.56435161828995\n",
      "1709386087.7845044\n",
      "step 102501, epoch 0 (total steps = 102501): 56.42040699720383\n",
      "1709386088.1311426\n",
      "step 102601, epoch 0 (total steps = 102601): 56.13737344741821\n",
      "1709386088.4757845\n",
      "step 102701, epoch 0 (total steps = 102701): 56.45834285020828\n",
      "1709386088.8233755\n",
      "step 102801, epoch 0 (total steps = 102801): 56.56302613019943\n",
      "1709386089.159251\n",
      "step 102901, epoch 0 (total steps = 102901): 56.10570502281189\n",
      "1709386089.5086076\n",
      "step 103001, epoch 0 (total steps = 103001): 56.215464651584625\n",
      "1709386089.8514965\n",
      "step 103101, epoch 0 (total steps = 103101): 56.32347071170807\n",
      "1709386090.200468\n",
      "step 103201, epoch 0 (total steps = 103201): 56.241226851940155\n",
      "1709386090.5529625\n",
      "step 103301, epoch 0 (total steps = 103301): 56.329758524894714\n",
      "1709386090.8995056\n",
      "step 103401, epoch 0 (total steps = 103401): 56.15553522109985\n",
      "1709386091.2420268\n",
      "step 103501, epoch 0 (total steps = 103501): 56.2640643119812\n",
      "1709386091.6293945\n",
      "step 103601, epoch 0 (total steps = 103601): 56.44982588291168\n",
      "1709386091.987307\n",
      "step 103701, epoch 0 (total steps = 103701): 56.29969084262848\n",
      "1709386092.3380308\n",
      "step 103801, epoch 0 (total steps = 103801): 56.16336369514465\n",
      "1709386092.6733146\n",
      "step 103901, epoch 0 (total steps = 103901): 56.39602655172348\n",
      "1709386093.0259218\n",
      "step 104001, epoch 0 (total steps = 104001): 56.31186527013779\n",
      "1709386093.3969262\n",
      "step 104101, epoch 0 (total steps = 104101): 56.31404423713684\n",
      "1709386093.7368586\n",
      "step 104201, epoch 0 (total steps = 104201): 55.949774861335754\n",
      "1709386094.0862896\n",
      "step 104301, epoch 0 (total steps = 104301): 56.49854099750519\n",
      "1709386094.4278371\n",
      "step 104401, epoch 0 (total steps = 104401): 56.03050309419632\n",
      "1709386094.7690418\n",
      "step 104501, epoch 0 (total steps = 104501): 56.34424513578415\n",
      "1709386095.119299\n",
      "step 104601, epoch 0 (total steps = 104601): 56.357805788517\n",
      "1709386095.454241\n",
      "step 104701, epoch 0 (total steps = 104701): 56.27530950307846\n",
      "1709386095.789014\n",
      "step 104801, epoch 0 (total steps = 104801): 56.242799520492554\n",
      "1709386096.1556582\n",
      "step 104901, epoch 0 (total steps = 104901): 56.23299425840378\n",
      "1709386096.4990902\n",
      "step 105001, epoch 0 (total steps = 105001): 56.293699741363525\n",
      "1709386096.8413436\n",
      "step 105101, epoch 0 (total steps = 105101): 56.15128767490387\n",
      "1709386097.1864502\n",
      "step 105201, epoch 0 (total steps = 105201): 56.111038744449615\n",
      "1709386097.5236828\n",
      "step 105301, epoch 0 (total steps = 105301): 56.34160089492798\n",
      "1709386097.8689365\n",
      "step 105401, epoch 0 (total steps = 105401): 56.130133271217346\n",
      "1709386098.219932\n",
      "step 105501, epoch 0 (total steps = 105501): 56.411397993564606\n",
      "1709386098.57519\n",
      "step 105601, epoch 0 (total steps = 105601): 56.28003013134003\n",
      "1709386098.9147973\n",
      "step 105701, epoch 0 (total steps = 105701): 56.13850378990173\n",
      "1709386099.2548616\n",
      "step 105801, epoch 0 (total steps = 105801): 56.19387626647949\n",
      "1709386099.6133912\n",
      "step 105901, epoch 0 (total steps = 105901): 56.200035572052\n",
      "1709386099.9566393\n",
      "step 106001, epoch 0 (total steps = 106001): 56.250855565071106\n",
      "1709386100.3003592\n",
      "step 106101, epoch 0 (total steps = 106101): 56.244627594947815\n",
      "1709386100.6389942\n",
      "step 106201, epoch 0 (total steps = 106201): 56.20559149980545\n",
      "1709386100.9862952\n",
      "step 106301, epoch 0 (total steps = 106301): 56.21663564443588\n",
      "1709386101.3322868\n",
      "step 106401, epoch 0 (total steps = 106401): 56.30882853269577\n",
      "1709386101.6858444\n",
      "step 106501, epoch 0 (total steps = 106501): 55.906337797641754\n",
      "1709386102.0280836\n",
      "step 106601, epoch 0 (total steps = 106601): 56.426745772361755\n",
      "1709386102.3728433\n",
      "step 106701, epoch 0 (total steps = 106701): 56.146604120731354\n",
      "1709386102.711315\n",
      "step 106801, epoch 0 (total steps = 106801): 56.032446801662445\n",
      "1709386103.0900388\n",
      "step 106901, epoch 0 (total steps = 106901): 56.344539642333984\n",
      "1709386103.4280858\n",
      "step 107001, epoch 0 (total steps = 107001): 56.47000169754028\n",
      "1709386103.7661746\n",
      "step 107101, epoch 0 (total steps = 107101): 56.09686982631683\n",
      "1709386104.142025\n",
      "step 107201, epoch 0 (total steps = 107201): 56.03733903169632\n",
      "1709386104.4927092\n",
      "step 107301, epoch 0 (total steps = 107301): 56.332524716854095\n",
      "1709386104.8566144\n",
      "step 107401, epoch 0 (total steps = 107401): 56.034273624420166\n",
      "1709386105.244488\n",
      "step 107501, epoch 0 (total steps = 107501): 56.26823651790619\n",
      "1709386105.591549\n",
      "step 107601, epoch 0 (total steps = 107601): 56.143635511398315\n",
      "1709386105.929005\n",
      "step 107701, epoch 0 (total steps = 107701): 56.37267106771469\n",
      "1709386106.274125\n",
      "step 107801, epoch 0 (total steps = 107801): 56.45586770772934\n",
      "1709386106.6295552\n",
      "step 107901, epoch 0 (total steps = 107901): 56.20653611421585\n",
      "1709386106.9771953\n",
      "step 108001, epoch 0 (total steps = 108001): 56.24576675891876\n",
      "1709386107.3265877\n",
      "step 108101, epoch 0 (total steps = 108101): 56.470546782016754\n",
      "1709386107.662511\n",
      "step 108201, epoch 0 (total steps = 108201): 56.18373930454254\n",
      "1709386108.026214\n",
      "step 108301, epoch 0 (total steps = 108301): 56.21142095327377\n",
      "1709386108.372224\n",
      "step 108401, epoch 0 (total steps = 108401): 56.06275510787964\n",
      "1709386108.7380598\n",
      "step 108501, epoch 0 (total steps = 108501): 56.122298419475555\n",
      "1709386109.0803266\n",
      "step 108601, epoch 0 (total steps = 108601): 55.912606596946716\n",
      "1709386109.4172444\n",
      "step 108701, epoch 0 (total steps = 108701): 56.42286533117294\n",
      "1709386109.7693772\n",
      "step 108801, epoch 0 (total steps = 108801): 56.05434948205948\n",
      "1709386110.1200876\n",
      "step 108901, epoch 0 (total steps = 108901): 56.004369258880615\n",
      "1709386110.4577177\n",
      "step 109001, epoch 0 (total steps = 109001): 56.27667433023453\n",
      "1709386110.8022487\n",
      "step 109101, epoch 0 (total steps = 109101): 56.12711954116821\n",
      "1709386111.144756\n",
      "step 109201, epoch 0 (total steps = 109201): 56.15024209022522\n",
      "1709386111.489185\n",
      "step 109301, epoch 0 (total steps = 109301): 56.26218265295029\n",
      "1709386111.853639\n",
      "step 109401, epoch 0 (total steps = 109401): 56.11936163902283\n",
      "1709386112.1956277\n",
      "step 109501, epoch 0 (total steps = 109501): 56.32078790664673\n",
      "1709386112.5396874\n",
      "step 109601, epoch 0 (total steps = 109601): 56.13895297050476\n",
      "1709386112.8719177\n",
      "step 109701, epoch 0 (total steps = 109701): 56.21703201532364\n",
      "1709386113.2153869\n",
      "step 109801, epoch 0 (total steps = 109801): 56.23661923408508\n",
      "1709386113.5509202\n",
      "step 109901, epoch 0 (total steps = 109901): 56.21603316068649\n",
      "1709386113.8864496\n",
      "step 110001, epoch 0 (total steps = 110001): 56.18946301937103\n",
      "1709386114.2253096\n",
      "step 110101, epoch 0 (total steps = 110101): 56.05610227584839\n",
      "1709386114.5602744\n",
      "step 110201, epoch 0 (total steps = 110201): 55.890466153621674\n",
      "1709386114.8986962\n",
      "step 110301, epoch 0 (total steps = 110301): 56.001867055892944\n",
      "1709386115.2367308\n",
      "step 110401, epoch 0 (total steps = 110401): 56.26042836904526\n",
      "1709386115.580692\n",
      "step 110501, epoch 0 (total steps = 110501): 56.02062487602234\n",
      "1709386115.9288402\n",
      "step 110601, epoch 0 (total steps = 110601): 56.14409399032593\n",
      "1709386116.3203766\n",
      "step 110701, epoch 0 (total steps = 110701): 56.18684923648834\n",
      "1709386116.65731\n",
      "step 110801, epoch 0 (total steps = 110801): 56.30440157651901\n",
      "1709386117.0185218\n",
      "step 110901, epoch 0 (total steps = 110901): 56.191225469112396\n",
      "1709386117.3632717\n",
      "step 111001, epoch 0 (total steps = 111001): 55.78520208597183\n",
      "1709386117.7277553\n",
      "step 111101, epoch 0 (total steps = 111101): 56.15868628025055\n",
      "1709386118.0806816\n",
      "step 111201, epoch 0 (total steps = 111201): 56.22141808271408\n",
      "1709386118.4239314\n",
      "step 111301, epoch 0 (total steps = 111301): 56.1479971408844\n",
      "1709386118.799801\n",
      "step 111401, epoch 0 (total steps = 111401): 55.86021935939789\n",
      "1709386119.1361966\n",
      "step 111501, epoch 0 (total steps = 111501): 56.16731923818588\n",
      "1709386119.4722755\n",
      "step 111601, epoch 0 (total steps = 111601): 56.12899607419968\n",
      "1709386119.811266\n",
      "step 111701, epoch 0 (total steps = 111701): 56.225436329841614\n",
      "1709386120.1537647\n",
      "step 111801, epoch 0 (total steps = 111801): 56.13671541213989\n",
      "1709386120.5264282\n",
      "step 111901, epoch 0 (total steps = 111901): 56.081972897052765\n",
      "1709386120.8677182\n",
      "step 112001, epoch 0 (total steps = 112001): 56.16399586200714\n",
      "1709386121.2104404\n",
      "step 112101, epoch 0 (total steps = 112101): 55.85657662153244\n",
      "1709386121.5558708\n",
      "step 112201, epoch 0 (total steps = 112201): 56.08666080236435\n",
      "1709386121.8995547\n",
      "step 112301, epoch 0 (total steps = 112301): 56.10077065229416\n",
      "1709386122.259866\n",
      "step 112401, epoch 0 (total steps = 112401): 56.12872099876404\n",
      "1709386122.618403\n",
      "step 112501, epoch 0 (total steps = 112501): 55.82428032159805\n",
      "1709386122.9690247\n",
      "step 112601, epoch 0 (total steps = 112601): 56.23700422048569\n",
      "1709386123.3210728\n",
      "step 112701, epoch 0 (total steps = 112701): 56.21130710840225\n",
      "1709386123.6723633\n",
      "step 112801, epoch 0 (total steps = 112801): 55.822021782398224\n",
      "1709386124.010988\n",
      "step 112901, epoch 0 (total steps = 112901): 56.05259960889816\n",
      "1709386124.3632355\n",
      "step 113001, epoch 0 (total steps = 113001): 56.27404087781906\n",
      "1709386124.7076561\n",
      "step 113101, epoch 0 (total steps = 113101): 56.0522478222847\n",
      "1709386125.06618\n",
      "step 113201, epoch 0 (total steps = 113201): 56.08587443828583\n",
      "1709386125.4417512\n",
      "step 113301, epoch 0 (total steps = 113301): 56.08528256416321\n",
      "1709386125.7872238\n",
      "step 113401, epoch 0 (total steps = 113401): 56.2878001332283\n",
      "1709386126.1311376\n",
      "step 113501, epoch 0 (total steps = 113501): 56.20393079519272\n",
      "1709386126.4665544\n",
      "step 113601, epoch 0 (total steps = 113601): 56.145891547203064\n",
      "1709386126.831219\n",
      "step 113701, epoch 0 (total steps = 113701): 55.9797260761261\n",
      "1709386127.1870046\n",
      "step 113801, epoch 0 (total steps = 113801): 56.1455135345459\n",
      "1709386127.5328913\n",
      "step 113901, epoch 0 (total steps = 113901): 55.96083855628967\n",
      "1709386127.8890984\n",
      "step 114001, epoch 0 (total steps = 114001): 56.121040761470795\n",
      "1709386128.2413723\n",
      "step 114101, epoch 0 (total steps = 114101): 56.102067947387695\n",
      "1709386128.5993295\n",
      "step 114201, epoch 0 (total steps = 114201): 55.90315568447113\n",
      "1709386128.9371023\n",
      "step 114301, epoch 0 (total steps = 114301): 55.8227373957634\n",
      "1709386129.2823687\n",
      "step 114401, epoch 0 (total steps = 114401): 56.107106029987335\n",
      "1709386129.6636105\n",
      "step 114501, epoch 0 (total steps = 114501): 55.93891650438309\n",
      "1709386130.0259376\n",
      "step 114601, epoch 0 (total steps = 114601): 55.997553646564484\n",
      "1709386130.3611603\n",
      "step 114701, epoch 0 (total steps = 114701): 55.82056140899658\n",
      "1709386130.7011213\n",
      "step 114801, epoch 0 (total steps = 114801): 56.134805262088776\n",
      "1709386131.0453672\n",
      "step 114901, epoch 0 (total steps = 114901): 55.87698358297348\n",
      "1709386131.3840914\n",
      "step 115001, epoch 0 (total steps = 115001): 56.14429634809494\n",
      "1709386131.7444828\n",
      "step 115101, epoch 0 (total steps = 115101): 56.15158134698868\n",
      "1709386132.0919123\n",
      "step 115201, epoch 0 (total steps = 115201): 56.20454502105713\n",
      "1709386132.4357615\n",
      "step 115301, epoch 0 (total steps = 115301): 56.085729122161865\n",
      "1709386132.78753\n",
      "step 115401, epoch 0 (total steps = 115401): 56.00136888027191\n",
      "1709386133.1679013\n",
      "step 115501, epoch 0 (total steps = 115501): 56.33234763145447\n",
      "1709386133.5080602\n",
      "step 115601, epoch 0 (total steps = 115601): 56.0462492108345\n",
      "1709386133.8456287\n",
      "step 115701, epoch 0 (total steps = 115701): 56.26540786027908\n",
      "1709386134.183997\n",
      "step 115801, epoch 0 (total steps = 115801): 55.94031500816345\n",
      "1709386134.5307107\n",
      "step 115901, epoch 0 (total steps = 115901): 56.17046296596527\n",
      "1709386134.9082658\n",
      "step 116001, epoch 0 (total steps = 116001): 56.083090126514435\n",
      "1709386135.2569916\n",
      "step 116101, epoch 0 (total steps = 116101): 56.32262694835663\n",
      "1709386135.6154704\n",
      "step 116201, epoch 0 (total steps = 116201): 55.925047636032104\n",
      "1709386135.9483008\n",
      "step 116301, epoch 0 (total steps = 116301): 56.01023691892624\n",
      "1709386136.2812853\n",
      "step 116401, epoch 0 (total steps = 116401): 56.21720993518829\n",
      "1709386136.6214077\n",
      "step 116501, epoch 0 (total steps = 116501): 56.04734426736832\n",
      "1709386136.9799798\n",
      "step 116601, epoch 0 (total steps = 116601): 55.90817713737488\n",
      "1709386137.3200603\n",
      "step 116701, epoch 0 (total steps = 116701): 55.86146557331085\n",
      "1709386137.6674914\n",
      "step 116801, epoch 0 (total steps = 116801): 55.96943122148514\n",
      "1709386138.0101936\n",
      "step 116901, epoch 0 (total steps = 116901): 55.790893495082855\n",
      "1709386138.359773\n",
      "step 117001, epoch 0 (total steps = 117001): 56.06189024448395\n",
      "1709386138.701754\n",
      "step 117101, epoch 0 (total steps = 117101): 55.94856983423233\n",
      "1709386139.0611217\n",
      "step 117201, epoch 0 (total steps = 117201): 55.77956688404083\n",
      "1709386139.3922002\n",
      "step 117301, epoch 0 (total steps = 117301): 55.88920992612839\n",
      "1709386139.7313178\n",
      "step 117401, epoch 0 (total steps = 117401): 55.86678767204285\n",
      "1709386140.0698004\n",
      "step 117501, epoch 0 (total steps = 117501): 56.05188536643982\n",
      "1709386140.4022472\n",
      "step 117601, epoch 0 (total steps = 117601): 55.55796015262604\n",
      "1709386140.7513676\n",
      "step 117701, epoch 0 (total steps = 117701): 55.87377852201462\n",
      "1709386141.0877814\n",
      "step 117801, epoch 0 (total steps = 117801): 56.00097572803497\n",
      "1709386141.4618409\n",
      "step 117901, epoch 0 (total steps = 117901): 55.826055228710175\n",
      "1709386141.81103\n",
      "step 118001, epoch 0 (total steps = 118001): 55.96359187364578\n",
      "1709386142.1536186\n",
      "step 118101, epoch 0 (total steps = 118101): 55.80161017179489\n",
      "1709386142.496277\n",
      "step 118201, epoch 0 (total steps = 118201): 55.93249309062958\n",
      "1709386142.8380363\n",
      "step 118301, epoch 0 (total steps = 118301): 56.02425503730774\n",
      "1709386143.1791751\n",
      "step 118401, epoch 0 (total steps = 118401): 55.80876988172531\n",
      "1709386143.5296354\n",
      "step 118501, epoch 0 (total steps = 118501): 55.854357838630676\n",
      "1709386143.903829\n",
      "step 118601, epoch 0 (total steps = 118601): 55.895259380340576\n",
      "1709386144.2729676\n",
      "step 118701, epoch 0 (total steps = 118701): 55.94686269760132\n",
      "1709386144.6066263\n",
      "step 118801, epoch 0 (total steps = 118801): 56.07819485664368\n",
      "1709386144.9409142\n",
      "step 118901, epoch 0 (total steps = 118901): 55.85003012418747\n",
      "1709386145.275806\n",
      "step 119001, epoch 0 (total steps = 119001): 56.04293501377106\n",
      "1709386145.628484\n",
      "step 119101, epoch 0 (total steps = 119101): 55.851220428943634\n",
      "1709386145.9712622\n",
      "step 119201, epoch 0 (total steps = 119201): 55.89272344112396\n",
      "1709386146.3411012\n",
      "step 119301, epoch 0 (total steps = 119301): 55.8761123418808\n",
      "1709386146.6896496\n",
      "step 119401, epoch 0 (total steps = 119401): 55.95711362361908\n",
      "1709386147.0264242\n",
      "step 119501, epoch 0 (total steps = 119501): 55.79069834947586\n",
      "1709386147.3636959\n",
      "step 119601, epoch 0 (total steps = 119601): 55.97089922428131\n",
      "1709386147.7226288\n",
      "step 119701, epoch 0 (total steps = 119701): 55.7541099190712\n",
      "1709386148.0743077\n",
      "step 119801, epoch 0 (total steps = 119801): 55.810619592666626\n",
      "1709386148.4112763\n",
      "step 119901, epoch 0 (total steps = 119901): 55.885253846645355\n",
      "1709386148.7575598\n",
      "step 120001, epoch 0 (total steps = 120001): 56.13929617404938\n",
      "1709386149.1015124\n",
      "step 120101, epoch 0 (total steps = 120101): 55.729788422584534\n",
      "1709386149.439614\n",
      "step 120201, epoch 0 (total steps = 120201): 55.87995421886444\n",
      "1709386149.7798333\n",
      "step 120301, epoch 0 (total steps = 120301): 56.09732633829117\n",
      "1709386150.1280017\n",
      "step 120401, epoch 0 (total steps = 120401): 55.82297623157501\n",
      "1709386150.4890466\n",
      "step 120501, epoch 0 (total steps = 120501): 56.12140214443207\n",
      "1709386150.844356\n",
      "step 120601, epoch 0 (total steps = 120601): 56.00497478246689\n",
      "1709386151.1844292\n",
      "step 120701, epoch 0 (total steps = 120701): 55.910051107406616\n",
      "1709386151.5422661\n",
      "step 120801, epoch 0 (total steps = 120801): 56.092138946056366\n",
      "1709386151.9159405\n",
      "step 120901, epoch 0 (total steps = 120901): 55.74968636035919\n",
      "1709386152.2915626\n",
      "step 121001, epoch 0 (total steps = 121001): 56.092012882232666\n",
      "1709386152.6289062\n",
      "step 121101, epoch 0 (total steps = 121101): 55.77864694595337\n",
      "1709386152.9730098\n",
      "step 121201, epoch 0 (total steps = 121201): 55.95352989435196\n",
      "1709386153.317259\n",
      "step 121301, epoch 0 (total steps = 121301): 55.91164273023605\n",
      "1709386153.6810055\n",
      "step 121401, epoch 0 (total steps = 121401): 55.88842386007309\n",
      "1709386154.046958\n",
      "step 121501, epoch 0 (total steps = 121501): 56.11532175540924\n",
      "1709386154.4237535\n",
      "step 121601, epoch 0 (total steps = 121601): 55.937637627124786\n",
      "1709386154.7613676\n",
      "step 121701, epoch 0 (total steps = 121701): 55.84094709157944\n",
      "1709386155.0999897\n",
      "step 121801, epoch 0 (total steps = 121801): 55.851254522800446\n",
      "1709386155.446942\n",
      "step 121901, epoch 0 (total steps = 121901): 55.76965230703354\n",
      "1709386155.7788646\n",
      "step 122001, epoch 0 (total steps = 122001): 55.80233037471771\n",
      "1709386156.1396828\n",
      "step 122101, epoch 0 (total steps = 122101): 56.06989657878876\n",
      "1709386156.4759276\n",
      "step 122201, epoch 0 (total steps = 122201): 55.94964772462845\n",
      "1709386156.81004\n",
      "step 122301, epoch 0 (total steps = 122301): 55.79154509305954\n",
      "1709386157.1439207\n",
      "step 122401, epoch 0 (total steps = 122401): 55.75225454568863\n",
      "1709386157.4954631\n",
      "step 122501, epoch 0 (total steps = 122501): 55.85849267244339\n",
      "1709386157.844865\n",
      "step 122601, epoch 0 (total steps = 122601): 55.83848565816879\n",
      "1709386158.1868715\n",
      "step 122701, epoch 0 (total steps = 122701): 55.669756054878235\n",
      "1709386158.5274987\n",
      "step 122801, epoch 0 (total steps = 122801): 55.8921280503273\n",
      "1709386158.8678153\n",
      "step 122901, epoch 0 (total steps = 122901): 55.858720660209656\n",
      "1709386159.2101405\n",
      "step 123001, epoch 0 (total steps = 123001): 55.87407046556473\n",
      "1709386159.5620048\n",
      "step 123101, epoch 0 (total steps = 123101): 55.8147514462471\n",
      "1709386159.9084451\n",
      "step 123201, epoch 0 (total steps = 123201): 55.7797315120697\n",
      "1709386160.2523277\n",
      "step 123301, epoch 0 (total steps = 123301): 55.96463078260422\n",
      "1709386160.6229594\n",
      "step 123401, epoch 0 (total steps = 123401): 55.70395618677139\n",
      "1709386160.9824636\n",
      "step 123501, epoch 0 (total steps = 123501): 55.69531577825546\n",
      "1709386161.335746\n",
      "step 123601, epoch 0 (total steps = 123601): 55.81666111946106\n",
      "1709386161.6805358\n",
      "step 123701, epoch 0 (total steps = 123701): 55.95352643728256\n",
      "1709386162.02631\n",
      "step 123801, epoch 0 (total steps = 123801): 55.696286618709564\n",
      "1709386162.383922\n",
      "step 123901, epoch 0 (total steps = 123901): 56.00453180074692\n",
      "1709386162.755884\n",
      "step 124001, epoch 0 (total steps = 124001): 55.93079894781113\n",
      "1709386163.101344\n",
      "step 124101, epoch 0 (total steps = 124101): 55.79737430810928\n",
      "1709386163.4364462\n",
      "step 124201, epoch 0 (total steps = 124201): 55.88927096128464\n",
      "1709386163.7846088\n",
      "step 124301, epoch 0 (total steps = 124301): 55.828959703445435\n",
      "1709386164.1541538\n",
      "step 124401, epoch 0 (total steps = 124401): 55.96822088956833\n",
      "1709386164.5109913\n",
      "step 124501, epoch 0 (total steps = 124501): 55.82339656352997\n",
      "1709386164.8505428\n",
      "step 124601, epoch 0 (total steps = 124601): 55.6706805229187\n",
      "1709386165.1888745\n",
      "step 124701, epoch 0 (total steps = 124701): 55.867207407951355\n",
      "1709386165.5187416\n",
      "step 124801, epoch 0 (total steps = 124801): 55.63901245594025\n",
      "1709386165.8515866\n",
      "step 124901, epoch 0 (total steps = 124901): 55.92425811290741\n",
      "1709386166.194181\n",
      "step 125001, epoch 0 (total steps = 125001): 55.929895997047424\n",
      "1709386166.526349\n",
      "step 125101, epoch 0 (total steps = 125101): 55.76721674203873\n",
      "1709386166.8643358\n",
      "step 125201, epoch 0 (total steps = 125201): 56.07221883535385\n",
      "1709386167.1974132\n",
      "step 125301, epoch 0 (total steps = 125301): 55.68069303035736\n",
      "1709386167.5486875\n",
      "step 125401, epoch 0 (total steps = 125401): 55.80206900835037\n",
      "1709386167.9017386\n",
      "step 125501, epoch 0 (total steps = 125501): 55.837125182151794\n",
      "1709386168.2360926\n",
      "step 125601, epoch 0 (total steps = 125601): 55.96445310115814\n",
      "1709386168.599742\n",
      "step 125701, epoch 0 (total steps = 125701): 55.854073882102966\n",
      "1709386168.9694583\n",
      "step 125801, epoch 0 (total steps = 125801): 55.61990141868591\n",
      "1709386169.312003\n",
      "step 125901, epoch 0 (total steps = 125901): 55.913586378097534\n",
      "1709386169.6572857\n",
      "step 126001, epoch 0 (total steps = 126001): 55.68040543794632\n",
      "1709386170.0047429\n",
      "step 126101, epoch 0 (total steps = 126101): 55.657237112522125\n",
      "1709386170.3626728\n",
      "step 126201, epoch 0 (total steps = 126201): 55.66054117679596\n",
      "1709386170.7057629\n",
      "step 126301, epoch 0 (total steps = 126301): 55.72753298282623\n",
      "1709386171.087807\n",
      "step 126401, epoch 0 (total steps = 126401): 56.007843136787415\n",
      "1709386171.4447362\n",
      "step 126501, epoch 0 (total steps = 126501): 55.73678821325302\n",
      "1709386171.7928264\n",
      "step 126601, epoch 0 (total steps = 126601): 55.96510726213455\n",
      "1709386172.1386554\n",
      "step 126701, epoch 0 (total steps = 126701): 56.09137636423111\n",
      "1709386172.4827998\n",
      "step 126801, epoch 0 (total steps = 126801): 55.56820386648178\n",
      "1709386172.8320289\n",
      "step 126901, epoch 0 (total steps = 126901): 55.73585903644562\n",
      "1709386173.1729465\n",
      "step 127001, epoch 0 (total steps = 127001): 55.56389898061752\n",
      "1709386173.522617\n",
      "step 127101, epoch 0 (total steps = 127101): 55.62395620346069\n",
      "1709386173.8597004\n",
      "step 127201, epoch 0 (total steps = 127201): 55.85892140865326\n",
      "1709386174.1939535\n",
      "step 127301, epoch 0 (total steps = 127301): 56.03336602449417\n",
      "1709386174.5288153\n",
      "step 127401, epoch 0 (total steps = 127401): 55.545580208301544\n",
      "1709386174.8845942\n",
      "step 127501, epoch 0 (total steps = 127501): 55.517207860946655\n",
      "1709386175.2252502\n",
      "step 127601, epoch 0 (total steps = 127601): 55.779413759708405\n",
      "1709386175.5746949\n",
      "step 127701, epoch 0 (total steps = 127701): 55.71889370679855\n",
      "1709386175.9354272\n",
      "step 127801, epoch 0 (total steps = 127801): 55.93352335691452\n",
      "1709386176.2785552\n",
      "step 127901, epoch 0 (total steps = 127901): 55.59200119972229\n",
      "1709386176.6636472\n",
      "step 128001, epoch 0 (total steps = 128001): 55.95816606283188\n",
      "1709386177.0044813\n",
      "step 128101, epoch 0 (total steps = 128101): 55.71072292327881\n",
      "1709386177.3891985\n",
      "step 128201, epoch 0 (total steps = 128201): 55.68561893701553\n",
      "1709386177.7312076\n",
      "step 128301, epoch 0 (total steps = 128301): 55.7336460351944\n",
      "1709386178.097791\n",
      "step 128401, epoch 0 (total steps = 128401): 55.64551067352295\n",
      "1709386178.4457998\n",
      "step 128501, epoch 0 (total steps = 128501): 56.02040749788284\n",
      "1709386178.7879267\n",
      "step 128601, epoch 0 (total steps = 128601): 55.91358882188797\n",
      "1709386179.1318362\n",
      "step 128701, epoch 0 (total steps = 128701): 55.6461905837059\n",
      "1709386179.4712026\n",
      "step 128801, epoch 0 (total steps = 128801): 55.65651136636734\n",
      "1709386179.8114805\n",
      "step 128901, epoch 0 (total steps = 128901): 55.63239735364914\n",
      "1709386180.151083\n",
      "step 129001, epoch 0 (total steps = 129001): 55.95554184913635\n",
      "1709386180.52216\n",
      "step 129101, epoch 0 (total steps = 129101): 55.8329735994339\n",
      "1709386180.8848023\n",
      "step 129201, epoch 0 (total steps = 129201): 55.833514511585236\n",
      "1709386181.2249186\n",
      "step 129301, epoch 0 (total steps = 129301): 55.81330096721649\n",
      "1709386181.5770478\n",
      "step 129401, epoch 0 (total steps = 129401): 55.5615257024765\n",
      "1709386181.9211779\n",
      "step 129501, epoch 0 (total steps = 129501): 55.681591868400574\n",
      "1709386182.2618003\n",
      "step 129601, epoch 0 (total steps = 129601): 55.8367024064064\n",
      "1709386182.6159985\n",
      "step 129701, epoch 0 (total steps = 129701): 55.709456384181976\n",
      "1709386182.958861\n",
      "step 129801, epoch 0 (total steps = 129801): 55.92900574207306\n",
      "1709386183.3063593\n",
      "step 129901, epoch 0 (total steps = 129901): 55.66061806678772\n",
      "1709386183.646989\n",
      "step 130001, epoch 0 (total steps = 130001): 55.73606127500534\n",
      "1709386183.993243\n",
      "step 130101, epoch 0 (total steps = 130101): 55.84050506353378\n",
      "1709386184.3373063\n",
      "step 130201, epoch 0 (total steps = 130201): 55.573892176151276\n",
      "1709386184.6786313\n",
      "step 130301, epoch 0 (total steps = 130301): 55.840702414512634\n",
      "1709386185.0254142\n",
      "step 130401, epoch 0 (total steps = 130401): 55.7563641667366\n",
      "1709386185.3731234\n",
      "step 130501, epoch 0 (total steps = 130501): 55.759745717048645\n",
      "1709386185.7255604\n",
      "step 130601, epoch 0 (total steps = 130601): 55.386178970336914\n",
      "1709386186.0654662\n",
      "step 130701, epoch 0 (total steps = 130701): 55.54654014110565\n",
      "1709386186.411979\n",
      "step 130801, epoch 0 (total steps = 130801): 55.47958105802536\n",
      "1709386186.8038769\n",
      "step 130901, epoch 0 (total steps = 130901): 55.631667256355286\n",
      "1709386187.149045\n",
      "step 131001, epoch 0 (total steps = 131001): 55.67774552106857\n",
      "1709386187.49517\n",
      "step 131101, epoch 0 (total steps = 131101): 55.476792097091675\n",
      "1709386187.8337271\n",
      "step 131201, epoch 0 (total steps = 131201): 55.65465772151947\n",
      "1709386188.170703\n",
      "step 131301, epoch 0 (total steps = 131301): 55.63341122865677\n",
      "1709386188.52378\n",
      "step 131401, epoch 0 (total steps = 131401): 55.785938799381256\n",
      "1709386188.8702617\n",
      "step 131501, epoch 0 (total steps = 131501): 55.90857255458832\n",
      "1709386189.2099686\n",
      "step 131601, epoch 0 (total steps = 131601): 55.526746690273285\n",
      "1709386189.5541058\n",
      "step 131701, epoch 0 (total steps = 131701): 55.58729529380798\n",
      "1709386189.9275856\n",
      "step 131801, epoch 0 (total steps = 131801): 55.58300846815109\n",
      "1709386190.2725837\n",
      "step 131901, epoch 0 (total steps = 131901): 55.96399164199829\n",
      "1709386190.625213\n",
      "step 132001, epoch 0 (total steps = 132001): 55.649480640888214\n",
      "1709386190.9677885\n",
      "step 132101, epoch 0 (total steps = 132101): 55.567117393016815\n",
      "1709386191.3272855\n",
      "step 132201, epoch 0 (total steps = 132201): 55.45989382266998\n",
      "1709386191.6679451\n",
      "step 132301, epoch 0 (total steps = 132301): 55.60734587907791\n",
      "1709386192.0098937\n",
      "step 132401, epoch 0 (total steps = 132401): 55.5086108148098\n",
      "1709386192.3492343\n",
      "step 132501, epoch 0 (total steps = 132501): 55.97630721330643\n",
      "1709386192.7071111\n",
      "step 132601, epoch 0 (total steps = 132601): 55.58261024951935\n",
      "1709386193.0639715\n",
      "step 132701, epoch 0 (total steps = 132701): 55.688706278800964\n",
      "1709386193.4084635\n",
      "step 132801, epoch 0 (total steps = 132801): 55.56875962018967\n",
      "1709386193.7480662\n",
      "step 132901, epoch 0 (total steps = 132901): 55.674331307411194\n",
      "1709386194.0907106\n",
      "step 133001, epoch 0 (total steps = 133001): 55.723878383636475\n",
      "1709386194.430262\n",
      "step 133101, epoch 0 (total steps = 133101): 55.6854122877121\n",
      "1709386194.7740924\n",
      "step 133201, epoch 0 (total steps = 133201): 55.63822644948959\n",
      "1709386195.120943\n",
      "step 133301, epoch 0 (total steps = 133301): 55.947277665138245\n",
      "1709386195.4700003\n",
      "step 133401, epoch 0 (total steps = 133401): 55.70677149295807\n",
      "1709386195.8075752\n",
      "step 133501, epoch 0 (total steps = 133501): 55.776641845703125\n",
      "1709386196.1530337\n",
      "step 133601, epoch 0 (total steps = 133601): 55.49554920196533\n",
      "1709386196.4901748\n",
      "step 133701, epoch 0 (total steps = 133701): 55.50510102510452\n",
      "1709386196.8340888\n",
      "step 133801, epoch 0 (total steps = 133801): 55.464402079582214\n",
      "1709386197.2045832\n",
      "step 133901, epoch 0 (total steps = 133901): 55.70987659692764\n",
      "1709386197.5449402\n",
      "step 134001, epoch 0 (total steps = 134001): 55.622084975242615\n",
      "1709386197.9103315\n",
      "step 134101, epoch 0 (total steps = 134101): 55.51328432559967\n",
      "1709386198.2500114\n",
      "step 134201, epoch 0 (total steps = 134201): 55.720412492752075\n",
      "1709386198.579985\n",
      "step 134301, epoch 0 (total steps = 134301): 55.47264504432678\n",
      "1709386198.9206755\n",
      "step 134401, epoch 0 (total steps = 134401): 55.50108176469803\n",
      "1709386199.272007\n",
      "step 134501, epoch 0 (total steps = 134501): 55.82409828901291\n",
      "1709386199.6078858\n",
      "step 134601, epoch 0 (total steps = 134601): 55.45213830471039\n",
      "1709386199.9488714\n",
      "step 134701, epoch 0 (total steps = 134701): 55.72708332538605\n",
      "1709386200.294015\n",
      "step 134801, epoch 0 (total steps = 134801): 55.57256376743317\n",
      "1709386200.6245508\n",
      "step 134901, epoch 0 (total steps = 134901): 55.773089826107025\n",
      "1709386200.973001\n",
      "step 135001, epoch 0 (total steps = 135001): 55.53297662734985\n",
      "1709386201.304474\n",
      "step 135101, epoch 0 (total steps = 135101): 55.61305224895477\n",
      "1709386201.6507037\n",
      "step 135201, epoch 0 (total steps = 135201): 55.9143391251564\n",
      "1709386202.0379076\n",
      "step 135301, epoch 0 (total steps = 135301): 55.71991765499115\n",
      "1709386202.3864307\n",
      "step 135401, epoch 0 (total steps = 135401): 55.54107540845871\n",
      "1709386202.7297935\n",
      "step 135501, epoch 0 (total steps = 135501): 55.60956597328186\n",
      "1709386203.0743206\n",
      "step 135601, epoch 0 (total steps = 135601): 55.66395062208176\n",
      "1709386203.4507418\n",
      "step 135701, epoch 0 (total steps = 135701): 55.662702560424805\n",
      "1709386203.786032\n",
      "step 135801, epoch 0 (total steps = 135801): 55.50060933828354\n",
      "1709386204.159693\n",
      "step 135901, epoch 0 (total steps = 135901): 55.643417060375214\n",
      "1709386204.4932215\n",
      "step 136001, epoch 0 (total steps = 136001): 55.89594155550003\n",
      "1709386204.8305635\n",
      "step 136101, epoch 0 (total steps = 136101): 55.552272498607635\n",
      "1709386205.1694694\n",
      "step 136201, epoch 0 (total steps = 136201): 55.608718514442444\n",
      "1709386205.5367818\n",
      "step 136301, epoch 0 (total steps = 136301): 55.6436984539032\n",
      "1709386205.8777242\n",
      "step 136401, epoch 0 (total steps = 136401): 55.730320274829865\n",
      "1709386206.218627\n",
      "step 136501, epoch 0 (total steps = 136501): 55.705171287059784\n",
      "1709386206.5646741\n",
      "step 136601, epoch 0 (total steps = 136601): 55.63653999567032\n",
      "1709386206.9307168\n",
      "step 136701, epoch 0 (total steps = 136701): 55.81414556503296\n",
      "1709386207.2784517\n",
      "step 136801, epoch 0 (total steps = 136801): 55.40798133611679\n",
      "1709386207.6303227\n",
      "step 136901, epoch 0 (total steps = 136901): 55.5875688791275\n",
      "1709386207.9620478\n",
      "step 137001, epoch 0 (total steps = 137001): 55.63655537366867\n",
      "1709386208.331154\n",
      "step 137101, epoch 0 (total steps = 137101): 55.61595267057419\n",
      "1709386208.6753983\n",
      "step 137201, epoch 0 (total steps = 137201): 55.61301666498184\n",
      "1709386209.0089505\n",
      "step 137301, epoch 0 (total steps = 137301): 55.63230460882187\n",
      "1709386209.3508465\n",
      "step 137401, epoch 0 (total steps = 137401): 55.59687566757202\n",
      "1709386209.7201345\n",
      "step 137501, epoch 0 (total steps = 137501): 55.41400343179703\n",
      "1709386210.091326\n",
      "step 137601, epoch 0 (total steps = 137601): 55.77529811859131\n",
      "1709386210.4598894\n",
      "step 137701, epoch 0 (total steps = 137701): 55.52213829755783\n",
      "1709386210.8038125\n",
      "step 137801, epoch 0 (total steps = 137801): 55.35375088453293\n",
      "1709386211.1433794\n",
      "step 137901, epoch 0 (total steps = 137901): 55.47335696220398\n",
      "1709386211.48787\n",
      "step 138001, epoch 0 (total steps = 138001): 55.75152438879013\n",
      "1709386211.8527713\n",
      "step 138101, epoch 0 (total steps = 138101): 55.64747554063797\n",
      "1709386212.1977692\n",
      "step 138201, epoch 0 (total steps = 138201): 55.44242089986801\n",
      "1709386212.5434961\n",
      "step 138301, epoch 0 (total steps = 138301): 55.55526286363602\n",
      "1709386212.8989937\n",
      "step 138401, epoch 0 (total steps = 138401): 55.39454001188278\n",
      "1709386213.2616346\n",
      "step 138501, epoch 0 (total steps = 138501): 55.49399662017822\n",
      "1709386213.6073797\n",
      "step 138601, epoch 0 (total steps = 138601): 55.71495985984802\n",
      "1709386213.9421082\n",
      "step 138701, epoch 0 (total steps = 138701): 55.67425614595413\n",
      "1709386214.291878\n",
      "step 138801, epoch 0 (total steps = 138801): 55.55479156970978\n",
      "1709386214.6257923\n",
      "step 138901, epoch 0 (total steps = 138901): 55.54202800989151\n",
      "1709386215.0025613\n",
      "step 139001, epoch 0 (total steps = 139001): 55.733429968357086\n",
      "1709386215.393374\n",
      "step 139101, epoch 0 (total steps = 139101): 55.416650116443634\n",
      "1709386215.7420354\n",
      "step 139201, epoch 0 (total steps = 139201): 55.44399690628052\n",
      "1709386216.0984774\n",
      "step 139301, epoch 0 (total steps = 139301): 55.600697338581085\n",
      "1709386216.480649\n",
      "step 139401, epoch 0 (total steps = 139401): 55.58917325735092\n",
      "1709386216.8577108\n",
      "step 139501, epoch 0 (total steps = 139501): 55.294853925704956\n",
      "1709386217.2053921\n",
      "step 139601, epoch 0 (total steps = 139601): 55.49918621778488\n",
      "1709386217.5873175\n",
      "step 139701, epoch 0 (total steps = 139701): 55.37795281410217\n",
      "1709386217.9729378\n",
      "step 139801, epoch 0 (total steps = 139801): 55.44481313228607\n",
      "1709386218.3178835\n",
      "step 139901, epoch 0 (total steps = 139901): 55.536646246910095\n",
      "1709386218.6669066\n",
      "step 140001, epoch 0 (total steps = 140001): 55.60620152950287\n",
      "1709386219.017235\n",
      "step 140101, epoch 0 (total steps = 140101): 55.73756229877472\n",
      "1709386219.3911898\n",
      "step 140201, epoch 0 (total steps = 140201): 55.435604989528656\n",
      "1709386219.7406876\n",
      "step 140301, epoch 0 (total steps = 140301): 55.61645418405533\n",
      "1709386220.0882251\n",
      "step 140401, epoch 0 (total steps = 140401): 55.75426745414734\n",
      "1709386220.4315925\n",
      "step 140501, epoch 0 (total steps = 140501): 55.506222784519196\n",
      "1709386220.782055\n",
      "step 140601, epoch 0 (total steps = 140601): 55.8007590174675\n",
      "1709386221.1399772\n",
      "step 140701, epoch 0 (total steps = 140701): 55.583243906497955\n",
      "1709386221.4922683\n",
      "step 140801, epoch 0 (total steps = 140801): 55.40775525569916\n",
      "1709386221.8406613\n",
      "step 140901, epoch 0 (total steps = 140901): 55.636336863040924\n",
      "1709386222.177209\n",
      "step 141001, epoch 0 (total steps = 141001): 55.76961570978165\n",
      "1709386222.5268168\n",
      "step 141101, epoch 0 (total steps = 141101): 55.32231950759888\n",
      "1709386222.8784108\n",
      "step 141201, epoch 0 (total steps = 141201): 55.413562059402466\n",
      "1709386223.2466156\n",
      "step 141301, epoch 0 (total steps = 141301): 55.4524250626564\n",
      "1709386223.5755367\n",
      "step 141401, epoch 0 (total steps = 141401): 55.448988139629364\n",
      "1709386223.9057474\n",
      "step 141501, epoch 0 (total steps = 141501): 55.70261490345001\n",
      "1709386224.235994\n",
      "step 141601, epoch 0 (total steps = 141601): 55.44714081287384\n",
      "1709386224.5903542\n",
      "step 141701, epoch 0 (total steps = 141701): 55.58790731430054\n",
      "1709386224.9306\n",
      "step 141801, epoch 0 (total steps = 141801): 55.65994083881378\n",
      "1709386225.2850103\n",
      "step 141901, epoch 0 (total steps = 141901): 55.69453543424606\n",
      "1709386225.621573\n",
      "step 142001, epoch 0 (total steps = 142001): 55.38333034515381\n",
      "1709386225.9611437\n",
      "step 142101, epoch 0 (total steps = 142101): 55.6580553650856\n",
      "1709386226.315425\n",
      "step 142201, epoch 0 (total steps = 142201): 55.483751237392426\n",
      "1709386226.64064\n",
      "step 142301, epoch 0 (total steps = 142301): 55.46757435798645\n",
      "1709386226.9849472\n",
      "step 142401, epoch 0 (total steps = 142401): 55.82631278038025\n",
      "1709386227.3366635\n",
      "step 142501, epoch 0 (total steps = 142501): 55.66197192668915\n",
      "1709386227.7042909\n",
      "step 142601, epoch 0 (total steps = 142601): 55.4297257065773\n",
      "1709386228.0385187\n",
      "step 142701, epoch 0 (total steps = 142701): 55.53798818588257\n",
      "1709386228.3799238\n",
      "step 142801, epoch 0 (total steps = 142801): 55.58631592988968\n",
      "1709386228.777859\n",
      "step 142901, epoch 0 (total steps = 142901): 55.48530548810959\n",
      "1709386229.1497304\n",
      "step 143001, epoch 0 (total steps = 143001): 55.54670304059982\n",
      "1709386229.5208514\n",
      "step 143101, epoch 0 (total steps = 143101): 55.463035345077515\n",
      "1709386229.8836327\n",
      "step 143201, epoch 0 (total steps = 143201): 55.52360391616821\n",
      "1709386230.2530448\n",
      "step 143301, epoch 0 (total steps = 143301): 55.536680996418\n",
      "1709386230.6283565\n",
      "step 143401, epoch 0 (total steps = 143401): 55.506606578826904\n",
      "1709386231.003752\n",
      "step 143501, epoch 0 (total steps = 143501): 55.52616888284683\n",
      "1709386231.3787\n",
      "step 143601, epoch 0 (total steps = 143601): 55.774680852890015\n",
      "1709386231.7487133\n",
      "step 143701, epoch 0 (total steps = 143701): 55.48617333173752\n",
      "1709386232.114433\n",
      "step 143801, epoch 0 (total steps = 143801): 55.43633383512497\n",
      "1709386232.491648\n",
      "step 143901, epoch 0 (total steps = 143901): 55.62101823091507\n",
      "1709386232.8565025\n",
      "step 144001, epoch 0 (total steps = 144001): 55.524569511413574\n",
      "1709386233.2303143\n",
      "step 144101, epoch 0 (total steps = 144101): 55.51683294773102\n",
      "1709386233.5997665\n",
      "step 144201, epoch 0 (total steps = 144201): 55.65668565034866\n",
      "1709386233.979854\n",
      "step 144301, epoch 0 (total steps = 144301): 55.04006880521774\n",
      "1709386234.3487566\n",
      "step 144401, epoch 0 (total steps = 144401): 55.178887248039246\n",
      "1709386234.7074196\n",
      "step 144501, epoch 0 (total steps = 144501): 55.46090346574783\n",
      "1709386235.087086\n",
      "step 144601, epoch 0 (total steps = 144601): 55.63015687465668\n",
      "1709386235.4981976\n",
      "step 144701, epoch 0 (total steps = 144701): 55.619421660900116\n",
      "1709386235.8935926\n",
      "step 144801, epoch 0 (total steps = 144801): 55.571348786354065\n",
      "1709386236.2630837\n",
      "step 144901, epoch 0 (total steps = 144901): 55.26937413215637\n",
      "1709386236.6580515\n",
      "step 145001, epoch 0 (total steps = 145001): 55.724613904953\n",
      "1709386237.032065\n",
      "step 145101, epoch 0 (total steps = 145101): 55.694828033447266\n",
      "1709386237.3989847\n",
      "step 145201, epoch 0 (total steps = 145201): 55.466868817806244\n",
      "1709386237.7667553\n",
      "step 145301, epoch 0 (total steps = 145301): 55.15237373113632\n",
      "1709386238.1483133\n",
      "step 145401, epoch 0 (total steps = 145401): 55.61922562122345\n",
      "1709386238.5288284\n",
      "step 145501, epoch 0 (total steps = 145501): 55.343106746673584\n",
      "1709386238.901803\n",
      "step 145601, epoch 0 (total steps = 145601): 55.58048927783966\n",
      "1709386239.2686\n",
      "step 145701, epoch 0 (total steps = 145701): 55.12739360332489\n",
      "1709386239.638659\n",
      "step 145801, epoch 0 (total steps = 145801): 55.677280247211456\n",
      "1709386240.0483925\n",
      "step 145901, epoch 0 (total steps = 145901): 55.112269937992096\n",
      "1709386240.4199882\n",
      "step 146001, epoch 0 (total steps = 146001): 55.342906296253204\n",
      "1709386240.8080692\n",
      "step 146101, epoch 0 (total steps = 146101): 55.58392310142517\n",
      "1709386241.1927683\n",
      "step 146201, epoch 0 (total steps = 146201): 55.25284218788147\n",
      "1709386241.575452\n",
      "step 146301, epoch 0 (total steps = 146301): 55.187245190143585\n",
      "1709386241.9553072\n",
      "step 146401, epoch 0 (total steps = 146401): 55.41924113035202\n",
      "1709386242.3060553\n",
      "step 146501, epoch 0 (total steps = 146501): 55.69670009613037\n",
      "1709386242.656314\n",
      "step 146601, epoch 0 (total steps = 146601): 55.41050165891647\n",
      "1709386243.05653\n",
      "step 146701, epoch 0 (total steps = 146701): 55.458896696567535\n",
      "1709386243.4184656\n",
      "step 146801, epoch 0 (total steps = 146801): 55.416244328022\n",
      "1709386243.7690392\n",
      "step 146901, epoch 0 (total steps = 146901): 55.397576332092285\n",
      "1709386244.1168087\n",
      "step 147001, epoch 0 (total steps = 147001): 55.33496469259262\n",
      "1709386244.494584\n",
      "step 147101, epoch 0 (total steps = 147101): 55.18923956155777\n",
      "1709386244.8793426\n",
      "step 147201, epoch 0 (total steps = 147201): 55.02188193798065\n",
      "1709386245.225007\n",
      "step 147301, epoch 0 (total steps = 147301): 55.21108144521713\n",
      "1709386245.5778651\n",
      "step 147401, epoch 0 (total steps = 147401): 55.41368532180786\n",
      "1709386245.9163432\n",
      "step 147501, epoch 0 (total steps = 147501): 55.437036216259\n",
      "1709386246.2658432\n",
      "step 147601, epoch 0 (total steps = 147601): 55.478169679641724\n",
      "1709386246.6032062\n",
      "step 147701, epoch 0 (total steps = 147701): 55.56158512830734\n",
      "1709386246.9805598\n",
      "step 147801, epoch 0 (total steps = 147801): 55.4537034034729\n",
      "1709386247.322428\n",
      "step 147901, epoch 0 (total steps = 147901): 55.60923397541046\n",
      "1709386247.6611996\n",
      "step 148001, epoch 0 (total steps = 148001): 55.43993479013443\n",
      "1709386248.0133417\n",
      "step 148101, epoch 0 (total steps = 148101): 55.71188086271286\n",
      "1709386248.3521411\n",
      "step 148201, epoch 0 (total steps = 148201): 55.07920724153519\n",
      "1709386248.7073505\n",
      "step 148301, epoch 0 (total steps = 148301): 55.54355627298355\n",
      "1709386249.0511134\n",
      "step 148401, epoch 0 (total steps = 148401): 55.60947400331497\n",
      "1709386249.3932617\n",
      "step 148501, epoch 0 (total steps = 148501): 55.46952927112579\n",
      "1709386249.7506406\n",
      "step 148601, epoch 0 (total steps = 148601): 55.380759835243225\n",
      "1709386250.0835934\n",
      "step 148701, epoch 0 (total steps = 148701): 55.3921520113945\n",
      "1709386250.4181285\n",
      "step 148801, epoch 0 (total steps = 148801): 54.94848847389221\n",
      "1709386250.7704592\n",
      "step 148901, epoch 0 (total steps = 148901): 55.34568041563034\n",
      "1709386251.1290135\n",
      "step 149001, epoch 0 (total steps = 149001): 55.41379249095917\n",
      "1709386251.476091\n",
      "step 149101, epoch 0 (total steps = 149101): 55.59109401702881\n",
      "1709386251.8368194\n",
      "step 149201, epoch 0 (total steps = 149201): 55.45395189523697\n",
      "1709386252.2159295\n",
      "step 149301, epoch 0 (total steps = 149301): 55.29694712162018\n",
      "1709386252.5925393\n",
      "step 149401, epoch 0 (total steps = 149401): 55.24285423755646\n",
      "1709386252.9349647\n",
      "step 149501, epoch 0 (total steps = 149501): 55.391133308410645\n",
      "1709386253.276084\n",
      "step 149601, epoch 0 (total steps = 149601): 55.33810156583786\n",
      "1709386253.6251652\n",
      "step 149701, epoch 0 (total steps = 149701): 55.40224611759186\n",
      "1709386253.9660933\n",
      "step 149801, epoch 0 (total steps = 149801): 55.32464689016342\n",
      "1709386254.3004463\n",
      "step 149901, epoch 0 (total steps = 149901): 55.44177186489105\n",
      "1709386254.6347134\n",
      "step 150001, epoch 0 (total steps = 150001): 55.3647301197052\n",
      "1709386254.9816563\n",
      "step 150101, epoch 0 (total steps = 150101): 55.35699111223221\n",
      "1709386255.347747\n",
      "step 150201, epoch 0 (total steps = 150201): 55.134830832481384\n",
      "1709386255.6895232\n",
      "step 150301, epoch 0 (total steps = 150301): 55.49651104211807\n",
      "1709386256.0287538\n",
      "step 150401, epoch 0 (total steps = 150401): 55.32301580905914\n",
      "1709386256.3689575\n",
      "step 150501, epoch 0 (total steps = 150501): 55.40853977203369\n",
      "1709386256.720326\n",
      "step 150601, epoch 0 (total steps = 150601): 55.256277680397034\n",
      "1709386257.0544035\n",
      "step 150701, epoch 0 (total steps = 150701): 55.16540837287903\n",
      "1709386257.4047308\n",
      "step 150801, epoch 0 (total steps = 150801): 55.1563925743103\n",
      "1709386257.7464101\n",
      "step 150901, epoch 0 (total steps = 150901): 55.39155352115631\n",
      "1709386258.0836575\n",
      "step 151001, epoch 0 (total steps = 151001): 55.41375821828842\n",
      "1709386258.4198334\n",
      "step 151101, epoch 0 (total steps = 151101): 55.45385318994522\n",
      "1709386258.7630036\n",
      "step 151201, epoch 0 (total steps = 151201): 55.28549838066101\n",
      "1709386259.104684\n",
      "step 151301, epoch 0 (total steps = 151301): 55.4864906668663\n",
      "1709386259.4395597\n",
      "step 151401, epoch 0 (total steps = 151401): 55.25549536943436\n",
      "1709386259.801357\n",
      "step 151501, epoch 0 (total steps = 151501): 55.46157521009445\n",
      "1709386260.1578214\n",
      "step 151601, epoch 0 (total steps = 151601): 55.328028321266174\n",
      "1709386260.4947946\n",
      "step 151701, epoch 0 (total steps = 151701): 55.30527275800705\n",
      "1709386260.8355258\n",
      "step 151801, epoch 0 (total steps = 151801): 55.1938938498497\n",
      "1709386261.1703198\n",
      "step 151901, epoch 0 (total steps = 151901): 55.437886118888855\n",
      "1709386261.516684\n",
      "step 152001, epoch 0 (total steps = 152001): 54.98374962806702\n",
      "1709386261.87015\n",
      "step 152101, epoch 0 (total steps = 152101): 55.24507677555084\n",
      "1709386262.211636\n",
      "step 152201, epoch 0 (total steps = 152201): 55.182249426841736\n",
      "1709386262.5980628\n",
      "step 152301, epoch 0 (total steps = 152301): 55.16190189123154\n",
      "1709386262.9686906\n",
      "step 152401, epoch 0 (total steps = 152401): 55.1390575170517\n",
      "1709386263.319446\n",
      "step 152501, epoch 0 (total steps = 152501): 55.33679932355881\n",
      "1709386263.665666\n",
      "step 152601, epoch 0 (total steps = 152601): 55.36256843805313\n",
      "1709386264.0279434\n",
      "step 152701, epoch 0 (total steps = 152701): 55.240100145339966\n",
      "1709386264.3793447\n",
      "step 152801, epoch 0 (total steps = 152801): 55.34357434511185\n",
      "1709386264.7339032\n",
      "step 152901, epoch 0 (total steps = 152901): 55.269470512866974\n",
      "1709386265.0952318\n",
      "step 153001, epoch 0 (total steps = 153001): 55.27989137172699\n",
      "1709386265.4360125\n",
      "step 153101, epoch 0 (total steps = 153101): 55.43065816164017\n",
      "1709386265.7900672\n",
      "step 153201, epoch 0 (total steps = 153201): 55.56495004892349\n",
      "1709386266.1333668\n",
      "step 153301, epoch 0 (total steps = 153301): 55.41592979431152\n",
      "1709386266.4895272\n",
      "step 153401, epoch 0 (total steps = 153401): 55.23106622695923\n",
      "1709386266.8271086\n",
      "step 153501, epoch 0 (total steps = 153501): 55.25271499156952\n",
      "1709386267.1707296\n",
      "step 153601, epoch 0 (total steps = 153601): 55.33699768781662\n",
      "1709386267.5410614\n",
      "step 153701, epoch 0 (total steps = 153701): 55.40585869550705\n",
      "1709386267.891698\n",
      "step 153801, epoch 0 (total steps = 153801): 55.46105867624283\n",
      "1709386268.2281537\n",
      "step 153901, epoch 0 (total steps = 153901): 55.07707858085632\n",
      "1709386268.577732\n",
      "step 154001, epoch 0 (total steps = 154001): 54.97502690553665\n",
      "1709386268.938973\n",
      "step 154101, epoch 0 (total steps = 154101): 55.27111750841141\n",
      "1709386269.2763338\n",
      "step 154201, epoch 0 (total steps = 154201): 55.42061936855316\n",
      "1709386269.6235344\n",
      "step 154301, epoch 0 (total steps = 154301): 55.38290709257126\n",
      "1709386269.9606426\n",
      "step 154401, epoch 0 (total steps = 154401): 55.31590151786804\n",
      "1709386270.3264582\n",
      "step 154501, epoch 0 (total steps = 154501): 55.23721879720688\n",
      "1709386270.6707044\n",
      "step 154601, epoch 0 (total steps = 154601): 55.267724215984344\n",
      "1709386271.0204525\n",
      "step 154701, epoch 0 (total steps = 154701): 55.21897631883621\n",
      "1709386271.3650107\n",
      "step 154801, epoch 0 (total steps = 154801): 55.506530821323395\n",
      "1709386271.7000048\n",
      "step 154901, epoch 0 (total steps = 154901): 55.21167033910751\n",
      "1709386272.0479958\n",
      "step 155001, epoch 0 (total steps = 155001): 55.189311504364014\n",
      "1709386272.4003656\n",
      "step 155101, epoch 0 (total steps = 155101): 55.26099890470505\n",
      "1709386272.748192\n",
      "step 155201, epoch 0 (total steps = 155201): 55.31785190105438\n",
      "1709386273.0927417\n",
      "step 155301, epoch 0 (total steps = 155301): 55.077952682971954\n",
      "1709386273.4609175\n",
      "step 155401, epoch 0 (total steps = 155401): 55.30023628473282\n",
      "1709386273.79852\n",
      "step 155501, epoch 0 (total steps = 155501): 55.37112307548523\n",
      "1709386274.1368833\n",
      "step 155601, epoch 0 (total steps = 155601): 55.45375740528107\n",
      "1709386274.4856198\n",
      "step 155701, epoch 0 (total steps = 155701): 55.30201715230942\n",
      "1709386274.8230417\n",
      "step 155801, epoch 0 (total steps = 155801): 55.26890671253204\n",
      "1709386275.1749232\n",
      "step 155901, epoch 0 (total steps = 155901): 55.37794989347458\n",
      "1709386275.5273395\n",
      "step 156001, epoch 0 (total steps = 156001): 55.39731568098068\n",
      "1709386275.8790152\n",
      "step 156101, epoch 0 (total steps = 156101): 55.39337754249573\n",
      "1709386276.2176993\n",
      "step 156201, epoch 0 (total steps = 156201): 55.07754182815552\n",
      "1709386276.562657\n",
      "step 156301, epoch 0 (total steps = 156301): 55.44450658559799\n",
      "1709386276.9390996\n",
      "step 156401, epoch 0 (total steps = 156401): 55.30465525388718\n",
      "1709386277.305903\n",
      "step 156501, epoch 0 (total steps = 156501): 55.49094617366791\n",
      "1709386277.641969\n",
      "step 156601, epoch 0 (total steps = 156601): 55.39199298620224\n",
      "1709386277.9834976\n",
      "step 156701, epoch 0 (total steps = 156701): 55.197945952415466\n",
      "1709386278.3269014\n",
      "step 156801, epoch 0 (total steps = 156801): 55.24091064929962\n",
      "1709386278.676185\n",
      "step 156901, epoch 0 (total steps = 156901): 55.17388969659805\n",
      "1709386279.0138402\n",
      "step 157001, epoch 0 (total steps = 157001): 55.30147314071655\n",
      "1709386279.360981\n",
      "step 157101, epoch 0 (total steps = 157101): 55.23538517951965\n",
      "1709386279.6968396\n",
      "step 157201, epoch 0 (total steps = 157201): 55.440048933029175\n",
      "1709386280.034716\n",
      "step 157301, epoch 0 (total steps = 157301): 55.34918123483658\n",
      "1709386280.393272\n",
      "step 157401, epoch 0 (total steps = 157401): 55.444910526275635\n",
      "1709386280.757844\n",
      "step 157501, epoch 0 (total steps = 157501): 55.11443895101547\n",
      "1709386281.108581\n",
      "step 157601, epoch 0 (total steps = 157601): 55.19922584295273\n",
      "1709386281.4832983\n",
      "step 157701, epoch 0 (total steps = 157701): 55.25928694009781\n",
      "1709386281.8585865\n",
      "step 157801, epoch 0 (total steps = 157801): 54.992107689380646\n",
      "1709386282.2202718\n",
      "step 157901, epoch 0 (total steps = 157901): 55.155087769031525\n",
      "1709386282.5632083\n",
      "step 158001, epoch 0 (total steps = 158001): 55.25781410932541\n",
      "1709386282.915346\n",
      "step 158101, epoch 0 (total steps = 158101): 55.174985229969025\n",
      "1709386283.2767816\n",
      "step 158201, epoch 0 (total steps = 158201): 55.28274244070053\n",
      "1709386283.6357362\n",
      "step 158301, epoch 0 (total steps = 158301): 55.056144177913666\n",
      "1709386283.9998229\n",
      "step 158401, epoch 0 (total steps = 158401): 55.09398239850998\n",
      "1709386284.3369136\n",
      "step 158501, epoch 0 (total steps = 158501): 55.21655732393265\n",
      "1709386284.6718495\n",
      "step 158601, epoch 0 (total steps = 158601): 55.01033407449722\n",
      "1709386285.008184\n",
      "step 158701, epoch 0 (total steps = 158701): 55.08573693037033\n",
      "1709386285.3451312\n",
      "step 158801, epoch 0 (total steps = 158801): 55.13513535261154\n",
      "1709386285.7097723\n",
      "step 158901, epoch 0 (total steps = 158901): 54.982942283153534\n",
      "1709386286.1076317\n",
      "step 159001, epoch 0 (total steps = 159001): 55.122759997844696\n",
      "1709386286.517972\n",
      "step 159101, epoch 0 (total steps = 159101): 55.27463686466217\n",
      "1709386286.9122193\n",
      "step 159201, epoch 0 (total steps = 159201): 55.28373181819916\n",
      "1709386287.3019946\n",
      "step 159301, epoch 0 (total steps = 159301): 55.11758077144623\n",
      "1709386287.6772683\n",
      "step 159401, epoch 0 (total steps = 159401): 55.1760316491127\n",
      "1709386288.0622125\n",
      "step 159501, epoch 0 (total steps = 159501): 55.275600135326385\n",
      "1709386288.4239147\n",
      "step 159601, epoch 0 (total steps = 159601): 54.84449565410614\n",
      "1709386288.8156242\n",
      "step 159701, epoch 0 (total steps = 159701): 55.18910175561905\n",
      "1709386289.1776636\n",
      "step 159801, epoch 0 (total steps = 159801): 55.19098973274231\n",
      "1709386289.585003\n",
      "step 159901, epoch 0 (total steps = 159901): 55.16365736722946\n",
      "1709386289.951416\n",
      "step 160001, epoch 0 (total steps = 160001): 55.113352596759796\n",
      "1709386290.325961\n",
      "step 160101, epoch 0 (total steps = 160101): 55.23177695274353\n",
      "1709386290.703958\n",
      "step 160201, epoch 0 (total steps = 160201): 55.37985575199127\n",
      "1709386291.06415\n",
      "step 160301, epoch 0 (total steps = 160301): 55.18611931800842\n",
      "1709386291.4246023\n",
      "step 160401, epoch 0 (total steps = 160401): 55.17718917131424\n",
      "1709386291.786838\n",
      "step 160501, epoch 0 (total steps = 160501): 55.33729499578476\n",
      "1709386292.1692147\n",
      "step 160601, epoch 0 (total steps = 160601): 55.181669533252716\n",
      "1709386292.53243\n",
      "step 160701, epoch 0 (total steps = 160701): 55.17892760038376\n",
      "1709386292.8898609\n",
      "step 160801, epoch 0 (total steps = 160801): 55.148584485054016\n",
      "1709386293.2396529\n",
      "step 160901, epoch 0 (total steps = 160901): 55.12641942501068\n",
      "1709386293.5798523\n",
      "step 161001, epoch 0 (total steps = 161001): 55.20189017057419\n",
      "1709386293.9387817\n",
      "step 161101, epoch 0 (total steps = 161101): 55.19977116584778\n",
      "1709386294.284095\n",
      "step 161201, epoch 0 (total steps = 161201): 55.17973893880844\n",
      "1709386294.64976\n",
      "step 161301, epoch 0 (total steps = 161301): 55.17179095745087\n",
      "1709386295.0466607\n",
      "step 161401, epoch 0 (total steps = 161401): 55.05685740709305\n",
      "1709386295.423616\n",
      "step 161501, epoch 0 (total steps = 161501): 55.217992186546326\n",
      "1709386295.8195179\n",
      "step 161601, epoch 0 (total steps = 161601): 55.28797572851181\n",
      "1709386296.1891875\n",
      "step 161701, epoch 0 (total steps = 161701): 55.19146937131882\n",
      "1709386296.5591419\n",
      "step 161801, epoch 0 (total steps = 161801): 55.31445652246475\n",
      "1709386296.8945339\n",
      "step 161901, epoch 0 (total steps = 161901): 55.30773603916168\n",
      "1709386297.2641447\n",
      "step 162001, epoch 0 (total steps = 162001): 55.22816854715347\n",
      "1709386297.6212\n",
      "step 162101, epoch 0 (total steps = 162101): 55.457616448402405\n",
      "1709386297.957188\n",
      "step 162201, epoch 0 (total steps = 162201): 55.06033653020859\n",
      "1709386298.2961292\n",
      "step 162301, epoch 0 (total steps = 162301): 55.34502959251404\n",
      "1709386298.6344895\n",
      "step 162401, epoch 0 (total steps = 162401): 55.07601851224899\n",
      "1709386298.9806147\n",
      "step 162501, epoch 0 (total steps = 162501): 55.44553852081299\n",
      "1709386299.3233702\n",
      "step 162601, epoch 0 (total steps = 162601): 55.11305695772171\n",
      "1709386299.6972458\n",
      "step 162701, epoch 0 (total steps = 162701): 55.3203729391098\n",
      "1709386300.0628138\n",
      "step 162801, epoch 0 (total steps = 162801): 55.14837658405304\n",
      "1709386300.3999965\n",
      "step 162901, epoch 0 (total steps = 162901): 55.106377959251404\n",
      "1709386300.7491624\n",
      "step 163001, epoch 0 (total steps = 163001): 55.40696835517883\n",
      "1709386301.0863054\n",
      "step 163101, epoch 0 (total steps = 163101): 55.256821036338806\n",
      "1709386301.441163\n",
      "step 163201, epoch 0 (total steps = 163201): 55.13019686937332\n",
      "1709386301.786679\n",
      "step 163301, epoch 0 (total steps = 163301): 55.51388704776764\n",
      "1709386302.1272705\n",
      "step 163401, epoch 0 (total steps = 163401): 54.938505589962006\n",
      "DONE\n",
      "end time 1709386304.94853\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "##########################\n",
    "# CHECK DEVICE\n",
    "##########################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "##########################\n",
    "# PARAMETERS\n",
    "##########################\n",
    "early_stop = False\n",
    "batch_size = 64\n",
    "embedding_size = 50  \n",
    "learning_rate = 5e-5  \n",
    "window_size = 2  \n",
    "min_token_freq = 5\n",
    "epochs = 1\n",
    "optimizer_choice = AdamW\n",
    "\n",
    "\n",
    "##########################\n",
    "# MODEL\n",
    "##########################\n",
    "use_med = True\n",
    "\n",
    "if not use_med:\n",
    "    model = Word2Vec(vocab_size=len(corpus_s.word_to_index), embedding_size=embedding_size).to(device)\n",
    "    data_loader = DataLoader(training_data_s, batch_size=batch_size, shuffle=True, num_workers=min(os.cpu_count()-2, 20))\n",
    "elif use_med:\n",
    "    model = Word2Vec(vocab_size=len(corpus.word_to_index), embedding_size=embedding_size).to(device)\n",
    "    data_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True,num_workers=min(os.cpu_count()-2, 20))\n",
    "    \n",
    "optimizer = optimizer_choice(lr=learning_rate, params=model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "##########################\n",
    "# TRAINING\n",
    "##########################\n",
    "wandb.init(project='medium_run_very_fast_l5', name='medium_run_very_fast_l5')\n",
    "start_time = time.time()\n",
    "print(\"start time\", start_time)\n",
    "loss_data = []\n",
    "total_step = 0\n",
    "loss_sum = 0\n",
    "for epoch in range(epochs):\n",
    "    epoch_step = 0\n",
    "    model.train()\n",
    "    for step, data in enumerate(tqdm(data_loader)):\n",
    "        \n",
    "        target_word_ids, context_ids, labels = data\n",
    "        \n",
    "        target_word_ids = target_word_ids.to(device)\n",
    "        context_ids = context_ids.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(target_word_ids, context_ids)\n",
    "        loss = loss_fn(preds,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_step += 1\n",
    "        epoch_step +=1 \n",
    "    \n",
    "   \n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        if (step) % 100 == 0:\n",
    "            wandb.log({\"loss last 100 steps\": loss_sum})\n",
    "            print(time.time())\n",
    "            print(f\"step {epoch_step}, epoch {epoch} (total steps = {total_step}): {loss_sum}\")\n",
    "            loss_data.append({'epoch':epoch, 'loss':loss.item(), 'epoch_step':epoch_step, 'total_step':total_step})\n",
    "            loss_sum = 0\n",
    "        if early_stop:\n",
    "            if total_step > early_stop:\n",
    "                print(\"BREAKING\")\n",
    "                break\n",
    "print(\"DONE\")\n",
    "end_time = time.time()\n",
    "print(\"end time\", end_time)\n",
    "\n",
    "\n",
    "##########################\n",
    "# PLOTS\n",
    "##########################\n",
    "model.eval()\n",
    "loss_df = pd.DataFrame(loss_data)\n",
    "#sns.lineplot(data=loss_df, x='total_step', y='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35193"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(corpus_s.word_to_index.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify things are working\n",
    "\n",
    "Once you have an initial model trained, try using the following code to query the model for what are the nearest neighbor of a word. This code is intended to help you debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to change things a bit to get this working with GPU, mostly just correctly moving stuff to correct device\n",
    "\n",
    "def get_neighbors(model, word_to_index, target_word):\n",
    "    \"\"\" \n",
    "    Finds the top 10 most similar words to a target word\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for word, index in tqdm(word_to_index.items(), total=len(word_to_index)):\n",
    "        similarity = compute_cosine_similarity(model, word_to_index, target_word, word)\n",
    "        result = {\"word\": word, \"score\": similarity}\n",
    "        outputs.append(result)\n",
    "\n",
    "    # Sort by highest scores\n",
    "    neighbors = sorted(outputs, key=lambda o: o['score'], reverse=True)\n",
    "    return neighbors[1:11]\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    \n",
    "    '''\n",
    "    device = next(model.parameters()).device  \n",
    "\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]))\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().squeeze().numpy(),\n",
    "                                      embedding_two.detach().squeeze().numpy())))\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    device = next(model.parameters()).device  \n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]).to(device))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]).to(device))\n",
    "\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().squeeze().cpu().numpy(),\n",
    "                                      embedding_two.detach().squeeze().cpu().numpy())))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f65040dde741a19b365ff4ea6f987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'families', 'score': 0.9972072839736938},\n",
       " {'word': 'problems', 'score': 0.9960184693336487},\n",
       " {'word': 'health', 'score': 0.9958488941192627},\n",
       " {'word': 'questions', 'score': 0.9957934021949768},\n",
       " {'word': 'feelings', 'score': 0.9953974485397339},\n",
       " {'word': 'details', 'score': 0.994978129863739},\n",
       " {'word': 'women', 'score': 0.9946364164352417},\n",
       " {'word': 'animals', 'score': 0.9944844245910645},\n",
       " {'word': 'control', 'score': 0.9944610595703125},\n",
       " {'word': 'techniques', 'score': 0.9944319128990173}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9e997d3e9b4dbda226468c39c7b1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "    kv = KeyedVectors(vector_size=model.embedding_size)        \n",
    "    vectors = []\n",
    "    words = []\n",
    "    device = model.target_embeddings.weight.device  # Get the device of the model embeddings\n",
    "\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        index_tensor = torch.LongTensor([index]).to(device)  \n",
    "        vector = model.target_embeddings(index_tensor).detach().cpu().numpy()[0]  \n",
    "        vectors.append(vector)\n",
    "        words.append(word)\n",
    "\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)\n",
    "\n",
    "\n",
    "\n",
    "save(model, corpus, \"med_default_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your vectors / data for the pytorch classifier in Part 4!\n",
    "\n",
    "We'll be to using these vectors later in Part 4. We want to save them in a format that PyTorch can easily use. In particular you'll need to save the _state dict_ of the embeddings, which captures all of its information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the mapping from word to index so we can figure out which embedding to use for different words. Save the `corpus` objects mapping to a file using your preferred format (e.g., pickle or json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
