{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Parts 1-3: Word2Vec\n",
    "\n",
    "This homework will have you implementing word2vec using PyTorch and let you familiarize yourself with building more complex neural networks and the larger PyTorch development infrastructure.\n",
    "\n",
    "Broadly, this homework consists of a few major parts:\n",
    "1. Implement a `Corpus` class that will load the dataset and convert it to a sequence of token ids\n",
    "2. Implement negative sampling to select tokens to be used as negative examples of words in the context\n",
    "3. Create your dataset of positive and negative examples per context and load it into PyTorch's `DataLoader` to use for sampling\n",
    "4. Implement a `Word2Vec` class that is a PyTorch neural network\n",
    "5. Implement a training loop that samples a _batch_ of target words and their respective positive/negative context words\n",
    "6. Implement rare word removal and frequent word subsampling\n",
    "7. Run your model on the full dataset for at least one epoch\n",
    "8. Do the exploratory parts of the homework\n",
    "9. Save vectors and word-indexing data for later use in training a classifier\n",
    "\n",
    "After Step 5, you should be able to run your word2vec implementation on a small dataset and verify that it's learning correctly. Once you can verify everything is working, proceed with steps 6 and beyond. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**\n",
    "\n",
    "### Estimated performance times on medium dataset\n",
    "\n",
    "We designed this homework to be run on a laptop-grade CPU, so no GPU is required. If your primary computing device is a tablet or similar device, this homework can also be _developed_ on that device but then run on a more powerful machine in the Great Lakes cluster (for free). Such cases are the exception though. Following, we report on the estimated times from our reference implementation on the medium dataset for longer-running or data-intensive pieces of the homework. Your timing may vary based on implementation design; major differences in time (e.g., 10x longer) usually point to a performance bug.\n",
    "\n",
    "* Reading and tokenizing: ~5 seconds\n",
    "* Subsampling and converting to token ids: ~15 seconds\n",
    "* Generating the list of training examples: ~2 minutes (~15 minutes before the random number generator fix)\n",
    "* Training one epoch: ~12 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff381829410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Handy command-line argument parsing\n",
    "import argparse\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'cat']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokenizer.tokenize(text=\"I am a cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an efficient random number generator (do this part this later)\n",
    "\n",
    "Computers have to work to generate random numbers. However, the effort for getting those random numbers varies by how many you ask for. In practice, it's _much_ more efficient to generate many random numbers at once, rather than one at a time. \n",
    "\n",
    "\n",
    "In generating the training data for word2vec, you'll be generating a lot of random numbers. We've added a helpful class that you can use to eventually speed things up. You should use an instance of this `RandomNumberGenerator` class to generate the numbers you need (rather than using `np.random`). This class should work as a quick drop-in in its current implementation. **You should change the implementation of this class _only_ after getting the rest of your code debugged**. Once you get things working, update the code in this class so that it will create large buffers of random numbers and then when asked for a new number, read the next number from the buffer instead of calling `random` or `randint`. Essentially, this class will pre-allocate many random numbers ahead of time and then return them in order to avoid the overhead of generating them one at a time. You could see up to an ~80% performance improvement in your negatve sampling code generation as a result of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNumberGenerator:\n",
    "    ''' \n",
    "    A wrapper class for a random number generator that will (eventually) hold buffers of pre-generated random numbers for\n",
    "    faster access. For now, it just calls np.random.randint and np.random.random to generate these numbers \n",
    "    at the time they are needed.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, buffer_size, seed=12345):\n",
    "        '''\n",
    "        Initializes the random number generator with a seed and a buffer size of random numbers to use\n",
    "\n",
    "        Args:\n",
    "            buffer_size: The number of random numbers to pre-generate. You will eventually want \n",
    "                         this to be a large-enough number than you're not frequently regenerating the buffer\n",
    "            seed: The seed for the random number generator\n",
    "        '''\n",
    "        self.max_val = -1\n",
    "        # TODO (later): create a random number generator using numpy and set its seed    \n",
    "        # TODO (later): pre-generate a buffer of random floats to use for random()\n",
    "\n",
    "    def random(self):\n",
    "        '''\n",
    "        Returns a random float value between 0 and 1\n",
    "        '''\n",
    "        # TODO (later): get a random number from the float buffer, rather than calling np.random.random\n",
    "        # NOTE: If you reach the end of the buffer, you should refill it with new random float numbers\n",
    "        return np.random.random()\n",
    "\n",
    "    def set_max_val(self, max_val):\n",
    "        '''\n",
    "        Sets the maximum integer value for randint and creates a buffer of random integers\n",
    "        '''\n",
    "        self.max_val = max_val\n",
    "        # NOTE: This default implemenation just sets the max_val and does not create a buffer of random integers\n",
    "        # TODO (later): Implement a buffer of random integers (for now, we'll just use np.random.randint)\n",
    "\n",
    "    def randint(self):\n",
    "        '''\n",
    "        Returns a random int value between 0 and self.max_val (inclusive)\n",
    "        '''        \n",
    "        if self.max_val == -1:\n",
    "            raise ValueError(\"Need to call set_max_val before calling randint\")\n",
    "        \n",
    "        # TODO (later): get a random number from the int buffer, rather than calling np.random.randint\n",
    "        # NOTE: If you reach the end of the buffer, you should refill it with new random ints\n",
    "        return np.random.randint(0, self.max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class to hold the data\n",
    "\n",
    "Before we get to training word2vec, we'll need to process the corpus into some representation. The `Corpus` class will handle much of the functionality for corpus reading and keeping track of which word types belong to which ids. The `Corpus` class will also handle the crucial functionality of generating negative samples for training (i.e., randomly-sampled words that were not in the target word's context).\n",
    "\n",
    "Some parts of this class can be completed after you've gotten word2vec up and running, so see the notes below and the details in the homework PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, rng):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.rng = rng\n",
    "\n",
    "        # These state variables become populated with function calls\n",
    "        #\n",
    "        # 1. load_data()\n",
    "        # 2. generate_negative_sampling_table()\n",
    "        #\n",
    "        # See those functions for how the various values get filled in\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.full_token_sequence_as_ids = None\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "        \n",
    "        with open(file_name, 'r') as f:\n",
    "            l = f.readlines()  \n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        all_tokens = []\n",
    "        print('Reading data and tokenizing')\n",
    "        for line in l:\n",
    "            tokens = self.tokenizer.tokenize(line)\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        print('Counting token frequencies')\n",
    "        token_counts = dict(Counter(all_tokens))\n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK>\n",
    "        # token. \n",
    "        #\n",
    "        # NOTE: You can do this step later if needed\n",
    "        \n",
    "        print(\"Performing minimum thresholding\")\n",
    "        tokens_to_replace = {token for token, count in token_counts.items() if count <= min_token_freq}\n",
    "        filtered_tokens = [token if token not in tokens_to_replace else \"<UNK>\" for token in all_tokens]\n",
    "\n",
    "\n",
    "\n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "        # occurs (including <UNK>)\n",
    "        self.word_counts = dict(Counter(filtered_tokens))\n",
    "        # print(\"UNK\", self.word_counts['<UNK>'])\n",
    "\n",
    "        \n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping.\n",
    "        \n",
    "        # Create word_to_index and index_to_word mappings\n",
    "\n",
    "        unique_words = list(self.word_counts.keys())\n",
    "        self.word_to_index = {word: i for i, word in enumerate(unique_words)}\n",
    "        self.index_to_word = {i: word for word, i in self.word_to_index.items()}\n",
    "        self.full_token_sequence_as_ids = [self.word_to_index[token] for token in filtered_tokens]\n",
    "        \n",
    "        # Step 6: Compute the probability of keeping any particular *token* of a\n",
    "        # word in the training sequence, which we'll use to subsample. This subsampling\n",
    "        # avoids having the training data be filled with many overly common words\n",
    "        # as positive examples in the context\n",
    "   \n",
    "                        \n",
    "        # Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "        # a new list self.full_token_sequence_as_ids where \n",
    "        #\n",
    "        # (1) we probabilistically choose whether to keep each *token* based on the\n",
    "        # subsampling probabilities (note that this does not mean we drop\n",
    "        # an entire word!) and \n",
    "        #\n",
    "        # (2) all tokens are convered to their unique ids for faster training.\n",
    "        #\n",
    "        # NOTE: You can skip the subsampling part and just do step 2 to get\n",
    "        # your model up and running.\n",
    "            \n",
    "        # NOTE 2: You will perform token-based subsampling based on the probabilities in\n",
    "        # word_to_sample_prob. When subsampling, you are modifying the sequence itself \n",
    "        # (like deleting an item in a list). This action effectively makes the context\n",
    "        # window  larger for some target words by removing context words that are common\n",
    "        # from a particular context before the training occurs (which then would now include\n",
    "        # other words that were previously just outside the window).\n",
    "\n",
    "\n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "        \n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "        '''\n",
    "        Generates a big list data structure that we can quickly randomly index into\n",
    "        in order to select a negative training example (i.e., a word that was\n",
    "        *not* present in the context). \n",
    "        '''       \n",
    "        \n",
    "        words = np.array(list(self.word_to_index.values()))\n",
    "        freqs = np.array(list(self.word_counts.values()))\n",
    "        weights = np.power(freqs, exp_power)\n",
    "        sum_weights = np.sum(weights)\n",
    "        probs = weights / sum_weights\n",
    "    \n",
    "        \n",
    "        # Step 1: Figure out how many instances of each word need to go into the\n",
    "        # negative sampling table. \n",
    "        #\n",
    "        # HINT: np.power and np.fill might be useful here        \n",
    "        print(\"Generating sampling table\")\n",
    "        samples_per_word = probs * table_size\n",
    "\n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "        adjusted_table_size = int(np.sum(np.floor(samples_per_word)))\n",
    "\n",
    "\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled.\n",
    "        #\n",
    "        # Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "        # 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "        # (before converting the words to IDs):\n",
    "        #\n",
    "        # [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "        #\n",
    "        \n",
    "        sampling_table = []\n",
    "        for word_idx, samples in enumerate(samples_per_word):\n",
    "            count = int(np.round(samples))\n",
    "            sampling_table.extend([word_idx]*count)\n",
    "        \n",
    "        self.negative_sampling_table = np.array(sampling_table)\n",
    "        \n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):\n",
    "        '''\n",
    "        Randomly samples the specified number of negative samples from the lookup\n",
    "        table and returns this list of IDs as a numpy array. As a performance\n",
    "        improvement, avoid sampling a negative example that has the same ID as\n",
    "        the current positive context word.\n",
    "        '''\n",
    "        #print(\"getting negative samples\")\n",
    "        results = []\n",
    "\n",
    "        # Create a list and sample from the negative_sampling_table to\n",
    "        # grow the list to num_samples, avoiding adding a negative example that\n",
    "        # has the same ID as the current context_word\n",
    "        while len(results) < num_samples:\n",
    "            sampled = np.random.choice(self.negative_sampling_table)\n",
    "            if sampled != cur_context_word_id:\n",
    "                results.append(sampled)\n",
    "        return np.array(results)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "UNK 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'hello': 0, 'world': 1, 'test': 2, 'example': 3},\n",
       " {0: 'hello', 1: 'world', 2: 'test', 3: 'example'},\n",
       " [0, 1, 0, 2, 0, 1, 3, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Dummy tokenizer function to simulate the behavior in the provided code\n",
    "def dummy_tokenizer(line):\n",
    "    # Simply split by spaces for this example\n",
    "    return line.split()\n",
    "\n",
    "# Dummy corpus\n",
    "corpus_lines = [\n",
    "    \"hello world hello\",\n",
    "    \"test hello world\",\n",
    "    \"example test example\"\n",
    "]\n",
    "\n",
    "# Minimum token frequency threshold\n",
    "min_token_freq = 1\n",
    "\n",
    "# Simulating the provided code with the dummy corpus\n",
    "\n",
    "# all tokens in the file\n",
    "all_tokens = []\n",
    "print('Reading data and tokenizing')\n",
    "for line in corpus_lines:\n",
    "    tokens = dummy_tokenizer(line)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Step 2: Count how many tokens we have of each type\n",
    "print('Counting token frequencies')\n",
    "token_counts = dict(Counter(all_tokens))\n",
    "\n",
    "# Step 3: Replace all tokens below the specified frequency with an <UNK> token.\n",
    "print(\"Performing minimum thresholding\")\n",
    "tokens_to_replace = {token for token, count in token_counts.items() if count <= min_token_freq}\n",
    "filtered_tokens = [token if token not in tokens_to_replace else \"<UNK>\" for token in all_tokens]\n",
    "\n",
    "# Step 4: update word_counts to be the number of times each word occurs (including <UNK>)\n",
    "word_counts = dict(Counter(filtered_tokens))\n",
    "print(\"UNK\", word_counts.get('<UNK>', 0))\n",
    "\n",
    "# Step 5: Create the mappings from word to unique integer ID and the reverse mapping.\n",
    "unique_words = list(word_counts.keys())\n",
    "word_to_index = {word: i for i, word in enumerate(unique_words)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "full_token_sequence_as_ids = [word_to_index[token] for token in filtered_tokens]\n",
    "\n",
    "# Results\n",
    "(word_to_index, index_to_word, full_token_sequence_as_ids)\n",
    "\n",
    "\n",
    "# corpus_lines = [\n",
    "#     \"hello world hello\",\n",
    "#     \"test hello world\",\n",
    "#     \"example test example\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the corpus\n",
    "\n",
    "Now that we have code to turn the text into training data, let's do so. We've provided several files for you to help:\n",
    "\n",
    "* `reviews-word2vec.tiny.txt` -- use this to debug your corpus reader\n",
    "* `reviews-word2vec.med.txt` -- use this to debug/verify the whole word2vec works\n",
    "* `reviews-word2vec.large.txt.gz` -- use this when everything works to generate your vectors for later parts\n",
    "* `reviews-word2vec.HUGE.gz` -- _do not use this_ unless (1) everything works and (2) you really want to test/explore. This file is not needed at all to do your homework.\n",
    "\n",
    "We recommend starting to debug with the first file, as it is small and fast to load (quicker to find bugs). When debugging, we recommend setting the `min_token_freq` argument to 2 so that you can verify that part of the code is working but you still have enough word types left to test the rest.\n",
    "\n",
    "You'll use the remaining files later, where they're described.\n",
    "\n",
    "In the next cell, create your `Corpus`, read in the data, and generate the negative sampling table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from reviews-word2vec.tiny.txt; saw 20985 tokens (937 unique)\n",
      "Generating sampling table\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(rng=RandomNumberGenerator)\n",
    "corpus.load_data(file_name=\"reviews-word2vec.tiny.txt\", min_token_freq=2)\n",
    "corpus.generate_negative_sampling_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the training data\n",
    "\n",
    "Once we have the corpus ready, we need to generate our training dataset. Each instance in the dataset is a target word and positive and negative examples of contexts words. Given the target word as input, we'll want to predict (or not predict) these positive and negative context words as outputs using our network. Your task here is to create a python `list` of instances. \n",
    "\n",
    "Your final training data should be a list of tuples in the format ([target_word_id], [word_id_1, ...], [predicted_labels]), where each item in the list is a list:\n",
    "1. The first item is a list consisting only of the target word's ID.\n",
    "2. The second item is a list of word ids for both context words and negative samples \n",
    "3. The third item is a list of labels to predicted for each of the word ids in the second list (i.e., `1` for context words and `0` for negative samples). \n",
    "\n",
    "You will feed these tuples into the PyTorch `DatasetLoader` later that will do the converstion to `Tensor` objects. You will need to make sure that all of the lists in each tuple are `np.array` instances and are not plain python lists for this `Tensor` converstion to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data(corpus, window_size=2, num_negative_samples_per_target=2):\n",
    "    window_size = 2\n",
    "    num_negative_samples_per_target = 2\n",
    "    max_words = window_size*2  + (num_negative_samples_per_target*(window_size)*2)\n",
    "    len_corpus = len(corpus.full_token_sequence_as_ids)\n",
    "    print(\"Max words\", max_words)\n",
    "\n",
    "    training_data = []\n",
    "\n",
    "    # Loop through each token in the corpus and generate an instance for each, \n",
    "    # adding it to training_data\n",
    "    for target_word_id in range(len_corpus+1):\n",
    "\n",
    "        neg_ids = []\n",
    "        all_ids = []\n",
    "        labels = []\n",
    "\n",
    "        start_idx = max(0, target_word_id - window_size)\n",
    "        end_idx = min(len_corpus, target_word_id + window_size)\n",
    "\n",
    "\n",
    "        # For exach target word in our dataset, select context words \n",
    "        # within +/- the window size in the token sequence\n",
    "        context_word_ids = [x for x in range(start_idx, end_idx+1) if x != target_word_id]\n",
    "\n",
    "        # For each positive target, we need to select negative examples of\n",
    "        # words that were not in the context. Use the num_negative_samples_per_target\n",
    "        # hyperparameter to generate these, using the generate_negative_samples()\n",
    "        # method from the Corpus class\n",
    "        for wid in context_word_ids:\n",
    "            t_neg_ids = corpus.generate_negative_samples(wid, num_negative_samples_per_target)\n",
    "            neg_ids.extend(t_neg_ids)\n",
    "\n",
    "        # NOTE: this part might not make sense until later when you do the training \n",
    "        # so feel free to revisit it to see why it happens.\n",
    "        #\n",
    "        # Our training will use batches of instances together (compare that \n",
    "        # with HW1's SGD that used one item at a time). PyTorch will require\n",
    "        # that all instances in a batches have the same size, which creates an issue\n",
    "        # for us here since the target wordss at the very beginning or end of the corpus\n",
    "        # have shorter contexts. \n",
    "        \n",
    "        # Adjust for edge cases at the beginning and end of the corpus\n",
    "\n",
    "        total_ids = len(context_word_ids) + len(neg_ids)\n",
    "        gap = max_words - total_ids\n",
    "        if gap >0:\n",
    "            extra_neg_ids = corpus.generate_negative_samples(wid, gap)\n",
    "            neg_ids.extend(extra_neg_ids)\n",
    "\n",
    "        # \n",
    "        # To work around these edge-cases, we need to ensure that each instance has\n",
    "        # the same size, which means it needs to have the same number of positive\n",
    "        # and negative examples. Since we are short on positive examples here (due\n",
    "        # to the edge of the corpus), we can just add more negative samples.\n",
    "        #\n",
    "        # YOUR TASK: determine what is the maximum number of context words (positive\n",
    "        # and negative) for any instance and then, for instances that have fewer than\n",
    "        # this number of context words, add in negative examples.\n",
    "        #\n",
    "        # NOTE: The maximum is fixed, so you can precompute this outside the loop\n",
    "        # ahead of time.\n",
    "        all_ids = context_word_ids + neg_ids\n",
    "        labels = [1]*len(context_word_ids) + [0]*len(neg_ids)\n",
    "        training_data.append((np.array([target_word_id]), np.array(all_ids), np.array(labels)))\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in training_data[:20]:\n",
    "#     neg_len = len(x[2]) - np.sum(x[2])\n",
    "#     pos_len = np.sum(x[2])\n",
    "#     total_len = len(x[2])\n",
    "#     print(\"Neg len\", neg_len, \"Pos len\", pos_len, \"Total ids\", total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network\n",
    "\n",
    "We'll create a new neural network as a subclass of `nn.Module` like we did in Homework 1. However, _unlike_ the network you built in Homework 1, we do not need to used linear layers to implement word2vec. Instead, we will use PyTorch's `Emedding` class, which maps an index (e.g., a word id in this case) to an embedding. \n",
    "\n",
    "Roughly speaking, word2vec's network makes a prediction by computing the dot product of the target word's embedding and a context word's embedding and then passing this dot product through the sigmoid function ($\\sigma$) to predict the probability that the context word was actually in the context. The homework write-up has lots of details on how this works. Your `forward()` function will have to implement this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        # Save what state you want and create the embeddings for your\n",
    "        # target and context words\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Initialize embeddings with non-zero random numbers\n",
    "        self.init_emb(init_range=0.5/self.vocab_size)\n",
    "        \n",
    "    def init_emb(self, init_range):\n",
    "        \n",
    "        # Fill your two embeddings with random numbers uniformly sampled\n",
    "        # between +/- init_range\n",
    "    \n",
    "        self.target_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, target_word_id, context_word_ids):\n",
    "        ''' \n",
    "        Predicts whether each context word was actually in the context of the target word.\n",
    "        The input is a tensor with a single target word's id and a tensor containing each\n",
    "        of the context words' ids (this includes both positive and negative examples).\n",
    "        '''\n",
    "        \n",
    "        # NOTE 1: This is probably the hardest part of the homework, so you'll\n",
    "        # need to figure out how to do the dot-product between embeddings and return\n",
    "        # the sigmoid. Be prepared for lots of debugging. For some reference,\n",
    "        # our implementation is three lines and really the hard part is just\n",
    "        # the last line. However, it's usually a matter of figuring out what\n",
    "        # that one line looks like that ends up being the hard part.\n",
    "        \n",
    "        # NOTE 2: In this homework you'll be dealing with *batches* of instances\n",
    "        # rather than a single instance at once. PyTorch mostly handles this\n",
    "        # seamlessly under the hood for you (which is very nice) but batching\n",
    "        # can show in weird ways and create challenges in debugging initially.\n",
    "        # For one, your inputs will get an extra dimension. So, for example,\n",
    "        # if you have a batch size of 4, your input for target_word_id will\n",
    "        # really be 4 x 1. If you get the embeddings of those targets,\n",
    "        # it then becomes 4x50! The same applies to the context_word_ids, except\n",
    "        # that was alreayd a list so now you have things with shape \n",
    "        #\n",
    "        #    (batch x context_words x embedding_size)\n",
    "        #\n",
    "        # One of your tasks will be to figure out how to get things lined up\n",
    "        # so everything \"just works\". When it does, the code looks surprisingly\n",
    "        # simple, but it might take a lot of debugging (or not!) to get there.\n",
    "        \n",
    "        # NOTE 3: We *strongly* discourage you from looking for existing \n",
    "        # implementations of word2vec online. Sadly, having reviewed most of the\n",
    "        # highly-visible ones, they are actually wrong (wow!) or are doing\n",
    "        # inefficient things like computing the full softmax instead of doing\n",
    "        # the negative sampling. Looking at these will likely leave you more\n",
    "        # confused than if you just tried to figure it out yourself.\n",
    "        \n",
    "        # NOTE 4: There many ways to implement this, some more efficient\n",
    "        # than others. You will want to get it working first and then\n",
    "        # test the timing to see how long it takes. As long as the\n",
    "        # code works (vector comparisons look good) you'll receive full\n",
    "        # credit. However, very slow implementations may take hours(!)\n",
    "        # to converge so plan ahead.\n",
    "        \n",
    "        \n",
    "        # Hint 1: You may want to review the mathematical operations on how\n",
    "        # to compute the dot product to see how to do these\n",
    "        \n",
    "        target_embeddings = self.target_embeddings(target_word_ids)  \n",
    "        context_embeddings = self.context_embeddings(context_word_ids)  \n",
    "        context_embeddings_transposed = context_embeddings.transpose(1, 2)  \n",
    "        preds = torch.sigmoid(torch.bmm(target_embeddings, context_embeddings_transposed)).squeeze(1)\n",
    "\n",
    "\n",
    "    \n",
    "        return preds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='log_time', ylabel='val'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "a =dict( enumerate(loss_l))\n",
    "b = pd.DataFrame(a, index=[0]).T.reset_index()\n",
    "b.columns = ['log_time', 'val']\n",
    "sns.lineplot(data=b, x='log_time', y='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network!\n",
    "\n",
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The trainin code will look surprisingly similar at times to your pytorch code from Homework 1 since all networks share the same base training setup. However, we'll add a few new elements to get you familiar with more common training techniques. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. Create a new `SummaryWriter` to periodically write our running-sum of the loss to a tensorboard\n",
    "5. Train your model \n",
    "\n",
    "Two new elements show up. First, we'll be using `DataLoader` which is going to sample data for us and put it in a batch (and also convert the data to `Tensor` objects. You can iterate over the batches and each iteration will return all the items eventually, one batch at a time (a full epoch's worth).\n",
    "\n",
    "The second new part is using `wandb`. As you might have noticed in Homework 1, training neural models can take some time. [Weights & Biases](https://wandb.ai/) is a handy web-based view that you can check during training to see how the model is doing. We'll use it here and periodically log a running sum of the loss after a set number of steps. The Homework write up has a plot of what this looks like. We'll be doing something simple here with wandb but it will come in handy later as you train larger models (for longer) and may want to visually check if your model is converging and is [easy to integrate](https://docs.wandb.ai/guides/integrations/pytorch).\n",
    "\n",
    "Once you get the code working, to start training, we recommend training on the `reviews-word2vec.med.txt` dataset. This data is small enough you can get through an epoch in a few minutes (or less) while still being large enough you can test whether the model is learning anything by examining common words. Below this cell we've added a few helper functions that you can use to debug and query your model. In particular, the `get_neighbors()` function is a great way to test: if your model has learned anything, the nearest neighbors for common words should seem reasonable (without having to jump through mental hoops). An easy word to test on the `med` data is \"january\" which should return month-related words as being most similar.\n",
    "\n",
    "**NOTE**: Since we're training biographies, the text itself will be skewed towards words likely to show up biographices--which isn't necessary like \"regular\" text. You may find that your model has few instances of words you think are common, or that the model learns poor or unusual neighbors for these. When querying the neighbors, it can help to think of which words you think are likely to show up in biographies on Wikipedia and use those as probes to see what the model has learned.\n",
    "\n",
    "Once you're convinced the model is learning, switch to the `med` data and train your model as specified in the PDF. Once trained, save your model using the `save()` function at the end of the notebook. This function records your data in a common format for word2vec vectors and lets you load the vectors into other libraries that have more advanced functionality. In particular, you can use the [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) code in other notebook included to explore the vectors and do simple vector analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from reviews-word2vec.tiny.txt; saw 20985 tokens (461 unique)\n",
      "Generating sampling table\n",
      "tiny_training_5.pkl LOADED\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle \n",
    "\n",
    "corpus = Corpus(rng=RandomNumberGenerator)\n",
    "corpus.load_data(file_name=\"reviews-word2vec.tiny.txt\", min_token_freq=5)\n",
    "corpus.generate_negative_sampling_table()\n",
    "#training_data = gen_training_data(corpus)\n",
    "\n",
    "file_path = 'tiny_training_5.pkl'\n",
    "if not os.path.exists(file_path):\n",
    "    training_data = gen_training_data(corpus)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(training_data, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(file_path, \"SAVED\")\n",
    "else:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        training_data = pickle.load(file)\n",
    "    print(file_path, \"LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471824645996\n",
      "0.6931472420692444\n",
      "0.6931471824645996\n",
      "0.6931472420692444\n",
      "0.6931474208831787\n",
      "0.6931471228599548\n",
      "0.6931471824645996\n",
      "0.6931471824645996\n",
      "0.6931471824645996\n",
      "0.6931471228599548\n",
      "0.6931471824645996\n",
      "0.6931477189064026\n",
      "0.6931469440460205\n",
      "0.6931468844413757\n",
      "0.6931471228599548\n",
      "0.6931472420692444\n",
      "0.6931467652320862\n",
      "0.6931474208831787\n",
      "0.6931476593017578\n",
      "0.6931467652320862\n",
      "0.6931471824645996\n",
      "0.6931471824645996\n",
      "0.6931478381156921\n",
      "0.6931471228599548\n",
      "0.6931472420692444\n",
      "0.6931473612785339\n",
      "0.6931468844413757\n",
      "0.6931465268135071\n",
      "0.6931474208831787\n",
      "0.6931471824645996\n",
      "0.6931473612785339\n",
      "0.6931474208831787\n",
      "0.6931462287902832\n",
      "0.6931478381156921\n",
      "0.6931467056274414\n",
      "0.6931468844413757\n",
      "0.6931468844413757\n",
      "0.6931467652320862\n",
      "0.6931470036506653\n",
      "0.6931471824645996\n",
      "0.6931468844413757\n",
      "0.6931471824645996\n",
      "0.6931471824645996\n",
      "0.6931474208831787\n",
      "0.6931476593017578\n",
      "0.6931470036506653\n",
      "0.6931469440460205\n",
      "0.6931471228599548\n",
      "0.6931479573249817\n",
      "0.6931474804878235\n",
      "0.6931467056274414\n",
      "0.6931472420692444\n",
      "0.6645285487174988\n",
      "0.6577033400535583\n",
      "0.6580213308334351\n",
      "0.6275389790534973\n",
      "0.6422518491744995\n",
      "0.6092782616615295\n",
      "0.6471320986747742\n",
      "0.6246159076690674\n",
      "0.6390219926834106\n",
      "0.6146238446235657\n",
      "0.5616210103034973\n",
      "0.6061498522758484\n",
      "0.5814173817634583\n",
      "0.598635733127594\n",
      "0.563798189163208\n",
      "0.5845521688461304\n",
      "0.6329932808876038\n",
      "0.6015578508377075\n",
      "0.5839301943778992\n",
      "0.5655264854431152\n",
      "0.5498371720314026\n",
      "0.6309111714363098\n",
      "0.5573359131813049\n",
      "0.5320706963539124\n",
      "0.557062029838562\n",
      "0.5477510690689087\n",
      "0.5774415731430054\n",
      "0.556351363658905\n",
      "0.539900004863739\n",
      "0.5531198382377625\n",
      "0.5415191054344177\n",
      "0.5623344779014587\n",
      "0.5644282102584839\n",
      "0.5478441119194031\n",
      "0.5627948641777039\n",
      "0.5791600346565247\n",
      "0.585282564163208\n",
      "0.5048801302909851\n",
      "0.5406479835510254\n",
      "0.5721406936645508\n",
      "0.5110147595405579\n",
      "0.5200875997543335\n",
      "0.5257443189620972\n",
      "0.5571553111076355\n",
      "0.5563779473304749\n",
      "0.43656083941459656\n",
      "0.5252367258071899\n",
      "0.5103636384010315\n",
      "0.5114844441413879\n",
      "0.5979716181755066\n",
      "0.5093752145767212\n",
      "0.5052415728569031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (target_embeddings): Embedding(20986, 200)\n",
       "  (context_embeddings): Embedding(20986, 200)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Set the hyperparameters\n",
    "batch_size = 4\n",
    "embedding_size = 200  # k\n",
    "learning_rate = 5e-4  # \n",
    "window_size = 2  # window 2\n",
    "min_token_freq = 5\n",
    "epochs = 2\n",
    "optimizer_choice = AdamW\n",
    "\n",
    "\n",
    "model = Word2Vec(vocab_size=len(training_data), embedding_size=embedding_size)\n",
    "\n",
    "data_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optimizer_choice(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "loss_l = []\n",
    "counter = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for target_word_ids, context_word_ids, labels in data_loader:\n",
    "        labels = labels.float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(target_word_ids, context_word_ids)\n",
    "        loss = loss_fn(torch.squeeze(preds), torch.squeeze(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        counter+=1\n",
    "        if counter % 100 == 0:\n",
    "            print(loss.item())\n",
    "            loss_l.append(loss.item())\n",
    "        \n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify things are working\n",
    "\n",
    "Once you have an initial model trained, try using the following code to query the model for what are the nearest neighbor of a word. This code is intended to help you debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(model, word_to_index, target_word):\n",
    "    \"\"\" \n",
    "    Finds the top 10 most similar words to a target word\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for word, index in tqdm(word_to_index.items(), total=len(word_to_index)):\n",
    "        similarity = compute_cosine_similarity(model, word_to_index, target_word, word)\n",
    "        result = {\"word\": word, \"score\": similarity}\n",
    "        outputs.append(result)\n",
    "\n",
    "    # Sort by highest scores\n",
    "    neighbors = sorted(outputs, key=lambda o: o['score'], reverse=True)\n",
    "    return neighbors[1:11]\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    '''\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]))\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().squeeze().numpy(),\n",
    "                                      embedding_two.detach().squeeze().numpy())))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c7a087290a47da97879eb5f35b2165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'good', 'score': 0.6158402562141418},\n",
       " {'word': 'interested', 'score': 0.6100205779075623},\n",
       " {'word': 'enjoyable', 'score': 0.5868216156959534},\n",
       " {'word': 'get', 'score': 0.5746874809265137},\n",
       " {'word': 'attention', 'score': 0.5718927979469299},\n",
       " {'word': 'leave', 'score': 0.5624241828918457},\n",
       " {'word': 'were', 'score': 0.5597525238990784},\n",
       " {'word': 'received', 'score': 0.5592596530914307},\n",
       " {'word': 'recommend', 'score': 0.5579097867012024},\n",
       " {'word': 'self', 'score': 0.5578015446662903}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2d7840c5494ddea67166fae6c4215c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'was', 'score': 0},\n",
       " {'word': 'bought', 'score': 0},\n",
       " {'word': 'as', 'score': 0},\n",
       " {'word': 'a', 'score': 0},\n",
       " {'word': 'gift', 'score': 0},\n",
       " {'word': 'but', 'score': 0},\n",
       " {'word': 'the', 'score': 0},\n",
       " {'word': 'person', 'score': 0},\n",
       " {'word': 'who', 'score': 0},\n",
       " {'word': 'got', 'score': 0}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"son\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "    kv = KeyedVectors(vector_size=model.embedding_size)        \n",
    "    vectors = []\n",
    "    words = []\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        vectors.append(model.target_embeddings(torch.LongTensor([index])).detach().numpy()[0])\n",
    "        words.append(word)\n",
    "\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your vectors / data for the pytorch classifier in Part 4!\n",
    "\n",
    "We'll be to using these vectors later in Part 4. We want to save them in a format that PyTorch can easily use. In particular you'll need to save the _state dict_ of the embeddings, which captures all of its information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the mapping from word to index so we can figure out which embedding to use for different words. Save the `corpus` objects mapping to a file using your preferred format (e.g., pickle or json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (misinformation_sim)",
   "language": "python",
   "name": "pycharm-9607488f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
